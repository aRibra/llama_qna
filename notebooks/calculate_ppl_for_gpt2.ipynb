{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d4a301a-6cda-4c2f-ac1a-f420737fd39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aribra/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "device = \"cuda\"\n",
    "model_id = \"openai-community/gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4289eb3-6441-4ed5-860b-25536ddb496c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ef4eb1-fc4d-43b5-ac56-aff662f690a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287644"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings.input_ids.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed6236ec-c0b7-47cf-b54b-c511acea1a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 628,  796, 5199,  ...,  220,  628,  198]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79d3eccf-4cee-4b76-9032-a89d3fdd23a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.n_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d4b43-0caf-48ff-994d-d251ea1686da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e61ef1f-3a5f-47c6-9b66-89484a9d8a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60399214-c3c5-4346-bc6d-eaf77a7ef8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                             | 0/257 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 5.79 GiB total capacity; 4.97 GiB already allocated; 91.25 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4452/2676274072.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# loss is calculated using CrossEntropyLoss which averages over valid labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1075\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    886\u001b[0m                 )\n\u001b[1;32m    887\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    889\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_upcast_and_reordered_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m_attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_attn_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 5.79 GiB total capacity; 4.97 GiB already allocated; 91.25 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(156160, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())\n",
    "\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5168f694-3e7f-4297-813b-9cca57143bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.1426, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ppl = torch.exp(torch.stack(nlls).mean())\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38e16938-b138-4b70-85e7-7bb78bdc5cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156160"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10b27741-10ee-4c85-b8ed-05e0c18dd787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(range(156160, seq_len, stride))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5870a285-634b-483c-a136-ba7279b0c26b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 512,\n",
       " 1024,\n",
       " 1536,\n",
       " 2048,\n",
       " 2560,\n",
       " 3072,\n",
       " 3584,\n",
       " 4096,\n",
       " 4608,\n",
       " 5120,\n",
       " 5632,\n",
       " 6144,\n",
       " 6656,\n",
       " 7168,\n",
       " 7680,\n",
       " 8192,\n",
       " 8704,\n",
       " 9216,\n",
       " 9728,\n",
       " 10240,\n",
       " 10752,\n",
       " 11264,\n",
       " 11776,\n",
       " 12288,\n",
       " 12800,\n",
       " 13312,\n",
       " 13824,\n",
       " 14336,\n",
       " 14848,\n",
       " 15360,\n",
       " 15872,\n",
       " 16384,\n",
       " 16896,\n",
       " 17408,\n",
       " 17920,\n",
       " 18432,\n",
       " 18944,\n",
       " 19456,\n",
       " 19968,\n",
       " 20480,\n",
       " 20992,\n",
       " 21504,\n",
       " 22016,\n",
       " 22528,\n",
       " 23040,\n",
       " 23552,\n",
       " 24064,\n",
       " 24576,\n",
       " 25088,\n",
       " 25600,\n",
       " 26112,\n",
       " 26624,\n",
       " 27136,\n",
       " 27648,\n",
       " 28160,\n",
       " 28672,\n",
       " 29184,\n",
       " 29696,\n",
       " 30208,\n",
       " 30720,\n",
       " 31232,\n",
       " 31744,\n",
       " 32256,\n",
       " 32768,\n",
       " 33280,\n",
       " 33792,\n",
       " 34304,\n",
       " 34816,\n",
       " 35328,\n",
       " 35840,\n",
       " 36352,\n",
       " 36864,\n",
       " 37376,\n",
       " 37888,\n",
       " 38400,\n",
       " 38912,\n",
       " 39424,\n",
       " 39936,\n",
       " 40448,\n",
       " 40960,\n",
       " 41472,\n",
       " 41984,\n",
       " 42496,\n",
       " 43008,\n",
       " 43520,\n",
       " 44032,\n",
       " 44544,\n",
       " 45056,\n",
       " 45568,\n",
       " 46080,\n",
       " 46592,\n",
       " 47104,\n",
       " 47616,\n",
       " 48128,\n",
       " 48640,\n",
       " 49152,\n",
       " 49664,\n",
       " 50176,\n",
       " 50688,\n",
       " 51200,\n",
       " 51712,\n",
       " 52224,\n",
       " 52736,\n",
       " 53248,\n",
       " 53760,\n",
       " 54272,\n",
       " 54784,\n",
       " 55296,\n",
       " 55808,\n",
       " 56320,\n",
       " 56832,\n",
       " 57344,\n",
       " 57856,\n",
       " 58368,\n",
       " 58880,\n",
       " 59392,\n",
       " 59904,\n",
       " 60416,\n",
       " 60928,\n",
       " 61440,\n",
       " 61952,\n",
       " 62464,\n",
       " 62976,\n",
       " 63488,\n",
       " 64000,\n",
       " 64512,\n",
       " 65024,\n",
       " 65536,\n",
       " 66048,\n",
       " 66560,\n",
       " 67072,\n",
       " 67584,\n",
       " 68096,\n",
       " 68608,\n",
       " 69120,\n",
       " 69632,\n",
       " 70144,\n",
       " 70656,\n",
       " 71168,\n",
       " 71680,\n",
       " 72192,\n",
       " 72704,\n",
       " 73216,\n",
       " 73728,\n",
       " 74240,\n",
       " 74752,\n",
       " 75264,\n",
       " 75776,\n",
       " 76288,\n",
       " 76800,\n",
       " 77312,\n",
       " 77824,\n",
       " 78336,\n",
       " 78848,\n",
       " 79360,\n",
       " 79872,\n",
       " 80384,\n",
       " 80896,\n",
       " 81408,\n",
       " 81920,\n",
       " 82432,\n",
       " 82944,\n",
       " 83456,\n",
       " 83968,\n",
       " 84480,\n",
       " 84992,\n",
       " 85504,\n",
       " 86016,\n",
       " 86528,\n",
       " 87040,\n",
       " 87552,\n",
       " 88064,\n",
       " 88576,\n",
       " 89088,\n",
       " 89600,\n",
       " 90112,\n",
       " 90624,\n",
       " 91136,\n",
       " 91648,\n",
       " 92160,\n",
       " 92672,\n",
       " 93184,\n",
       " 93696,\n",
       " 94208,\n",
       " 94720,\n",
       " 95232,\n",
       " 95744,\n",
       " 96256,\n",
       " 96768,\n",
       " 97280,\n",
       " 97792,\n",
       " 98304,\n",
       " 98816,\n",
       " 99328,\n",
       " 99840,\n",
       " 100352,\n",
       " 100864,\n",
       " 101376,\n",
       " 101888,\n",
       " 102400,\n",
       " 102912,\n",
       " 103424,\n",
       " 103936,\n",
       " 104448,\n",
       " 104960,\n",
       " 105472,\n",
       " 105984,\n",
       " 106496,\n",
       " 107008,\n",
       " 107520,\n",
       " 108032,\n",
       " 108544,\n",
       " 109056,\n",
       " 109568,\n",
       " 110080,\n",
       " 110592,\n",
       " 111104,\n",
       " 111616,\n",
       " 112128,\n",
       " 112640,\n",
       " 113152,\n",
       " 113664,\n",
       " 114176,\n",
       " 114688,\n",
       " 115200,\n",
       " 115712,\n",
       " 116224,\n",
       " 116736,\n",
       " 117248,\n",
       " 117760,\n",
       " 118272,\n",
       " 118784,\n",
       " 119296,\n",
       " 119808,\n",
       " 120320,\n",
       " 120832,\n",
       " 121344,\n",
       " 121856,\n",
       " 122368,\n",
       " 122880,\n",
       " 123392,\n",
       " 123904,\n",
       " 124416,\n",
       " 124928,\n",
       " 125440,\n",
       " 125952,\n",
       " 126464,\n",
       " 126976,\n",
       " 127488,\n",
       " 128000,\n",
       " 128512,\n",
       " 129024,\n",
       " 129536,\n",
       " 130048,\n",
       " 130560,\n",
       " 131072,\n",
       " 131584,\n",
       " 132096,\n",
       " 132608,\n",
       " 133120,\n",
       " 133632,\n",
       " 134144,\n",
       " 134656,\n",
       " 135168,\n",
       " 135680,\n",
       " 136192,\n",
       " 136704,\n",
       " 137216,\n",
       " 137728,\n",
       " 138240,\n",
       " 138752,\n",
       " 139264,\n",
       " 139776,\n",
       " 140288,\n",
       " 140800,\n",
       " 141312,\n",
       " 141824,\n",
       " 142336,\n",
       " 142848,\n",
       " 143360,\n",
       " 143872,\n",
       " 144384,\n",
       " 144896,\n",
       " 145408,\n",
       " 145920,\n",
       " 146432,\n",
       " 146944,\n",
       " 147456,\n",
       " 147968,\n",
       " 148480,\n",
       " 148992,\n",
       " 149504,\n",
       " 150016,\n",
       " 150528,\n",
       " 151040,\n",
       " 151552,\n",
       " 152064,\n",
       " 152576,\n",
       " 153088,\n",
       " 153600,\n",
       " 154112,\n",
       " 154624,\n",
       " 155136,\n",
       " 155648,\n",
       " 156160,\n",
       " 156672,\n",
       " 157184,\n",
       " 157696,\n",
       " 158208,\n",
       " 158720,\n",
       " 159232,\n",
       " 159744,\n",
       " 160256,\n",
       " 160768,\n",
       " 161280,\n",
       " 161792,\n",
       " 162304,\n",
       " 162816,\n",
       " 163328,\n",
       " 163840,\n",
       " 164352,\n",
       " 164864,\n",
       " 165376,\n",
       " 165888,\n",
       " 166400,\n",
       " 166912,\n",
       " 167424,\n",
       " 167936,\n",
       " 168448,\n",
       " 168960,\n",
       " 169472,\n",
       " 169984,\n",
       " 170496,\n",
       " 171008,\n",
       " 171520,\n",
       " 172032,\n",
       " 172544,\n",
       " 173056,\n",
       " 173568,\n",
       " 174080,\n",
       " 174592,\n",
       " 175104,\n",
       " 175616,\n",
       " 176128,\n",
       " 176640,\n",
       " 177152,\n",
       " 177664,\n",
       " 178176,\n",
       " 178688,\n",
       " 179200,\n",
       " 179712,\n",
       " 180224,\n",
       " 180736,\n",
       " 181248,\n",
       " 181760,\n",
       " 182272,\n",
       " 182784,\n",
       " 183296,\n",
       " 183808,\n",
       " 184320,\n",
       " 184832,\n",
       " 185344,\n",
       " 185856,\n",
       " 186368,\n",
       " 186880,\n",
       " 187392,\n",
       " 187904,\n",
       " 188416,\n",
       " 188928,\n",
       " 189440,\n",
       " 189952,\n",
       " 190464,\n",
       " 190976,\n",
       " 191488,\n",
       " 192000,\n",
       " 192512,\n",
       " 193024,\n",
       " 193536,\n",
       " 194048,\n",
       " 194560,\n",
       " 195072,\n",
       " 195584,\n",
       " 196096,\n",
       " 196608,\n",
       " 197120,\n",
       " 197632,\n",
       " 198144,\n",
       " 198656,\n",
       " 199168,\n",
       " 199680,\n",
       " 200192,\n",
       " 200704,\n",
       " 201216,\n",
       " 201728,\n",
       " 202240,\n",
       " 202752,\n",
       " 203264,\n",
       " 203776,\n",
       " 204288,\n",
       " 204800,\n",
       " 205312,\n",
       " 205824,\n",
       " 206336,\n",
       " 206848,\n",
       " 207360,\n",
       " 207872,\n",
       " 208384,\n",
       " 208896,\n",
       " 209408,\n",
       " 209920,\n",
       " 210432,\n",
       " 210944,\n",
       " 211456,\n",
       " 211968,\n",
       " 212480,\n",
       " 212992,\n",
       " 213504,\n",
       " 214016,\n",
       " 214528,\n",
       " 215040,\n",
       " 215552,\n",
       " 216064,\n",
       " 216576,\n",
       " 217088,\n",
       " 217600,\n",
       " 218112,\n",
       " 218624,\n",
       " 219136,\n",
       " 219648,\n",
       " 220160,\n",
       " 220672,\n",
       " 221184,\n",
       " 221696,\n",
       " 222208,\n",
       " 222720,\n",
       " 223232,\n",
       " 223744,\n",
       " 224256,\n",
       " 224768,\n",
       " 225280,\n",
       " 225792,\n",
       " 226304,\n",
       " 226816,\n",
       " 227328,\n",
       " 227840,\n",
       " 228352,\n",
       " 228864,\n",
       " 229376,\n",
       " 229888,\n",
       " 230400,\n",
       " 230912,\n",
       " 231424,\n",
       " 231936,\n",
       " 232448,\n",
       " 232960,\n",
       " 233472,\n",
       " 233984,\n",
       " 234496,\n",
       " 235008,\n",
       " 235520,\n",
       " 236032,\n",
       " 236544,\n",
       " 237056,\n",
       " 237568,\n",
       " 238080,\n",
       " 238592,\n",
       " 239104,\n",
       " 239616,\n",
       " 240128,\n",
       " 240640,\n",
       " 241152,\n",
       " 241664,\n",
       " 242176,\n",
       " 242688,\n",
       " 243200,\n",
       " 243712,\n",
       " 244224,\n",
       " 244736,\n",
       " 245248,\n",
       " 245760,\n",
       " 246272,\n",
       " 246784,\n",
       " 247296,\n",
       " 247808,\n",
       " 248320,\n",
       " 248832,\n",
       " 249344,\n",
       " 249856,\n",
       " 250368,\n",
       " 250880,\n",
       " 251392,\n",
       " 251904,\n",
       " 252416,\n",
       " 252928,\n",
       " 253440,\n",
       " 253952,\n",
       " 254464,\n",
       " 254976,\n",
       " 255488,\n",
       " 256000,\n",
       " 256512,\n",
       " 257024,\n",
       " 257536,\n",
       " 258048,\n",
       " 258560,\n",
       " 259072,\n",
       " 259584,\n",
       " 260096,\n",
       " 260608,\n",
       " 261120,\n",
       " 261632,\n",
       " 262144,\n",
       " 262656,\n",
       " 263168,\n",
       " 263680,\n",
       " 264192,\n",
       " 264704,\n",
       " 265216,\n",
       " 265728,\n",
       " 266240,\n",
       " 266752,\n",
       " 267264,\n",
       " 267776,\n",
       " 268288,\n",
       " 268800,\n",
       " 269312,\n",
       " 269824,\n",
       " 270336,\n",
       " 270848,\n",
       " 271360,\n",
       " 271872,\n",
       " 272384,\n",
       " 272896,\n",
       " 273408,\n",
       " 273920,\n",
       " 274432,\n",
       " 274944,\n",
       " 275456,\n",
       " 275968,\n",
       " 276480,\n",
       " 276992,\n",
       " 277504,\n",
       " 278016,\n",
       " 278528,\n",
       " 279040,\n",
       " 279552,\n",
       " 280064,\n",
       " 280576,\n",
       " 281088,\n",
       " 281600,\n",
       " 282112,\n",
       " 282624,\n",
       " 283136,\n",
       " 283648,\n",
       " 284160,\n",
       " 284672,\n",
       " 285184,\n",
       " 285696,\n",
       " 286208,\n",
       " 286720,\n",
       " 287232]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, seq_len, stride))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ff24451-62b6-4b99-8d58-60aa5992e781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88576"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a3ef181-1074-4d46-b774-2d568906aeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings.input_ids.size(1) // 512"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
