{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a8de98-8b27-44fc-92e1-a8a2ef30dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "df = pd.read_csv(\"train_MK_books.csv\")\n",
    "\n",
    "train = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df5ef613-165c-4011-80a5-20eba84dfd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4212\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59fefc05-04b7-4ed9-816e-43b8dae9b445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 400\n",
       " }),\n",
       " array([ 860, 3772, 3092,  466, 3444, 3171, 2919,  130, 1685,  769, 2391,\n",
       "        2433, 1184, 3385, 4117, 2904,  474, 1082, 2558, 2047, 2747,  975,\n",
       "        1806,  189, 2734, 3005, 1899, 1267, 1528, 3202, 3556, 3890,  646,\n",
       "        2888, 2435,  600, 2363, 2061,  241, 2041, 2824, 2612, 1363, 1478,\n",
       "        2556,  775, 4014,   34, 3152, 1955, 1585, 3943, 3073, 1021, 3461,\n",
       "        2613, 3843, 1500,  161, 1981,  995, 3342, 3798, 1275, 1016,  337,\n",
       "         878, 1076, 3993,  379,  492, 2062, 3884,   64, 2568, 2027, 2695,\n",
       "        1495,  391, 3561, 2278, 3099,  200, 3104, 2454, 3645,  804, 2731,\n",
       "        2773, 1570, 2690, 3840, 1028,  502,  870,  206, 1484,  863, 2790,\n",
       "         563, 4191, 1757, 1678, 3242, 1059, 1722, 3314, 3157, 2625, 2729,\n",
       "        1597, 3060, 2693, 3627, 1363, 1981, 1663, 1529, 2038, 3302, 2237,\n",
       "        1306, 4029, 2675, 1282,  709, 3748,  663, 1998, 3445, 3743, 1495,\n",
       "        3304, 3763, 1853, 1291, 3581, 3457, 1636, 3696, 2999, 3152,  698,\n",
       "        2160, 4097,  854, 3474, 1707, 2777, 1733, 3510,  202, 3255,  766,\n",
       "        2327, 2931,  197, 1930, 3582,  608, 3272, 1147, 3397, 2511, 1794,\n",
       "         659, 2811, 1369, 1986,  146, 3219, 2911, 1734, 1843,  488, 2976,\n",
       "        1959, 2385, 2919, 1802, 4061, 3369,  262,  623, 1016, 3643, 2049,\n",
       "        3723, 3108, 4090, 2056, 3170, 1682, 2255, 1154, 3191, 1696, 3991,\n",
       "        1648, 1445, 4199, 3923,  253, 3183, 2557,   98, 2200, 2961, 3777,\n",
       "        2869, 3234, 3791,  956, 4067,  160, 1300, 3987, 1527, 1218, 3104,\n",
       "        2735,  954, 3446, 1045, 1693, 3436,  562, 2485, 3354,  225,  959,\n",
       "        3067,  815, 3120,   16, 1243, 2205, 2524, 1069, 3444, 2565, 1060,\n",
       "        2327, 3420,  301,  606, 3170,  699, 1139,  190, 2300,  980, 2975,\n",
       "        1184, 3327, 3394,  127, 2065, 1816,  569, 1895, 2733, 3863, 4014,\n",
       "        2901, 1686, 3009,  154, 2537, 1409, 4057,  784, 3175, 2806,  537,\n",
       "        1841, 2455,  524, 2950, 4131, 4115, 2368, 3847, 2701,  971, 1166,\n",
       "        2774, 1982,  853, 4146, 3736, 3769,  574, 3261, 1148, 1715, 3854,\n",
       "        2491, 2660, 3124, 3051,    4,  876,  991, 4142,  635, 1693, 1682,\n",
       "         119,  830, 3675, 1369,  740,  116, 2838, 1150, 1931, 1012, 1664,\n",
       "        1081,  512, 2849,  559, 3672, 3343, 4070, 1470,  580, 2294, 3147,\n",
       "        1679, 3506, 2744, 3356, 4045, 1243, 3118, 2141, 3059, 2959, 2265,\n",
       "        2479, 1635, 1757, 2070, 2849, 3251, 3730, 2489, 1631, 2816, 1015,\n",
       "        1348, 3087, 2839,  335,  895, 3546,  851, 1687, 1931, 3746, 2427,\n",
       "        1568, 4000, 2738, 3498, 3913, 1066, 2444, 4107, 3373, 3969, 2546,\n",
       "         753, 2082, 2774, 2896, 4185,  775, 1625, 3069,  417,  114,  616,\n",
       "        3974, 3267,  825, 1516, 3146,  412,  728, 3348, 2808,  675, 3465,\n",
       "        2504, 2967, 2786, 1122]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "rand_ixs = np.random.randint(0, len(train), 400)\n",
    "sample_test = train.select(rand_ixs)\n",
    "sample_test, rand_ixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3a7001a-fea0-438c-83df-f9b6c7babd6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b072b1c7a4343a99a95924867763b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b623dd70cf3140d595dd487cc670c453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87e9994914c474f86a0ca43142b1ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0208f818db8845ca8a5152d27ba10262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bfebd5959847f3aa1787249e40f3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc28ef5158094291a08d66e6e3de30aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86df2583f775461d814e4eba33affeae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "305a8101-9fdc-4907-97aa-e2066e38da22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4358\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da4b63c5-6a77-43ba-a240-98926a827b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29de6341-05fb-4320-bd77-fc141f8ba64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n = Robert Boulter = \\n\\n\\n\\n\\n Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy \\'s Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . \\n\\n\\n In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 2006 episode of the television series , Doctors , followed by a role in the 2007 theatre production of How to Curse directed by Josie Rourke . How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham . Boulter starred in two films in 2008 , Daylight Robbery by filmmaker Paris Leonti , and Donkey Punch directed by Olly Blackburn . In May 2008 , Boulter made a guest appearance on a two @-@ part episode arc of the television series Waking the Dead , followed by an appearance on the television series Survivors in November 2008 . He had a recurring role in ten episodes of the television series Casualty in 2010 , as \" Kieron Fletcher \" . Boulter starred in the 2011 film Mercenaries directed by Paris Leonti . \\n\\n\\n\\n\\n = = Career = = \\n\\n\\n\\n\\n\\n\\n = = = 2000 – 2005 = = = \\n\\n\\n\\n\\n In 2000 Boulter had a guest @-@ starring role on the television series The Bill ; he portrayed \" Scott Parry \" in the episode , \" In Safe Hands \" . Boulter starred as \" Scott \" in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . A review of Boulter \\'s performance in The Independent on Sunday described him as \" horribly menacing \" in the role , and he received critical reviews in The Herald , and Evening Standard . He appeared in the television series Judge John Deed in 2002 as \" Addem Armitage \" in the episode \" Political Expediency \" , and had a role as a different character \" Toby Steele \" on The Bill . \\n\\n\\n He had a recurring role in 2003 on two episodes of The Bill , as character \" Connor Price \" . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy \\'s Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . Boulter starred as \" Darren \" , in the 2005 theatre productions of the Philip Ridley play Mercury Fur . It was performed at the Drum Theatre in Plymouth , and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . Boulter received a favorable review in The Daily Telegraph : \" The acting is shatteringly intense , with wired performances from Ben Whishaw ( now unrecognisable from his performance as Trevor Nunn \\'s Hamlet ) , Robert Boulter , Shane Zaza and Fraser Ayres . \" The Guardian noted , \" Ben Whishaw and Robert Boulter offer tenderness amid the savagery . \" \\n\\n\\n\\n\\n = = = 2006 – present = = = \\n\\n\\n\\n\\n In 2006 Boulter starred in the play Citizenship written by Mark Ravenhill . The play was part of a series which featured different playwrights , titled Burn / Chatroom / Citizenship . In a 2006 interview , fellow actor Ben Whishaw identified Boulter as one of his favorite co @-@ stars : \" I loved working with a guy called Robert Boulter , who was in the triple bill of Burn , Chatroom and Citizenship at the National . He played my brother in Mercury Fur . \" He portrayed \" Jason Tyler \" on the 2006 episode of the television series , Doctors , titled \" Something I Ate \" . Boulter starred as \" William \" in the 2007 production of How to Curse directed by Josie Rourke . How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham . In a review of the production for The Daily Telegraph , theatre critic Charles Spencer noted , \" Robert Boulter brings a touching vulnerability to the stage as William . \" \\n\\n\\n Boulter starred in two films in 2008 , Daylight Robbery by filmmaker Paris Leonti , and Donkey Punch directed by Olly Blackburn . Boulter portrayed a character named \" Sean \" in Donkey Punch , who tags along with character \" Josh \" as the \" quiet brother ... who hits it off with Tammi \" . Boulter guest starred on a two @-@ part episode arc \" Wounds \" in May 2008 of the television series Waking the Dead as character \" Jimmy Dearden \" . He appeared on the television series Survivors as \" Neil \" in November 2008 . He had a recurring role in ten episodes of the television series Casualty in 2010 , as \" Kieron Fletcher \" . He portrayed an emergency physician applying for a medical fellowship . He commented on the inherent difficulties in portraying a physician on television : \" Playing a doctor is a strange experience . Pretending you know what you \\'re talking about when you don \\'t is very bizarre but there are advisers on set who are fantastic at taking you through procedures and giving you the confidence to stand there and look like you know what you \\'re doing . \" Boulter starred in the 2011 film Mercenaries directed by Paris Leonti . \\n\\n\\n\\n\\n = = Filmography = = \\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\n\\n\".join(test[\"text\"][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2a5bcf60-dadd-4eb9-aa68-bed205cf429b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aribra/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee33c4a88a9419cb6e1551342def760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ff61b2f7dd4bbfacc10ffddce507c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce24bed4f0714cde80ad0288b9c036f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fd44a3fdd44ebe9cbfae9d711077c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/677 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c661e54c2a024ebd9de5c486a873b374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792527a93b67450aaa4cfeaaf379af5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95f990a3ab743b49c1fa0696decfec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747926e19758440f804bfbca4cb4d00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39eb3ac03c841f7b146b9bfb9a41df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757e284918f04bd7ac95746f94e536b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aribra/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219b681c92664168b33cb90e29b0fcec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data:  Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 400\n",
      "}) [ 860 3772 3092  466 3444 3171 2919  130 1685  769 2391 2433 1184 3385\n",
      " 4117 2904  474 1082 2558 2047 2747  975 1806  189 2734 3005 1899 1267\n",
      " 1528 3202 3556 3890  646 2888 2435  600 2363 2061  241 2041 2824 2612\n",
      " 1363 1478 2556  775 4014   34 3152 1955 1585 3943 3073 1021 3461 2613\n",
      " 3843 1500  161 1981  995 3342 3798 1275 1016  337  878 1076 3993  379\n",
      "  492 2062 3884   64 2568 2027 2695 1495  391 3561 2278 3099  200 3104\n",
      " 2454 3645  804 2731 2773 1570 2690 3840 1028  502  870  206 1484  863\n",
      " 2790  563 4191 1757 1678 3242 1059 1722 3314 3157 2625 2729 1597 3060\n",
      " 2693 3627 1363 1981 1663 1529 2038 3302 2237 1306 4029 2675 1282  709\n",
      " 3748  663 1998 3445 3743 1495 3304 3763 1853 1291 3581 3457 1636 3696\n",
      " 2999 3152  698 2160 4097  854 3474 1707 2777 1733 3510  202 3255  766\n",
      " 2327 2931  197 1930 3582  608 3272 1147 3397 2511 1794  659 2811 1369\n",
      " 1986  146 3219 2911 1734 1843  488 2976 1959 2385 2919 1802 4061 3369\n",
      "  262  623 1016 3643 2049 3723 3108 4090 2056 3170 1682 2255 1154 3191\n",
      " 1696 3991 1648 1445 4199 3923  253 3183 2557   98 2200 2961 3777 2869\n",
      " 3234 3791  956 4067  160 1300 3987 1527 1218 3104 2735  954 3446 1045\n",
      " 1693 3436  562 2485 3354  225  959 3067  815 3120   16 1243 2205 2524\n",
      " 1069 3444 2565 1060 2327 3420  301  606 3170  699 1139  190 2300  980\n",
      " 2975 1184 3327 3394  127 2065 1816  569 1895 2733 3863 4014 2901 1686\n",
      " 3009  154 2537 1409 4057  784 3175 2806  537 1841 2455  524 2950 4131\n",
      " 4115 2368 3847 2701  971 1166 2774 1982  853 4146 3736 3769  574 3261\n",
      " 1148 1715 3854 2491 2660 3124 3051    4  876  991 4142  635 1693 1682\n",
      "  119  830 3675 1369  740  116 2838 1150 1931 1012 1664 1081  512 2849\n",
      "  559 3672 3343 4070 1470  580 2294 3147 1679 3506 2744 3356 4045 1243\n",
      " 3118 2141 3059 2959 2265 2479 1635 1757 2070 2849 3251 3730 2489 1631\n",
      " 2816 1015 1348 3087 2839  335  895 3546  851 1687 1931 3746 2427 1568\n",
      " 4000 2738 3498 3913 1066 2444 4107 3373 3969 2546  753 2082 2774 2896\n",
      " 4185  775 1625 3069  417  114  616 3974 3267  825 1516 3146  412  728\n",
      " 3348 2808  675 3465 2504 2967 2786 1122]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from random import randrange\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"train_MK_books.csv\")\n",
    "\n",
    "train = Dataset.from_pandas(df)\n",
    "\n",
    "model_id = \"ilufy/meta-llama-2-7b-mk-physics-domain-tuned-2500\"\n",
    "\n",
    "# Get the type\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "# Load the pretrained model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map=\"auto\")\n",
    "\n",
    "\n",
    "# Select random indices for metric evaluation\n",
    "np.random.seed(42)\n",
    "rand_ixs = np.random.randint(0, len(train), 400)\n",
    "sample_test = train.select(rand_ixs)\n",
    "\n",
    "print(\"Test data: \", sample_test, rand_ixs)\n",
    "\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "max_length = model.config.max_length\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "device = \"cuda\"\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())\n",
    "\n",
    "print(\"[Preplexity]: ppl = \", ppl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abfcf689-a20b-4015-aa38-fa3780ad8490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "577f771a-caea-4c7c-bfeb-cd4f07c6901b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f42fe46-443a-4087-b4f3-a5e130ba64cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.19.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4840bab-1951-40d3-adb6-a366fbe17257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_dataset in module datasets.load:\n",
      "\n",
      "load_dataset(path: str, name: Optional[str] = None, data_dir: Optional[str] = None, data_files: Union[str, Sequence[str], Mapping[str, Union[str, Sequence[str]]], NoneType] = None, split: Union[str, datasets.splits.Split, NoneType] = None, cache_dir: Optional[str] = None, features: Optional[datasets.features.features.Features] = None, download_config: Optional[datasets.download.download_config.DownloadConfig] = None, download_mode: Union[datasets.download.download_manager.DownloadMode, str, NoneType] = None, verification_mode: Union[datasets.utils.info_utils.VerificationMode, str, NoneType] = None, ignore_verifications='deprecated', keep_in_memory: Optional[bool] = None, save_infos: bool = False, revision: Union[str, datasets.utils.version.Version, NoneType] = None, token: Union[bool, str, NoneType] = None, use_auth_token='deprecated', task='deprecated', streaming: bool = False, num_proc: Optional[int] = None, storage_options: Optional[Dict] = None, trust_remote_code: bool = None, **config_kwargs) -> Union[datasets.dataset_dict.DatasetDict, datasets.arrow_dataset.Dataset, datasets.dataset_dict.IterableDatasetDict, datasets.iterable_dataset.IterableDataset]\n",
      "    Load a dataset from the Hugging Face Hub, or a local dataset.\n",
      "    \n",
      "    You can find the list of datasets on the [Hub](https://huggingface.co/datasets) or with [`huggingface_hub.list_datasets`].\n",
      "    \n",
      "    A dataset is a directory that contains:\n",
      "    \n",
      "    - some data files in generic formats (JSON, CSV, Parquet, text, etc.).\n",
      "    - and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures.\n",
      "    \n",
      "    Note that dataset scripts can also download and read data files from anywhere - in case your data files already exist online.\n",
      "    \n",
      "    This function does the following under the hood:\n",
      "    \n",
      "        1. Download and import in the library the dataset script from `path` if it's not already cached inside the library.\n",
      "    \n",
      "            If the dataset has no dataset script, then a generic dataset script is imported instead (JSON, CSV, Parquet, text, etc.)\n",
      "    \n",
      "            Dataset scripts are small python scripts that define dataset builders. They define the citation, info and format of the dataset,\n",
      "            contain the path or URL to the original data files and the code to load examples from the original data files.\n",
      "    \n",
      "            You can find the complete list of datasets in the Datasets [Hub](https://huggingface.co/datasets).\n",
      "    \n",
      "        2. Run the dataset script which will:\n",
      "    \n",
      "            * Download the dataset file from the original URL (see the script) if it's not already available locally or cached.\n",
      "            * Process and cache the dataset in typed Arrow tables for caching.\n",
      "    \n",
      "                Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.\n",
      "                They can be directly accessed from disk, loaded in RAM or even streamed over the web.\n",
      "    \n",
      "        3. Return a dataset built from the requested splits in `split` (default: all).\n",
      "    \n",
      "    It also allows to load a dataset from a local directory or a dataset repository on the Hugging Face Hub without dataset script.\n",
      "    In this case, it automatically loads all the data files from the directory or the dataset repository.\n",
      "    \n",
      "    Args:\n",
      "    \n",
      "        path (`str`):\n",
      "            Path or name of the dataset.\n",
      "            Depending on `path`, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.\n",
      "    \n",
      "            For local datasets:\n",
      "    \n",
      "            - if `path` is a local directory (containing data files only)\n",
      "              -> load a generic dataset builder (csv, json, text etc.) based on the content of the directory\n",
      "              e.g. `'./path/to/directory/with/my/csv/data'`.\n",
      "            - if `path` is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory)\n",
      "              -> load the dataset builder from the dataset script\n",
      "              e.g. `'./dataset/squad'` or `'./dataset/squad/squad.py'`.\n",
      "    \n",
      "            For datasets on the Hugging Face Hub (list all available datasets with [`huggingface_hub.list_datasets`])\n",
      "    \n",
      "            - if `path` is a dataset repository on the HF hub (containing data files only)\n",
      "              -> load a generic dataset builder (csv, text etc.) based on the content of the repository\n",
      "              e.g. `'username/dataset_name'`, a dataset repository on the HF hub containing your data files.\n",
      "            - if `path` is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)\n",
      "              -> load the dataset builder from the dataset script in the dataset repository\n",
      "              e.g. `glue`, `squad`, `'username/dataset_name'`, a dataset repository on the HF hub containing a dataset script `'dataset_name.py'`.\n",
      "    \n",
      "        name (`str`, *optional*):\n",
      "            Defining the name of the dataset configuration.\n",
      "        data_dir (`str`, *optional*):\n",
      "            Defining the `data_dir` of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is `None`,\n",
      "            the behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.\n",
      "        data_files (`str` or `Sequence` or `Mapping`, *optional*):\n",
      "            Path(s) to source data file(s).\n",
      "        split (`Split` or `str`):\n",
      "            Which split of the data to load.\n",
      "            If `None`, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).\n",
      "            If given, will return a single Dataset.\n",
      "            Splits can be combined and specified like in tensorflow-datasets.\n",
      "        cache_dir (`str`, *optional*):\n",
      "            Directory to read/write data. Defaults to `\"~/.cache/huggingface/datasets\"`.\n",
      "        features (`Features`, *optional*):\n",
      "            Set the features type to use for this dataset.\n",
      "        download_config ([`DownloadConfig`], *optional*):\n",
      "            Specific download configuration parameters.\n",
      "        download_mode ([`DownloadMode`] or `str`, defaults to `REUSE_DATASET_IF_EXISTS`):\n",
      "            Download/generate mode.\n",
      "        verification_mode ([`VerificationMode`] or `str`, defaults to `BASIC_CHECKS`):\n",
      "            Verification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/...).\n",
      "    \n",
      "            <Added version=\"2.9.1\"/>\n",
      "        ignore_verifications (`bool`, defaults to `False`):\n",
      "            Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/...).\n",
      "    \n",
      "            <Deprecated version=\"2.9.1\">\n",
      "    \n",
      "            `ignore_verifications` was deprecated in version 2.9.1 and will be removed in 3.0.0.\n",
      "            Please use `verification_mode` instead.\n",
      "    \n",
      "            </Deprecated>\n",
      "        keep_in_memory (`bool`, defaults to `None`):\n",
      "            Whether to copy the dataset in-memory. If `None`, the dataset\n",
      "            will not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\n",
      "            nonzero. See more details in the [improve performance](../cache#improve-performance) section.\n",
      "        save_infos (`bool`, defaults to `False`):\n",
      "            Save the dataset information (checksums/size/splits/...).\n",
      "        revision ([`Version`] or `str`, *optional*):\n",
      "            Version of the dataset script to load.\n",
      "            As datasets have their own git repository on the Datasets Hub, the default version \"main\" corresponds to their \"main\" branch.\n",
      "            You can specify a different version than the default \"main\" by using a commit SHA or a git tag of the dataset repository.\n",
      "        token (`str` or `bool`, *optional*):\n",
      "            Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n",
      "            If `True`, or not specified, will get token from `\"~/.huggingface\"`.\n",
      "        use_auth_token (`str` or `bool`, *optional*):\n",
      "            Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n",
      "            If `True`, or not specified, will get token from `\"~/.huggingface\"`.\n",
      "    \n",
      "            <Deprecated version=\"2.14.0\">\n",
      "    \n",
      "            `use_auth_token` was deprecated in favor of `token` in version 2.14.0 and will be removed in 3.0.0.\n",
      "    \n",
      "            </Deprecated>\n",
      "        task (`str`):\n",
      "            The task to prepare the dataset for during training and evaluation. Casts the dataset's [`Features`] to standardized column names and types as detailed in `datasets.tasks`.\n",
      "    \n",
      "            <Deprecated version=\"2.13.0\">\n",
      "    \n",
      "            `task` was deprecated in version 2.13.0 and will be removed in 3.0.0.\n",
      "    \n",
      "            </Deprecated>\n",
      "        streaming (`bool`, defaults to `False`):\n",
      "            If set to `True`, don't download the data files. Instead, it streams the data progressively while\n",
      "            iterating on the dataset. An [`IterableDataset`] or [`IterableDatasetDict`] is returned instead in this case.\n",
      "    \n",
      "            Note that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.\n",
      "            Json files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats\n",
      "            like rar and xz are not yet supported. The tgz format doesn't allow streaming.\n",
      "        num_proc (`int`, *optional*, defaults to `None`):\n",
      "            Number of processes when downloading and generating the dataset locally.\n",
      "            Multiprocessing is disabled by default.\n",
      "    \n",
      "            <Added version=\"2.7.0\"/>\n",
      "        storage_options (`dict`, *optional*, defaults to `None`):\n",
      "            **Experimental**. Key/value pairs to be passed on to the dataset file-system backend, if any.\n",
      "    \n",
      "            <Added version=\"2.11.0\"/>\n",
      "        trust_remote_code (`bool`, defaults to `True`):\n",
      "            Whether or not to allow for datasets defined on the Hub using a dataset script. This option\n",
      "            should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
      "            execute code present on the Hub on your local machine.\n",
      "    \n",
      "            <Tip warning={true}>\n",
      "    \n",
      "            `trust_remote_code` will default to False in the next major release.\n",
      "    \n",
      "            </Tip>\n",
      "    \n",
      "            <Added version=\"2.16.0\"/>\n",
      "        **config_kwargs (additional keyword arguments):\n",
      "            Keyword arguments to be passed to the `BuilderConfig`\n",
      "            and used in the [`DatasetBuilder`].\n",
      "    \n",
      "    Returns:\n",
      "        [`Dataset`] or [`DatasetDict`]:\n",
      "        - if `split` is not `None`: the dataset requested,\n",
      "        - if `split` is `None`, a [`~datasets.DatasetDict`] with each split.\n",
      "    \n",
      "        or [`IterableDataset`] or [`IterableDatasetDict`]: if `streaming=True`\n",
      "    \n",
      "        - if `split` is not `None`, the dataset is requested\n",
      "        - if `split` is `None`, a [`~datasets.streaming.IterableDatasetDict`] with each split.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    Load a dataset from the Hugging Face Hub:\n",
      "    \n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('rotten_tomatoes', split='train')\n",
      "    \n",
      "    # Map data files to splits\n",
      "    >>> data_files = {'train': 'train.csv', 'test': 'test.csv'}\n",
      "    >>> ds = load_dataset('namespace/your_dataset_name', data_files=data_files)\n",
      "    ```\n",
      "    \n",
      "    Load a local dataset:\n",
      "    \n",
      "    ```py\n",
      "    # Load a CSV file\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')\n",
      "    \n",
      "    # Load a JSON file\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('json', data_files='path/to/local/my_dataset.json')\n",
      "    \n",
      "    # Load from a local loading script\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train')\n",
      "    ```\n",
      "    \n",
      "    Load an [`~datasets.IterableDataset`]:\n",
      "    \n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('rotten_tomatoes', split='train', streaming=True)\n",
      "    ```\n",
      "    \n",
      "    Load an image dataset with the `ImageFolder` dataset builder:\n",
      "    \n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datasets.load_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "282167c9-a6ba-4129-8be9-e2c8e182c51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 29871,    13,  ...,    13,    13,    13]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9910a9d1-6f1c-46be-b340-1b5e40950254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671bf639-0462-4c43-a6bc-52de560c3a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47be62a-d488-4134-b413-e4c0d4a30d10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
