{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6076,
     "status": "ok",
     "timestamp": 1715742445741,
     "user": {
      "displayName": "Ibrahim Abedrabbo",
      "userId": "05855272535580949954"
     },
     "user_tz": -180
    },
    "id": "kiqYGs6L8OqH",
    "outputId": "31e3b0b8-f4b2-4303-844d-28b0dc4256c2"
   },
   "outputs": [],
   "source": [
    "# !pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1715742458212,
     "user": {
      "displayName": "Ibrahim Abedrabbo",
      "userId": "05855272535580949954"
     },
     "user_tz": -180
    },
    "id": "uGOrPjHRN8J8",
    "outputId": "d72d014b-0a82-4d69-d0ee-196c728126db"
   },
   "outputs": [],
   "source": [
    "# ! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4788,
     "status": "ok",
     "timestamp": 1715744930402,
     "user": {
      "displayName": "Ibrahim Abedrabbo",
      "userId": "05855272535580949954"
     },
     "user_tz": -180
    },
    "id": "SAKDMqmRNKUs",
    "outputId": "2727b641-326a-4e23-d989-7010de1e070d"
   },
   "outputs": [],
   "source": [
    "# !pip3 install tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4702,
     "status": "ok",
     "timestamp": 1715744568581,
     "user": {
      "displayName": "Ibrahim Abedrabbo",
      "userId": "05855272535580949954"
     },
     "user_tz": -180
    },
    "id": "giyAOFK0WFmb",
    "outputId": "a281202f-4c75-43cc-f824-e2b971173c1c"
   },
   "outputs": [],
   "source": [
    "# ! pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1715748292365,
     "user": {
      "displayName": "Ibrahim Abedrabbo",
      "userId": "05855272535580949954"
     },
     "user_tz": -180
    },
    "id": "mI-WJS5RNKUt"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1715742506644,
     "user": {
      "displayName": "Ibrahim Abedrabbo",
      "userId": "05855272535580949954"
     },
     "user_tz": -180
    },
    "id": "5alov-jANKUu",
    "outputId": "b1b7a06f-a0b1-40d4-ac48-7c43659b4dcc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.87s/it]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Get the type\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "hf_model_repo = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Get the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_repo)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_repo,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "K2Jy4eTmNKUv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "QnA_Generator-coaxnn-paper.ipynb\tfinetuning_qna_paper\n",
      "all_coaxnn_paper_responsez_1st_run.txt\tpretraining_paper\n",
      "coaxnn_paper_qna_Ref_train.csv\t\ttrain_coaxnn_paper.csv\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dFv6Bi-VNKUv"
   },
   "outputs": [],
   "source": [
    "all_csvs = [\n",
    "    \"data/train_coaxnn_paper.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1715748591647,
     "user": {
      "displayName": "Ibrahim Abedrabbo",
      "userId": "05855272535580949954"
     },
     "user_tz": -180
    },
    "id": "NFO0YOm8NKUv"
   },
   "outputs": [],
   "source": [
    "# all_responsez_file = open(\"all_coaxnn_paper_responsez_1st_run.txt\", 'w')\n",
    "# all_responsez = {\"text\": [], \"response\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper_text_chunks:  94\n"
     ]
    }
   ],
   "source": [
    "csvf = \"data/train_coaxnn_paper.csv\"\n",
    "text_chunks = pd.read_csv(csvf)\n",
    "\n",
    "paper_text_chunks = text_chunks['text'][:94]\n",
    "refs_text_chunks = text_chunks['text'][95:]\n",
    "\n",
    "# paper_text_chunks = paper_text_chunks[-5:]\n",
    "\n",
    "print(\"paper_text_chunks: \", len(paper_text_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27156403,
     "status": "ok",
     "timestamp": 1715775860611,
     "user": {
      "displayName": "Ibrahim Abedrabbo",
      "userId": "05855272535580949954"
     },
     "user_tz": -180
    },
    "id": "Dmo-tOb4NKUw",
    "outputId": "079e04fd-50a6-4fbc-b98e-9cb5d3261bc9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csvf =  train_coaxnn_paper.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing_train_coaxnn_paper.csv:   0%|          | 0/94 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/aribra/miniconda3/envs/llama2/lib/python3.12/site-packages/transformers/generation/utils.py:1659: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "processing_train_coaxnn_paper.csv: 100%|██████████| 94/94 [16:13<00:00, 10.35s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# for csvf in all_csvs:\n",
    "\n",
    "\n",
    "print(\"csvf = \", csvf)\n",
    "\n",
    "ixcounter = -1\n",
    "\n",
    "for txtch in tqdm(paper_text_chunks, f\"processing_{csvf}\"):\n",
    "\n",
    "    txtch = txtch.replace('-\\n','')\n",
    "\n",
    "    # prompt = \\\n",
    "    # f\"\"\"\n",
    "    # Instruction: Understand the below text and generate one question and answer pair.\\n\n",
    "    # Input:\\n\n",
    "    # {txtch}\\n\n",
    "    # Output:\\n\n",
    "    # \"\"\"\n",
    "\n",
    "    prompt = \\\n",
    "    f\"\"\"\n",
    "    Instruction: Generate three question and answer pairs for the below text. All questions must start with \"Question\". All answers must start wtih \"Answer\"\\n\n",
    "    Input:\\n\n",
    "    {txtch}\\n\n",
    "    Output:\\n\n",
    "    \"\"\"\n",
    "\n",
    "    # print(prompt)\n",
    "\n",
    "    # print(\"\\n\\n\")\n",
    "\n",
    "    # Generate response\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids\n",
    "    outputs = model.generate(input_ids=input_ids,\n",
    "                            max_new_tokens=200,\n",
    "                            temperature=0.6)\n",
    "\n",
    "    # gen_tokens; exclude input tokens from the final decoded output\n",
    "    gen_tokens = outputs[:, input_ids.shape[1]:]\n",
    "    response = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "    all_responsez[\"text\"].append(txtch)\n",
    "    all_responsez[\"response\"].append(response)\n",
    "    \n",
    "    all_responsez_file.write(f\"{response}\\n++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "    all_responsez_file.flush()\n",
    "\n",
    "    # (running locally on RTX 2060) cool donw GPU before next run\n",
    "    # time.sleep(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists():\n",
    "    qna_df_responsez = pd.DataFrame(all_responsez)\n",
    "    qna_df_responsez.to_csv(\"data/coaxnn_paper_qna_all_responsez.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contents lists available at ScienceDirect\\n\\nJ...</td>\n",
       "      <td>Question 1: What is the main contribution of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Efficient neural networks\\nModel approximation...</td>\n",
       "      <td>Question 1: What are the approximate strategie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>propose a novel model optimization framework, ...</td>\n",
       "      <td>Question: What is the main goal of the propose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>up to 1.53× speedup while reducing energy by u...</td>\n",
       "      <td>Question 1: What is the main goal of the text?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to the limited resources.\\n\\nMany efforts have...</td>\n",
       "      <td>Question 1: What are the efforts made to enabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>ability to classify simple images. Besides, si...</td>\n",
       "      <td>Question 1: What is the main advantage of usin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>significantly reducing computation costs for n...</td>\n",
       "      <td>Question 1: What are the benefits of GA-based ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Therefore, the runtime overhead of the GA is n...</td>\n",
       "      <td>Question: What is the main advantage of CoAxNN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>requirements of intelligent tasks. Moreover, t...</td>\n",
       "      <td>Question 1: What are the requirements of intel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>ent layers have different sensitives for model...</td>\n",
       "      <td>Question: What are the different sensitives fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   Contents lists available at ScienceDirect\\n\\nJ...   \n",
       "1   Efficient neural networks\\nModel approximation...   \n",
       "2   propose a novel model optimization framework, ...   \n",
       "3   up to 1.53× speedup while reducing energy by u...   \n",
       "4   to the limited resources.\\n\\nMany efforts have...   \n",
       "..                                                ...   \n",
       "89  ability to classify simple images. Besides, si...   \n",
       "90  significantly reducing computation costs for n...   \n",
       "91  Therefore, the runtime overhead of the GA is n...   \n",
       "92  requirements of intelligent tasks. Moreover, t...   \n",
       "93  ent layers have different sensitives for model...   \n",
       "\n",
       "                                             response  \n",
       "0   Question 1: What is the main contribution of t...  \n",
       "1   Question 1: What are the approximate strategie...  \n",
       "2   Question: What is the main goal of the propose...  \n",
       "3   Question 1: What is the main goal of the text?...  \n",
       "4   Question 1: What are the efforts made to enabl...  \n",
       "..                                                ...  \n",
       "89  Question 1: What is the main advantage of usin...  \n",
       "90  Question 1: What are the benefits of GA-based ...  \n",
       "91  Question: What is the main advantage of CoAxNN...  \n",
       "92  Question 1: What are the requirements of intel...  \n",
       "93  Question: What are the different sensitives fo...  \n",
       "\n",
       "[94 rows x 2 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df_responsez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Question: What is the purpose of removing unimportant filters in the text?\n",
      "Answer: The purpose of removing unimportant filters is to reduce computational costs.\n",
      "\n",
      "    2. Question: What is the next step after evaluating the fitness of the corresponding individuals according to the accuracy and latency of the compressed multi-stage model?\n",
      "Answer: The next step is to update the chromosome pool, generating the next generation of individuals.\n",
      "\n",
      "    3. Question: What is the general approach of executing a neural network model?\n",
      "Answer: The general approach of executing a neural network model is a one-staged approach, which processes all the inputs in the same manner, starting from the input operator and performing it operator by operator until the final exit operator.\n"
     ]
    }
   ],
   "source": [
    "print(qna_df_responsez.iloc[30]['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_QA_pairs(qa_df):\n",
    "    pairs = {\"Question\": [], \"Answer\": []}\n",
    "    \n",
    "    pattern = re.compile(r\"Question\\s*\\d*:\\s*(.*?)\\s*Answer\\s*\\d*:\\s*(.*?)(?=\\n\\s*\\d*\\.\\s*Question|\\n\\s*Question|\\Z)\", re.DOTALL)\n",
    "\n",
    "    for qa_response in qa_df['response']:\n",
    "\n",
    "        matches = pattern.findall(qa_response)\n",
    "\n",
    "        for question, answer in matches:\n",
    "            # print(f\"Question: {question.strip()}\")\n",
    "            if answer:\n",
    "                # print(f\"Answer: {answer.strip()}\")\n",
    "                pairs[\"Question\"].append(question)\n",
    "                pairs[\"Answer\"].append(answer)\n",
    "            else:\n",
    "                print(\"skipped: \", question)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = get_QA_pairs(qna_df_responsez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the main contribution of the paper?',\n",
       " 'What are the keywords associated with this article?',\n",
       " 'What is the main goal of the paper?',\n",
       " 'What are the approximate strategies used in the study?',\n",
       " 'What is the purpose of combining different approximate strategies?',\n",
       " 'What is the proposed model optimization framework called?',\n",
       " 'What is the main goal of the proposed model optimization framework?',\n",
       " 'What are the principles of different approximate optimizations used in the proposed approach?',\n",
       " 'What is the effectiveness of CoAxNN according to the experimental results?',\n",
       " 'What is the main goal of the text?',\n",
       " 'What are the limitations of current deep learning models?',\n",
       " 'What are the potential benefits of on-device deep learning?',\n",
       " 'What are the efforts made to enable efficient on-device deep learning?',\n",
       " 'What are the challenges in combining different optimization strategies?',\n",
       " 'What are the approximate strategies based on distinct principles?',\n",
       " 'What is the main aim of this paper?',\n",
       " 'What is the key issue in combining different strategies?',\n",
       " 'Who are the corresponding authors of this paper?',\n",
       " 'Who contributed equally to this work?',\n",
       " 'When was this work received in revised form?',\n",
       " 'What is the novel neural network optimization framework presented in this paper?',\n",
       " 'What is the main idea of the text?',\n",
       " 'What are the two main optimization techniques used in CoAxNN?',\n",
       " 'What is the key novelty of CoAxNN compared to previous work?',\n",
       " 'What are the optimization techniques studied in the paper?',\n",
       " 'What is the main contribution of the paper?',\n",
       " 'How does the framework construct the design space?',\n",
       " 'What is the main goal of CoAxNN?',\n",
       " 'What does the phrase \"trivial accuracy loss\" mean in the context of the paper?',\n",
       " 'What is the main contribution of the paper?',\n",
       " 'What is the main idea of the text?',\n",
       " 'What are the details of the optimization framework described in Section 3?',\n",
       " 'What is the purpose of the experimental evaluation in Section 4?',\n",
       " 'What are the two methods of pruning used in the text?',\n",
       " 'What is the purpose of pruning according to the text?',\n",
       " 'What are two approaches used for pruning according to the text?',\n",
       " 'What are the main challenges in optimizing the OBD method?',\n",
       " 'What are unstructured sparse models?',\n",
       " 'What are filter pruning methods?',\n",
       " 'What is the main focus of much recent work in deep learning?',\n",
       " 'What is the purpose of dynamically pruning filters in a soft manner?',\n",
       " \"What is the goal of Li et al.'s fusioncatalyzed filter pruning approach?\",\n",
       " 'What is the goal of the hardware-aware pruning method introduced by Plochaet et al.?',\n",
       " 'What is the importance criterion proposed by Zhuang et al. to evaluate the importance of filters in structured pruning?',\n",
       " 'Can pruning be useful as an architecture search paradigm?',\n",
       " 'What is the main contribution of Li et al. [15]?',\n",
       " 'What is the proposed method in Li et al. [16]?',\n",
       " 'What is the main contribution of Ding et al. [17]?',\n",
       " 'What technique do the authors adopt to realize practical performance improvement for neural network models?',\n",
       " 'What are staging-based approximation techniques?',\n",
       " \"What is the main idea of Teerapittayanon et al.'s work?\",\n",
       " 'What is adaptive computing?',\n",
       " 'What did Teerapittayanon et al. demonstrate?',\n",
       " 'What did Fang et al. present?',\n",
       " 'What is the main contribution of the paper by Li et al.?',\n",
       " 'What did Figurnov et al. study in their paper?',\n",
       " 'What is the main idea behind the earlyexiting method proposed by Farhadi et al.?',\n",
       " 'What does the text say about the exit position of the sample in the multi-branch network?',\n",
       " 'What is the purpose of the low-cost early exit network proposed by Jo et al.?',\n",
       " 'What is the main contribution of the paper?',\n",
       " 'What is the main problem addressed in the text?',\n",
       " 'How did Panda et al. [19] and Teerapittayanon et al. [18] set the location and threshold for each exit in the conditional neural network model?',\n",
       " 'How did Jayakodi et al. [24] find the best thresholds for the specified trade-off between accuracy and energy consumption of inference?',\n",
       " 'What is the main contribution of Park et al. [28] in the field of DSE?',\n",
       " \"What is the key idea of Li et al. [11]'s proposed method for filter pruning?\",\n",
       " \"What is the main advantage of Qian et al. [30]'s proposed method for DSE?\",\n",
       " 'What is the main goal of the paper?',\n",
       " 'What is the focus of the pruning-based approximate strategy?',\n",
       " 'What is the main objective of the staging-based approximate strategy?',\n",
       " 'What are the configuration parameters considered in the staging-based approximate strategy?',\n",
       " 'How does the staging-based approximate strategy improve the execution speed of the model?',\n",
       " 'How does the combination of different approximate strategies affect the effect of the model optimization?',\n",
       " 'What is shown in Fig. 1?',\n",
       " 'What do the triples (𝑥, 𝑦, 𝑧) represent in Fig. 1?',\n",
       " 'What is the relationship between the number of stages and the computational cost shown in Fig. 1?',\n",
       " 'What is the relationship between the number of stages and the computational cost in the context of the text?',\n",
       " 'How does the staging-based optimization affect the computational cost of a model?',\n",
       " 'What can be observed from the partial experimental results in Fig. 1?',\n",
       " 'What does the paper conclude about the optimization effect of different configuration parameters?',\n",
       " 'What does the paper observe in Fig. 1(a)?',\n",
       " 'What does the paper show in Fig. 2?',\n",
       " 'What is the main focus of the paper?',\n",
       " 'What is the normalized computational cost of the staging-based optimization strategy?',\n",
       " 'What is the computational cost of CoAxNN?',\n",
       " 'What is the computational cost of CoAxNN?',\n",
       " 'What is the pruning rate of CoAxNN?',\n",
       " 'What is the normalized computational cost of CoAxNN?',\n",
       " 'What does CoAxNN perform according to the genes of the chromosome for each individual?',\n",
       " 'What does CoAxNN attach to the original model?',\n",
       " 'What does CoAxNN predict according to the threshold of each stage?',\n",
       " 'What is the purpose of removing unimportant filters in the text?',\n",
       " 'What is the next step after evaluating the fitness of the corresponding individuals according to the accuracy and latency of the compressed multi-stage model?',\n",
       " 'What is the general approach of executing a neural network model?',\n",
       " 'What is the purpose of the early exiting strategy in CoAxNN?',\n",
       " 'What does the notation \\ue23a ∗ represent in CoAxNN?',\n",
       " 'What is the purpose of the number of stages 𝜏 in CoAxNN?',\n",
       " 'What is the main difference between the original neural network model and the approximate model with the staging-based strategy?',\n",
       " 'What is the purpose of the exit checker in the staging-based strategy?',\n",
       " 'What does 𝜏 represent in the given text?',\n",
       " 'What is the significance of 𝑐𝑖 in the given text?',\n",
       " 'What is the meaning of \\ue23f in the given text?',\n",
       " 'What is the main problem with exit branches in deep learning?',\n",
       " 'What is the confidence threshold used for in deep learning?',\n",
       " 'What is the structure of each exit branch in deep learning?',\n",
       " 'What are the different types of operators used for feature extraction in the text?',\n",
       " 'What is the purpose of the 𝑓 ∗\\n𝑏𝑖\\noperator in the text?',\n",
       " 'What is the difference in the configuration and complexity of the intermediate feature maps for different depths of the main neural networks?',\n",
       " 'What are the two factors that will affect each other in the early-exiting method?',\n",
       " 'What is the purpose of putting the availability of each stage into the design space of the GA in CoAxNN?',\n",
       " 'What is the main problem that CoAxNN addresses in the text?',\n",
       " 'How does CoAxNN choose to attach exit branches?',\n",
       " 'What is the purpose of introducing a feature extractor and a linear classifier for each exit branch?',\n",
       " 'What is the purpose of introducing a feature extractor and a linear classifier in CoAxNN?',\n",
       " 'What is the main problem with linear classifiers?',\n",
       " 'What is the purpose of adding an extra pooling operator in CoAxNN?',\n",
       " 'What is the design of 𝑖 in CoAxNN?',\n",
       " 'What is the purpose of using entropy as the entropy-aware activation operator in the CoAxNN model?',\n",
       " 'What is the formula for calculating the entropy of the predicted probability distribution in the CoAxNN model?',\n",
       " 'What is the main advantage of using pruning-based approximate optimization in CoAxNN?',\n",
       " 'What is the difference between structured and unstructured pruning in the neural network pruning technique?',\n",
       " 'What is the importance of each filter in a convolutional operator based on the 𝓁2-norm?',\n",
       " 'What is the purpose of pruning filters in CoAxNN?',\n",
       " 'How does the filter pruning method work in CoAxNN?',\n",
       " 'What is the dynamic pruning scheme used in CoAxNN?',\n",
       " 'What is the purpose of utilizing a dynamic pruning scheme for staging-based approximate CNNs?',\n",
       " 'What is the purpose of joint training in the training process of neural network models with exit branches?',\n",
       " 'What is the main objective of CoAxNN?\\n    2.',\n",
       " 'What is the formula used to calculate the cross-entropy loss function in CoAxNN?\\n    4.',\n",
       " 'What is the purpose of CoAxNN?',\n",
       " 'What is the input to CoAxNN?',\n",
       " 'What is the output of CoAxNN?',\n",
       " 'What is the purpose of the dynamic pruning scheme in CoAxNN?\\n    📝',\n",
       " 'What is the purpose of the \"if P[p][i] is available then\" statement in the code?',\n",
       " 'What is the purpose of the \"𝑙𝑜𝑠𝑠\" variable in the code?',\n",
       " 'What is the purpose of the line \"𝑤𝑒𝑖𝑔ℎ𝑡𝑒𝑑_𝑙𝑜𝑠𝑠 = 0;\" in the code?',\n",
       " 'What is the search space for determining which stage is available in CoAxNN?',\n",
       " 'What is the search space for thresholds in CoAxNN?',\n",
       " 'What is the search space for pruning rate in CoAxNN?',\n",
       " 'What is the main idea of the text?',\n",
       " 'What is the purpose of the algorithm shown in Algorithm 2?',\n",
       " 'What is the output of the CoAxNN algorithm?',\n",
       " 'How is the prediction result obtained by CoAxNN?',\n",
       " 'What is the confidence threshold used by CoAxNN?',\n",
       " 'What is the accuracy function returned by CoAxNN?',\n",
       " 'How does CoAxNN evaluate the latency score?',\n",
       " 'How does GA-based DSE get the (near-)optimal solutions about the goal of accuracy and latency?',\n",
       " 'What is the output of CoAxNN?',\n",
       " 'What does the text highlight about the optimization process?',\n",
       " 'What is the purpose of removing unimportant filters in the optimization process?',\n",
       " 'What is the significance of the accuracy requirement in the optimization process?',\n",
       " 'What is the purpose of the Algorithm 2 in the given text?',\n",
       " 'What is the output of the Algorithm 2 in the given text?',\n",
       " 'What is the significance of the line \"if P[p][i] is available then\" in the given text?',\n",
       " 'What is the speedup of the optimized models compared to the original models on the Jetson AGX Orin platform?',\n",
       " 'What is the energy consumption of the optimized models compared to the original models on the Jetson AGX Orin platform?',\n",
       " 'What is the effectiveness of the proposed method on the CIFAR dataset?',\n",
       " 'What are the two datasets that are part of the CIFAR dataset?',\n",
       " 'What is the number of classes in the CIFAR-10 dataset?',\n",
       " 'What is the number of images in the CINIC-10 dataset?',\n",
       " 'What is the main goal of the GA-based DSE in the text?',\n",
       " 'What is the data argumentation strategy used in the GA-based DSE?',\n",
       " 'What is the main conclusion that can be drawn from the results shown in Fig. 4?',\n",
       " 'What is CoAxNN?',\n",
       " 'What is the purpose of using GA-based DSE in most cases?',\n",
       " 'What is the meaning of \"ACC. Drop\" in the context of the text?',\n",
       " 'What does the \"ACC. Drop\" value represent in the text?',\n",
       " 'What does a smaller \"ACC. Drop\" value indicate in the text?',\n",
       " 'What does the term \"overfitting\" refer to in the text?',\n",
       " 'How does CoAxNN reduce the computational complexity of the ResNet-20 model while maintaining its accuracy?',\n",
       " 'How does CoAxNN compare to other state-of-the-art model optimization methods in terms of computational complexity and accuracy?',\n",
       " 'What is the computational cost of the optimized ResNet-32 model using CoAxNN?',\n",
       " 'What does CoAxNN reduce the computational cost of?',\n",
       " 'What is the accuracy drop of CoAxNN?',\n",
       " 'How does CoAxNN achieve a similar accuracy loss while reducing computational complexity?',\n",
       " 'What does the text say about the computational complexity of existing methods compared to CoAxNN?',\n",
       " 'What is the difference in accuracy between the baseline and accelerated models in Table 1?',\n",
       " 'How does CoAxNN perform compared to other state-of-the-art methods in Table 1?',\n",
       " 'What is the top-1 accuracy of the ResNet-56 model on the MNIST dataset?',\n",
       " 'What is the FLOPs count of the CoAxNN model on the CIFAR-10 dataset?',\n",
       " 'What is the top-1 accuracy of the SFP model on the TAS dataset?',\n",
       " 'What is the accuracy loss of ResNet-20?',\n",
       " 'What is the weighted average FLOPs of ResNet-32?',\n",
       " 'How many stages are used in CoAxNN for ResNet-110?',\n",
       " 'What does CoAxNN employ?',\n",
       " 'What are the two stages used for?',\n",
       " 'How many stages are used for more complex models?',\n",
       " 'What is the pruning rate for the optimized ResNet-20 model?',\n",
       " 'What is the number of stages in the optimized ResNet-20 model?',\n",
       " 'What is the position of the first stage in the optimized ResNet-20 model?',\n",
       " 'What is the pruning rate of ResNet-32?',\n",
       " 'What is the position of the threshold for ResNet-56?',\n",
       " 'What is the threshold of ResNet-110 for the last stage?',\n",
       " 'What is the position of three stages in the optimized ResNet-56?',\n",
       " 'What is the position of three stages in the optimized ResNet-110?',\n",
       " 'How does CoAxNN perform on the CIFAR-100 dataset?',\n",
       " 'What is the computation reduction achieved by CoAxNN?',\n",
       " 'What is the accuracy loss of the optimized model with CoAxNN?',\n",
       " 'What is the computational complexity of the optimized model with CoAxNN?',\n",
       " 'What is the number of images predicted by the first few stages of the optimized model on CIFAR-100?',\n",
       " 'What is the percentage of images predicted by the first few stages of the optimized model on CIFAR-10?',\n",
       " 'What is the computational cost of the original ResNet-18 model?',\n",
       " 'How much did the accuracy of the ResNet-18 model decrease when the FLOPs were reduced from 5.49E8 to 2.21E8?',\n",
       " 'How much did the computational complexity of CoAxNN reduce while achieving a 0.50% accuracy gain for the ResNet-18 model?',\n",
       " 'What is the computational complexity reduction achieved by CoAxNN?',\n",
       " 'How does CoAxNN improve the top-1 accuracy of the ResNet-50 model?',\n",
       " 'What is the computational complexity reduction achieved by FPC?',\n",
       " 'What is the accuracy drop of the ResNet-18 and ResNet-50 in Table 8?',\n",
       " 'What is the number of stages used by the ResNet-18 and ResNet-50 in Table 9?',\n",
       " 'What is the pruning rate of the ResNet-18 in Table 9?',\n",
       " 'What are the top-1 accuracy percentages for the optimized neural network models on CIFAR-100?',\n",
       " 'What is the percentage drop in FLOPs for the optimized models compared to the baseline model?',\n",
       " 'What is the percentage of accuracy drop for the ResNet-56 model with the optimal configuration found by CoAxNN?',\n",
       " 'What is the number of FLOPs for the ResNet-110 model with the optimal configuration found by CoAxNN?',\n",
       " 'How does CoAxNN compare to state-of-the-art methods?',\n",
       " 'What is the main advantage of using staging-based approximate strategies?',\n",
       " 'What is the main disadvantage of pruning-based approximate strategies?',\n",
       " 'What is the main goal of the paper?',\n",
       " 'What is the problem with the pruning method?',\n",
       " 'What is the advantage of CoAxNN compared to other methods?',\n",
       " 'What is the average inference latency for CoAxNN on the CIFAR10 dataset?',\n",
       " 'What is the speedup of CoAxNN for the ResNet-20 model on the CIFAR10 dataset?',\n",
       " 'What is the largest model accelerated by CoAxNN in the table?',\n",
       " 'What is the percentage of accuracy loss achieved by CoAxNN on the CIFAR-10 dataset?',\n",
       " 'What is the speedup achieved by CoAxNN on different models?',\n",
       " 'How much energy consumption is reduced by CoAxNN on different models?',\n",
       " 'What percentage of accuracy loss did CoAxNN reduce on the CIFAR-10 dataset?',\n",
       " 'What percentage of energy consumption did CoAxNN reduce on the ResNet-20, ResNet-32, ResNet-56, and ResNet-110 models?',\n",
       " 'What is the difference in execution latency between the baseline models and the optimized models in the study?',\n",
       " 'What is the reason for the increase in execution latency in the optimized models, according to the study?',\n",
       " \"What is the implication of the study's findings regarding the use of filter pruning for neural network acceleration?\",\n",
       " 'What is the percentage of FLOPs reduction for the ResNet-18 model with the optimal configuration?',\n",
       " 'What is the percentage of FLOPs reduction for the ResNet-50 model with the optimal configuration?',\n",
       " 'What is the number of FLOPs for the ResNet-18 model with the optimal configuration?',\n",
       " 'What is the energy reduction of the optimized ResNet-20 model?',\n",
       " 'What is the energy reduction of the optimized ResNet-32 model?',\n",
       " 'What is the energy reduction of the optimized ResNet-56 model?',\n",
       " 'What does \"CoAxNN-ACT\" denote in the given text?',\n",
       " 'What is the critical motivation of CoAxNN?',\n",
       " 'What does the ablation study in Fig. 5 show?',\n",
       " 'What is the main conclusion that can be drawn from the visualization results in Fig. 5?',\n",
       " 'How does the CoAxNN-ACT model differ from the CoAxNN-ALL model?',\n",
       " 'What can be inferred from the visualization results in Fig. 6?',\n",
       " 'What is the main advantage of using CoAxNN?',\n",
       " 'What is the main disadvantage of using the baseline model?',\n",
       " 'What is the purpose of collecting the latency of each operator in the profiling phase?',\n",
       " 'What are the benefits of GA-based DSE?',\n",
       " 'What are the overheads of GA-based DSE?',\n",
       " 'Can CoAxNN be used for other intelligent tasks?',\n",
       " 'What is the main advantage of CoAxNN?',\n",
       " 'Can CoAxNN be applied to other intelligent tasks?',\n",
       " 'What is the advantage of using CoAxNN?',\n",
       " 'What are the requirements of intelligent tasks?',\n",
       " 'What are the limitations of CoAxNN?',\n",
       " 'What future studies will be explored in CoAxNN?',\n",
       " 'What are the different sensitives for model accuracy?',\n",
       " 'What will be explored in future studies?',\n",
       " 'What is the conclusion of the paper?']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pairs['Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257, 257)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_pairs['Question']), len(qa_pairs['Answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_dfb = pd.DataFrame(qa_pairs)\n",
    "# qna_dfb.to_csv(\"coaxnn_paper_qna_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the main contribution of the paper?</td>\n",
       "      <td>The paper proposes a new method for optimizing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the keywords associated with this art...</td>\n",
       "      <td>The keywords associated with this article are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the main goal of the paper?</td>\n",
       "      <td>The main goal of the paper is to develop a new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the approximate strategies used in th...</td>\n",
       "      <td>Neural network pruning.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the purpose of combining different app...</td>\n",
       "      <td>To improve the performance of on-device infere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>What are the limitations of CoAxNN?</td>\n",
       "      <td>The limitations of CoAxNN include the use of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>What future studies will be explored in CoAxNN?</td>\n",
       "      <td>Future studies in CoAxNN will include explorin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>What are the different sensitives for model ac...</td>\n",
       "      <td>Different layers have different sensitives for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>What will be explored in future studies?</td>\n",
       "      <td>Setting different pruning ratios for different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>What is the conclusion of the paper?</td>\n",
       "      <td>In this paper, we proposed an efficient optimi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Question  \\\n",
       "0          What is the main contribution of the paper?   \n",
       "1    What are the keywords associated with this art...   \n",
       "2                  What is the main goal of the paper?   \n",
       "3    What are the approximate strategies used in th...   \n",
       "4    What is the purpose of combining different app...   \n",
       "..                                                 ...   \n",
       "252                What are the limitations of CoAxNN?   \n",
       "253    What future studies will be explored in CoAxNN?   \n",
       "254  What are the different sensitives for model ac...   \n",
       "255           What will be explored in future studies?   \n",
       "256               What is the conclusion of the paper?   \n",
       "\n",
       "                                                Answer  \n",
       "0    The paper proposes a new method for optimizing...  \n",
       "1    The keywords associated with this article are ...  \n",
       "2    The main goal of the paper is to develop a new...  \n",
       "3                              Neural network pruning.  \n",
       "4    To improve the performance of on-device infere...  \n",
       "..                                                 ...  \n",
       "252  The limitations of CoAxNN include the use of a...  \n",
       "253  Future studies in CoAxNN will include explorin...  \n",
       "254  Different layers have different sensitives for...  \n",
       "255  Setting different pruning ratios for different...  \n",
       "256  In this paper, we proposed an efficient optimi...  \n",
       "\n",
       "[257 rows x 2 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_dfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_qa_pairs = {\"Question\":[], \"Answer\":[]}\n",
    "\n",
    "for ix, ref in enumerate(refs_text_chunks.values):\n",
    "    q = f\"What is reference [{ix+1}]?\"\n",
    "    refs_qa_pairs[\"Question\"].append(q)\n",
    "    refs_qa_pairs[\"Answer\"].append(ref)\n",
    "\n",
    "ref_qna_df = pd.DataFrame(refs_qa_pairs)\n",
    "# ref_qna_df.to_csv(\"coaxnn_paper_qna_Ref_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paper text and references Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_train_coaxnn_train_DF = pd.concat([qna_dfb, ref_qna_df])\n",
    "qna_train_coaxnn_train_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "F7eaH0Ig8Tsq"
   },
   "outputs": [],
   "source": [
    "if not os.path.exist(\"data/coaxnn_paper_qna_train.csv\"):\n",
    "    qna_train_coaxnn_train_DF.to_csv(\"data/coaxnn_paper_qna_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "2qNWNMgLNnPP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is reference [30]?'\n",
      " 'Reference [30]: Y. Qian, Z. He, Y. Wang, B. Wang, X. Ling, Z. Gu, H. Wang, S. Zeng, W. Swaileh,\\nHierarchical threshold pruning based on uniform response criterion, IEEE Trans.\\nNeural Netw. Learn. Syst. (2023).']\n"
     ]
    }
   ],
   "source": [
    "rand_ix = np.random.randint(len(qna_train_coaxnn_train_DF))\n",
    "print(qna_train_coaxnn_train_DF.iloc[rand_ix].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/coaxnn_paper_qna_train.csv\")\n",
    "\n",
    "df['text_question'] = 'Question:\\n' + df['Question']\n",
    "df['text_answer'] = 'Answer:\\n' + df['Answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The paper proposes a new method for optimizing on-device deep learning with conditional approximate neural networks.\n"
     ]
    }
   ],
   "source": [
    "print(df['text_answer'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 308/308 [00:00<00:00, 236862.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What is the main contribution of the paper?\n",
      "Question:\n",
      "What are the keywords associated with this article?\n",
      "Question:\n",
      "What is the main goal of the paper?\n",
      "Question:\n",
      "What are the approximate strategies used in the study?\n",
      "Question:\n",
      "What is the purpose of combining different approximate strategies?\n",
      "Question:\n",
      "What is the proposed model optimization framework called?\n",
      "Question:\n",
      "What is the main goal of the proposed model optimization framework?\n",
      "Question:\n",
      "What are the principles of different approximate optimizations used in the proposed approach?\n",
      "Question:\n",
      "What is the effectiveness of CoAxNN according to the experimental results?\n",
      "Question:\n",
      "What is the main goal of the text?\n",
      "Question:\n",
      "What are the limitations of current deep learning models?\n",
      "Question:\n",
      "What are the potential benefits of on-device deep learning?\n",
      "Question:\n",
      "What are the efforts made to enable efficient on-device deep learning?\n",
      "Question:\n",
      "What are the challenges in combining different optimization strategies?\n",
      "Question:\n",
      "What are the approximate strategies based on distinct principles?\n",
      "Question:\n",
      "What is the main aim of this paper?\n",
      "Question:\n",
      "What is the key issue in combining different strategies?\n",
      "Question:\n",
      "Who are the corresponding authors of this paper?\n",
      "Question:\n",
      "Who contributed equally to this work?\n",
      "Question:\n",
      "When was this work received in revised form?\n",
      "Question:\n",
      "What is the novel neural network optimization framework presented in this paper?\n",
      "Question:\n",
      "What is the main idea of the text?\n",
      "Question:\n",
      "What are the two main optimization techniques used in CoAxNN?\n",
      "Question:\n",
      "What is the key novelty of CoAxNN compared to previous work?\n",
      "Question:\n",
      "What are the optimization techniques studied in the paper?\n",
      "Question:\n",
      "What is the main contribution of the paper?\n",
      "Question:\n",
      "How does the framework construct the design space?\n",
      "Question:\n",
      "What is the main goal of CoAxNN?\n",
      "Question:\n",
      "What does the phrase \"trivial accuracy loss\" mean in the context of the paper?\n",
      "Question:\n",
      "What is the main contribution of the paper?\n",
      "Question:\n",
      "What is the main idea of the text?\n",
      "Question:\n",
      "What are the details of the optimization framework described in Section 3?\n",
      "Question:\n",
      "What is the purpose of the experimental evaluation in Section 4?\n",
      "Question:\n",
      "What are the two methods of pruning used in the text?\n",
      "Question:\n",
      "What is the purpose of pruning according to the text?\n",
      "Question:\n",
      "What are two approaches used for pruning according to the text?\n",
      "Question:\n",
      "What are the main challenges in optimizing the OBD method?\n",
      "Question:\n",
      "What are unstructured sparse models?\n",
      "Question:\n",
      "What are filter pruning methods?\n",
      "Question:\n",
      "What is the main focus of much recent work in deep learning?\n",
      "Question:\n",
      "What is the purpose of dynamically pruning filters in a soft manner?\n",
      "Question:\n",
      "What is the goal of Li et al.'s fusioncatalyzed filter pruning approach?\n",
      "Question:\n",
      "What is the goal of the hardware-aware pruning method introduced by Plochaet et al.?\n",
      "Question:\n",
      "What is the importance criterion proposed by Zhuang et al. to evaluate the importance of filters in structured pruning?\n",
      "Question:\n",
      "Can pruning be useful as an architecture search paradigm?\n",
      "Question:\n",
      "What is the main contribution of Li et al. [15]?\n",
      "Question:\n",
      "What is the proposed method in Li et al. [16]?\n",
      "Question:\n",
      "What is the main contribution of Ding et al. [17]?\n",
      "Question:\n",
      "What technique do the authors adopt to realize practical performance improvement for neural network models?\n",
      "Question:\n",
      "What are staging-based approximation techniques?\n",
      "Question:\n",
      "What is the main idea of Teerapittayanon et al.'s work?\n",
      "Question:\n",
      "What is adaptive computing?\n",
      "Question:\n",
      "What did Teerapittayanon et al. demonstrate?\n",
      "Question:\n",
      "What did Fang et al. present?\n",
      "Question:\n",
      "What is the main contribution of the paper by Li et al.?\n",
      "Question:\n",
      "What did Figurnov et al. study in their paper?\n",
      "Question:\n",
      "What is the main idea behind the earlyexiting method proposed by Farhadi et al.?\n",
      "Question:\n",
      "What does the text say about the exit position of the sample in the multi-branch network?\n",
      "Question:\n",
      "What is the purpose of the low-cost early exit network proposed by Jo et al.?\n",
      "Question:\n",
      "What is the main contribution of the paper?\n",
      "Question:\n",
      "What is the main problem addressed in the text?\n",
      "Question:\n",
      "How did Panda et al. [19] and Teerapittayanon et al. [18] set the location and threshold for each exit in the conditional neural network model?\n",
      "Question:\n",
      "How did Jayakodi et al. [24] find the best thresholds for the specified trade-off between accuracy and energy consumption of inference?\n",
      "Question:\n",
      "What is the main contribution of Park et al. [28] in the field of DSE?\n",
      "Question:\n",
      "What is the key idea of Li et al. [11]'s proposed method for filter pruning?\n",
      "Question:\n",
      "What is the main advantage of Qian et al. [30]'s proposed method for DSE?\n",
      "Question:\n",
      "What is the main goal of the paper?\n",
      "Question:\n",
      "What is the focus of the pruning-based approximate strategy?\n",
      "Question:\n",
      "What is the main objective of the staging-based approximate strategy?\n",
      "Question:\n",
      "What are the configuration parameters considered in the staging-based approximate strategy?\n",
      "Question:\n",
      "How does the staging-based approximate strategy improve the execution speed of the model?\n",
      "Question:\n",
      "How does the combination of different approximate strategies affect the effect of the model optimization?\n",
      "Question:\n",
      "What is shown in Fig. 1?\n",
      "Question:\n",
      "What do the triples (𝑥, 𝑦, 𝑧) represent in Fig. 1?\n",
      "Question:\n",
      "What is the relationship between the number of stages and the computational cost shown in Fig. 1?\n",
      "Question:\n",
      "What is the relationship between the number of stages and the computational cost in the context of the text?\n",
      "Question:\n",
      "How does the staging-based optimization affect the computational cost of a model?\n",
      "Question:\n",
      "What can be observed from the partial experimental results in Fig. 1?\n",
      "Question:\n",
      "What does the paper conclude about the optimization effect of different configuration parameters?\n",
      "Question:\n",
      "What does the paper observe in Fig. 1(a)?\n",
      "Question:\n",
      "What does the paper show in Fig. 2?\n",
      "Question:\n",
      "What is the main focus of the paper?\n",
      "Question:\n",
      "What is the normalized computational cost of the staging-based optimization strategy?\n",
      "Question:\n",
      "What is the computational cost of CoAxNN?\n",
      "Question:\n",
      "What is the computational cost of CoAxNN?\n",
      "Question:\n",
      "What is the pruning rate of CoAxNN?\n",
      "Question:\n",
      "What is the normalized computational cost of CoAxNN?\n",
      "Question:\n",
      "What does CoAxNN perform according to the genes of the chromosome for each individual?\n",
      "Question:\n",
      "What does CoAxNN attach to the original model?\n",
      "Question:\n",
      "What does CoAxNN predict according to the threshold of each stage?\n",
      "Question:\n",
      "What is the purpose of removing unimportant filters in the text?\n",
      "Question:\n",
      "What is the next step after evaluating the fitness of the corresponding individuals according to the accuracy and latency of the compressed multi-stage model?\n",
      "Question:\n",
      "What is the general approach of executing a neural network model?\n",
      "Question:\n",
      "What is the purpose of the early exiting strategy in CoAxNN?\n",
      "Question:\n",
      "What does the notation  ∗ represent in CoAxNN?\n",
      "Question:\n",
      "What is the purpose of the number of stages 𝜏 in CoAxNN?\n",
      "Question:\n",
      "What is the main difference between the original neural network model and the approximate model with the staging-based strategy?\n",
      "Question:\n",
      "What is the purpose of the exit checker in the staging-based strategy?\n",
      "Question:\n",
      "What does 𝜏 represent in the given text?\n",
      "Question:\n",
      "What is the significance of 𝑐𝑖 in the given text?\n",
      "Question:\n",
      "What is the meaning of  in the given text?\n",
      "Question:\n",
      "What is the main problem with exit branches in deep learning?\n",
      "Question:\n",
      "What is the confidence threshold used for in deep learning?\n",
      "Question:\n",
      "What is the structure of each exit branch in deep learning?\n",
      "Question:\n",
      "What are the different types of operators used for feature extraction in the text?\n",
      "Question:\n",
      "What is the purpose of the 𝑓 ∗\n",
      "𝑏𝑖\n",
      "operator in the text?\n",
      "Question:\n",
      "What is the difference in the configuration and complexity of the intermediate feature maps for different depths of the main neural networks?\n",
      "Question:\n",
      "What are the two factors that will affect each other in the early-exiting method?\n",
      "Question:\n",
      "What is the purpose of putting the availability of each stage into the design space of the GA in CoAxNN?\n",
      "Question:\n",
      "What is the main problem that CoAxNN addresses in the text?\n",
      "Question:\n",
      "How does CoAxNN choose to attach exit branches?\n",
      "Question:\n",
      "What is the purpose of introducing a feature extractor and a linear classifier for each exit branch?\n",
      "Question:\n",
      "What is the purpose of introducing a feature extractor and a linear classifier in CoAxNN?\n",
      "Question:\n",
      "What is the main problem with linear classifiers?\n",
      "Question:\n",
      "What is the purpose of adding an extra pooling operator in CoAxNN?\n",
      "Question:\n",
      "What is the design of 𝑖 in CoAxNN?\n",
      "Question:\n",
      "What is the purpose of using entropy as the entropy-aware activation operator in the CoAxNN model?\n",
      "Question:\n",
      "What is the formula for calculating the entropy of the predicted probability distribution in the CoAxNN model?\n",
      "Question:\n",
      "What is the main advantage of using pruning-based approximate optimization in CoAxNN?\n",
      "Question:\n",
      "What is the difference between structured and unstructured pruning in the neural network pruning technique?\n",
      "Question:\n",
      "What is the importance of each filter in a convolutional operator based on the 𝓁2-norm?\n",
      "Question:\n",
      "What is the purpose of pruning filters in CoAxNN?\n",
      "Question:\n",
      "How does the filter pruning method work in CoAxNN?\n",
      "Question:\n",
      "What is the dynamic pruning scheme used in CoAxNN?\n",
      "Question:\n",
      "What is the purpose of utilizing a dynamic pruning scheme for staging-based approximate CNNs?\n",
      "Question:\n",
      "What is the purpose of joint training in the training process of neural network models with exit branches?\n",
      "Question:\n",
      "What is the main objective of CoAxNN?\n",
      "    2.\n",
      "Question:\n",
      "What is the formula used to calculate the cross-entropy loss function in CoAxNN?\n",
      "    4.\n",
      "Question:\n",
      "What is the purpose of CoAxNN?\n",
      "Question:\n",
      "What is the input to CoAxNN?\n",
      "Question:\n",
      "What is the output of CoAxNN?\n",
      "Question:\n",
      "What is the purpose of the dynamic pruning scheme in CoAxNN?\n",
      "    📝\n",
      "Question:\n",
      "What is the purpose of the \"if P[p][i] is available then\" statement in the code?\n",
      "Question:\n",
      "What is the purpose of the \"𝑙𝑜𝑠𝑠\" variable in the code?\n",
      "Question:\n",
      "What is the purpose of the line \"𝑤𝑒𝑖𝑔ℎ𝑡𝑒𝑑_𝑙𝑜𝑠𝑠 = 0;\" in the code?\n",
      "Question:\n",
      "What is the search space for determining which stage is available in CoAxNN?\n",
      "Question:\n",
      "What is the search space for thresholds in CoAxNN?\n",
      "Question:\n",
      "What is the search space for pruning rate in CoAxNN?\n",
      "Question:\n",
      "What is the main idea of the text?\n",
      "Question:\n",
      "What is the purpose of the algorithm shown in Algorithm 2?\n",
      "Question:\n",
      "What is the output of the CoAxNN algorithm?\n",
      "Question:\n",
      "How is the prediction result obtained by CoAxNN?\n",
      "Question:\n",
      "What is the confidence threshold used by CoAxNN?\n",
      "Question:\n",
      "What is the accuracy function returned by CoAxNN?\n",
      "Question:\n",
      "How does CoAxNN evaluate the latency score?\n",
      "Question:\n",
      "How does GA-based DSE get the (near-)optimal solutions about the goal of accuracy and latency?\n",
      "Question:\n",
      "What is the output of CoAxNN?\n",
      "Question:\n",
      "What does the text highlight about the optimization process?\n",
      "Question:\n",
      "What is the purpose of removing unimportant filters in the optimization process?\n",
      "Question:\n",
      "What is the significance of the accuracy requirement in the optimization process?\n",
      "Question:\n",
      "What is the purpose of the Algorithm 2 in the given text?\n",
      "Question:\n",
      "What is the output of the Algorithm 2 in the given text?\n",
      "Question:\n",
      "What is the significance of the line \"if P[p][i] is available then\" in the given text?\n",
      "Question:\n",
      "What is the speedup of the optimized models compared to the original models on the Jetson AGX Orin platform?\n",
      "Question:\n",
      "What is the energy consumption of the optimized models compared to the original models on the Jetson AGX Orin platform?\n",
      "Question:\n",
      "What is the effectiveness of the proposed method on the CIFAR dataset?\n",
      "Question:\n",
      "What are the two datasets that are part of the CIFAR dataset?\n",
      "Question:\n",
      "What is the number of classes in the CIFAR-10 dataset?\n",
      "Question:\n",
      "What is the number of images in the CINIC-10 dataset?\n",
      "Question:\n",
      "What is the main goal of the GA-based DSE in the text?\n",
      "Question:\n",
      "What is the data argumentation strategy used in the GA-based DSE?\n",
      "Question:\n",
      "What is the main conclusion that can be drawn from the results shown in Fig. 4?\n",
      "Question:\n",
      "What is CoAxNN?\n",
      "Question:\n",
      "What is the purpose of using GA-based DSE in most cases?\n",
      "Question:\n",
      "What is the meaning of \"ACC. Drop\" in the context of the text?\n",
      "Question:\n",
      "What does the \"ACC. Drop\" value represent in the text?\n",
      "Question:\n",
      "What does a smaller \"ACC. Drop\" value indicate in the text?\n",
      "Question:\n",
      "What does the term \"overfitting\" refer to in the text?\n",
      "Question:\n",
      "How does CoAxNN reduce the computational complexity of the ResNet-20 model while maintaining its accuracy?\n",
      "Question:\n",
      "How does CoAxNN compare to other state-of-the-art model optimization methods in terms of computational complexity and accuracy?\n",
      "Question:\n",
      "What is the computational cost of the optimized ResNet-32 model using CoAxNN?\n",
      "Question:\n",
      "What does CoAxNN reduce the computational cost of?\n",
      "Question:\n",
      "What is the accuracy drop of CoAxNN?\n",
      "Question:\n",
      "How does CoAxNN achieve a similar accuracy loss while reducing computational complexity?\n",
      "Question:\n",
      "What does the text say about the computational complexity of existing methods compared to CoAxNN?\n",
      "Question:\n",
      "What is the difference in accuracy between the baseline and accelerated models in Table 1?\n",
      "Question:\n",
      "How does CoAxNN perform compared to other state-of-the-art methods in Table 1?\n",
      "Question:\n",
      "What is the top-1 accuracy of the ResNet-56 model on the MNIST dataset?\n",
      "Question:\n",
      "What is the FLOPs count of the CoAxNN model on the CIFAR-10 dataset?\n",
      "Question:\n",
      "What is the top-1 accuracy of the SFP model on the TAS dataset?\n",
      "Question:\n",
      "What is the accuracy loss of ResNet-20?\n",
      "Question:\n",
      "What is the weighted average FLOPs of ResNet-32?\n",
      "Question:\n",
      "How many stages are used in CoAxNN for ResNet-110?\n",
      "Question:\n",
      "What does CoAxNN employ?\n",
      "Question:\n",
      "What are the two stages used for?\n",
      "Question:\n",
      "How many stages are used for more complex models?\n",
      "Question:\n",
      "What is the pruning rate for the optimized ResNet-20 model?\n",
      "Question:\n",
      "What is the number of stages in the optimized ResNet-20 model?\n",
      "Question:\n",
      "What is the position of the first stage in the optimized ResNet-20 model?\n",
      "Question:\n",
      "What is the pruning rate of ResNet-32?\n",
      "Question:\n",
      "What is the position of the threshold for ResNet-56?\n",
      "Question:\n",
      "What is the threshold of ResNet-110 for the last stage?\n",
      "Question:\n",
      "What is the position of three stages in the optimized ResNet-56?\n",
      "Question:\n",
      "What is the position of three stages in the optimized ResNet-110?\n",
      "Question:\n",
      "How does CoAxNN perform on the CIFAR-100 dataset?\n",
      "Question:\n",
      "What is the computation reduction achieved by CoAxNN?\n",
      "Question:\n",
      "What is the accuracy loss of the optimized model with CoAxNN?\n",
      "Question:\n",
      "What is the computational complexity of the optimized model with CoAxNN?\n",
      "Question:\n",
      "What is the number of images predicted by the first few stages of the optimized model on CIFAR-100?\n",
      "Question:\n",
      "What is the percentage of images predicted by the first few stages of the optimized model on CIFAR-10?\n",
      "Question:\n",
      "What is the computational cost of the original ResNet-18 model?\n",
      "Question:\n",
      "How much did the accuracy of the ResNet-18 model decrease when the FLOPs were reduced from 5.49E8 to 2.21E8?\n",
      "Question:\n",
      "How much did the computational complexity of CoAxNN reduce while achieving a 0.50% accuracy gain for the ResNet-18 model?\n",
      "Question:\n",
      "What is the computational complexity reduction achieved by CoAxNN?\n",
      "Question:\n",
      "How does CoAxNN improve the top-1 accuracy of the ResNet-50 model?\n",
      "Question:\n",
      "What is the computational complexity reduction achieved by FPC?\n",
      "Question:\n",
      "What is the accuracy drop of the ResNet-18 and ResNet-50 in Table 8?\n",
      "Question:\n",
      "What is the number of stages used by the ResNet-18 and ResNet-50 in Table 9?\n",
      "Question:\n",
      "What is the pruning rate of the ResNet-18 in Table 9?\n",
      "Question:\n",
      "What are the top-1 accuracy percentages for the optimized neural network models on CIFAR-100?\n",
      "Question:\n",
      "What is the percentage drop in FLOPs for the optimized models compared to the baseline model?\n",
      "Question:\n",
      "What is the percentage of accuracy drop for the ResNet-56 model with the optimal configuration found by CoAxNN?\n",
      "Question:\n",
      "What is the number of FLOPs for the ResNet-110 model with the optimal configuration found by CoAxNN?\n",
      "Question:\n",
      "How does CoAxNN compare to state-of-the-art methods?\n",
      "Question:\n",
      "What is the main advantage of using staging-based approximate strategies?\n",
      "Question:\n",
      "What is the main disadvantage of pruning-based approximate strategies?\n",
      "Question:\n",
      "What is the main goal of the paper?\n",
      "Question:\n",
      "What is the problem with the pruning method?\n",
      "Question:\n",
      "What is the advantage of CoAxNN compared to other methods?\n",
      "Question:\n",
      "What is the average inference latency for CoAxNN on the CIFAR10 dataset?\n",
      "Question:\n",
      "What is the speedup of CoAxNN for the ResNet-20 model on the CIFAR10 dataset?\n",
      "Question:\n",
      "What is the largest model accelerated by CoAxNN in the table?\n",
      "Question:\n",
      "What is the percentage of accuracy loss achieved by CoAxNN on the CIFAR-10 dataset?\n",
      "Question:\n",
      "What is the speedup achieved by CoAxNN on different models?\n",
      "Question:\n",
      "How much energy consumption is reduced by CoAxNN on different models?\n",
      "Question:\n",
      "What percentage of accuracy loss did CoAxNN reduce on the CIFAR-10 dataset?\n",
      "Question:\n",
      "What percentage of energy consumption did CoAxNN reduce on the ResNet-20, ResNet-32, ResNet-56, and ResNet-110 models?\n",
      "Question:\n",
      "What is the difference in execution latency between the baseline models and the optimized models in the study?\n",
      "Question:\n",
      "What is the reason for the increase in execution latency in the optimized models, according to the study?\n",
      "Question:\n",
      "What is the implication of the study's findings regarding the use of filter pruning for neural network acceleration?\n",
      "Question:\n",
      "What is the percentage of FLOPs reduction for the ResNet-18 model with the optimal configuration?\n",
      "Question:\n",
      "What is the percentage of FLOPs reduction for the ResNet-50 model with the optimal configuration?\n",
      "Question:\n",
      "What is the number of FLOPs for the ResNet-18 model with the optimal configuration?\n",
      "Question:\n",
      "What is the energy reduction of the optimized ResNet-20 model?\n",
      "Question:\n",
      "What is the energy reduction of the optimized ResNet-32 model?\n",
      "Question:\n",
      "What is the energy reduction of the optimized ResNet-56 model?\n",
      "Question:\n",
      "What does \"CoAxNN-ACT\" denote in the given text?\n",
      "Question:\n",
      "What is the critical motivation of CoAxNN?\n",
      "Question:\n",
      "What does the ablation study in Fig. 5 show?\n",
      "Question:\n",
      "What is the main conclusion that can be drawn from the visualization results in Fig. 5?\n",
      "Question:\n",
      "How does the CoAxNN-ACT model differ from the CoAxNN-ALL model?\n",
      "Question:\n",
      "What can be inferred from the visualization results in Fig. 6?\n",
      "Question:\n",
      "What is the main advantage of using CoAxNN?\n",
      "Question:\n",
      "What is the main disadvantage of using the baseline model?\n",
      "Question:\n",
      "What is the purpose of collecting the latency of each operator in the profiling phase?\n",
      "Question:\n",
      "What are the benefits of GA-based DSE?\n",
      "Question:\n",
      "What are the overheads of GA-based DSE?\n",
      "Question:\n",
      "Can CoAxNN be used for other intelligent tasks?\n",
      "Question:\n",
      "What is the main advantage of CoAxNN?\n",
      "Question:\n",
      "Can CoAxNN be applied to other intelligent tasks?\n",
      "Question:\n",
      "What is the advantage of using CoAxNN?\n",
      "Question:\n",
      "What are the requirements of intelligent tasks?\n",
      "Question:\n",
      "What are the limitations of CoAxNN?\n",
      "Question:\n",
      "What future studies will be explored in CoAxNN?\n",
      "Question:\n",
      "What are the different sensitives for model accuracy?\n",
      "Question:\n",
      "What will be explored in future studies?\n",
      "Question:\n",
      "What is the conclusion of the paper?\n",
      "Question:\n",
      "What is reference [1]?\n",
      "Question:\n",
      "What is reference [2]?\n",
      "Question:\n",
      "What is reference [3]?\n",
      "Question:\n",
      "What is reference [4]?\n",
      "Question:\n",
      "What is reference [5]?\n",
      "Question:\n",
      "What is reference [6]?\n",
      "Question:\n",
      "What is reference [7]?\n",
      "Question:\n",
      "What is reference [8]?\n",
      "Question:\n",
      "What is reference [9]?\n",
      "Question:\n",
      "What is reference [10]?\n",
      "Question:\n",
      "What is reference [11]?\n",
      "Question:\n",
      "What is reference [12]?\n",
      "Question:\n",
      "What is reference [13]?\n",
      "Question:\n",
      "What is reference [14]?\n",
      "Question:\n",
      "What is reference [15]?\n",
      "Question:\n",
      "What is reference [16]?\n",
      "Question:\n",
      "What is reference [17]?\n",
      "Question:\n",
      "What is reference [18]?\n",
      "Question:\n",
      "What is reference [19]?\n",
      "Question:\n",
      "What is reference [20]?\n",
      "Question:\n",
      "What is reference [21]?\n",
      "Question:\n",
      "What is reference [22]?\n",
      "Question:\n",
      "What is reference [23]?\n",
      "Question:\n",
      "What is reference [24]?\n",
      "Question:\n",
      "What is reference [25]?\n",
      "Question:\n",
      "What is reference [26]?\n",
      "Question:\n",
      "What is reference [27]?\n",
      "Question:\n",
      "What is reference [28]?\n",
      "Question:\n",
      "What is reference [29]?\n",
      "Question:\n",
      "What is reference [30]?\n",
      "Question:\n",
      "What is reference [31]?\n",
      "Question:\n",
      "What is reference [32]?\n",
      "Question:\n",
      "What is reference [33]?\n",
      "Question:\n",
      "What is reference [34]?\n",
      "Question:\n",
      "What is reference [35]?\n",
      "Question:\n",
      "What is reference [36]?\n",
      "Question:\n",
      "What is reference [37]?\n",
      "Question:\n",
      "What is reference [38]?\n",
      "Question:\n",
      "What is reference [39]?\n",
      "Question:\n",
      "What is reference [40]?\n",
      "Question:\n",
      "What is reference [41]?\n",
      "Question:\n",
      "What is reference [42]?\n",
      "Question:\n",
      "What is reference [43]?\n",
      "Question:\n",
      "What is reference [44]?\n",
      "Question:\n",
      "What is reference [45]?\n",
      "Question:\n",
      "What is reference [46]?\n",
      "Question:\n",
      "What is reference [47]?\n",
      "Question:\n",
      "What is reference [48]?\n",
      "Question:\n",
      "What is reference [49]?\n",
      "Question:\n",
      "What is reference [50]?\n",
      "Question:\n",
      "What is reference [51]?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for input_question in tqdm(df['text_question']):\n",
    "    print(input_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 308/308 [00:00<00:00, 41727.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The paper proposes a new method for optimizing on-device deep learning with conditional approximate neural networks.\n",
      "Answer:\n",
      "The keywords associated with this article are on-device deep learning, efficient neural networks, model approximation and optimization.\n",
      "Answer:\n",
      "The main goal of the paper is to develop a new method for deploying deep neural networks on resource-constrained devices while reducing their computational complexity.\n",
      "Answer:\n",
      "Neural network pruning.\n",
      "Answer:\n",
      "To improve the performance of on-device inference.\n",
      "Answer:\n",
      "CoAxNN.\n",
      "Answer:\n",
      "The main goal of the proposed model optimization framework is to effectively combine different approximate strategies to facilitate on-device deep learning via model approximation.\n",
      "Answer:\n",
      "The principles of different approximate optimizations used in the proposed approach include the use of different approximation methods such as quantization, pruning, and knowledge distillation.\n",
      "Answer:\n",
      "The effectiveness of CoAxNN is demonstrated through experimental results, which show that it achieves up to 1.53× speedup while reducing energy by up to 34.61%, with trivial accuracy loss on CIFAR-10/100 and CINIC-10 datasets.\n",
      "Answer:\n",
      "The main goal of the text is to introduce a new approach for efficient on-device deep learning.\n",
      "Answer:\n",
      "Current deep learning models are becoming wider and deeper, leading to tremendous computational costs and expensive energy consumption for model execution.\n",
      "Answer:\n",
      "On-device deep learning has the potential for privacy protection and real-time response.\n",
      "Answer:\n",
      "Pruning-based strategies and quantization-based methods.\n",
      "Answer:\n",
      "It is still a challenging problem to effectively combine them.\n",
      "Answer:\n",
      "Emerging staging-based approximate strategies.\n",
      "Answer:\n",
      "The main aim of this paper is to address the challenging problem of designing an efficient model optimization framework to make full advantage of the superiority of different optimization strategies.\n",
      "Answer:\n",
      "The key issue in combining different strategies is that the configuration parameters of different strategies may affect each other, influencing the optimization effect of the model, and even leading to poor optimization results.\n",
      "Answer:\n",
      "The corresponding authors of this paper are G. Li, X. Ma, Q. Yu, L. Liu, H. Liu, and W. Xueying.\n",
      "Answer:\n",
      "Guangli Li and Xiu Ma.\n",
      "Answer:\n",
      "Received in revised form on 18 July 2023.\n",
      "Answer:\n",
      "CoAxNN (Conditional Approximate Neural Networks).\n",
      "Answer:\n",
      "The main idea of the text is that the authors have developed a new deep learning framework called CoAxNN, which combines staging-based and pruning-based approximate strategies for efficient on-device deep learning.\n",
      "Answer:\n",
      "The two main optimization techniques used in CoAxNN are pruning and staging.\n",
      "Answer:\n",
      "The key novelty of CoAxNN is that it provides an effective and efficient mechanism to combine pruning and staging, allowing for more efficient model optimization.\n",
      "Answer:\n",
      "The optimization techniques studied in the paper include pruning and staging.\n",
      "Answer:\n",
      "The main contribution of the paper is the presentation of a novel neural network optimization framework, CoAxNN, which effectively combines staging-based and pruning-based approximate strategies to improve actual performance while meeting accuracy requirements for efficient on-device model inference.\n",
      "Answer:\n",
      "According to the principles of staging-based and pruning-based approximate strategies, the framework constructs the design space and automatically searches for reasonable configuration parameters, including the number of stages, the position of stages, the threshold of stages, and the pruning rate, so as to make efficient on-device model inference.\n",
      "Answer:\n",
      "The main goal of CoAxNN is to achieve efficient model optimization by making full use of the advantages of both approximate strategies and the design space.\n",
      "Answer:\n",
      "In the context of the paper, the phrase \"trivial accuracy loss\" means that the performance of the optimized model is very close to the performance of the state-of-the-art model, with only a small difference in accuracy.\n",
      "Answer:\n",
      "The main contribution of the paper is the development of a novel optimization framework that can significantly improve the performance of model inference on commercial edge devices, such as Jetson AGX Orin, by leveraging approximate strategies and the design space.\n",
      "Answer:\n",
      "The main idea of the text is to introduce a new optimization framework for neural network models.\n",
      "Answer:\n",
      "The details of the optimization framework described in Section 3 are the pruning-based approximation and the structured pruning.\n",
      "Answer:\n",
      "The purpose of the experimental evaluation in Section 4 is to evaluate the performance of the optimized models.\n",
      "Answer:\n",
      "Magnitude-based pruning and dynamic network surgery.\n",
      "Answer:\n",
      "To reduce the storage and computation demands by an order of magnitude.\n",
      "Answer:\n",
      "Optimal Brain Damage (OBD) and Optimal Brain Surgeon (OBS).\n",
      "Answer:\n",
      "The main challenges in optimizing the OBD method are the condition that the Hessian matrix is non-diagonal and the difficulty in supporting these approaches with existing software and hardware.\n",
      "Answer:\n",
      "Unstructured sparse models are models that require specific matrix multiplication calculations and storage formats, which can hardly leverage existing high-efficiency BLAS libraries.\n",
      "Answer:\n",
      "Filter pruning methods are methods that dynamically prune filters in a soft manner, which zeroizes the unimportant filters and keeps updating them in the training stage.\n",
      "Answer:\n",
      "The main focus of much recent work in deep learning is on filter pruning methods.\n",
      "Answer:\n",
      "The purpose of dynamically pruning filters in a soft manner is to zeroize the unimportant filters and keep updating them in the training stage.\n",
      "Answer:\n",
      "The goal of Li et al.'s fusioncatalyzed filter pruning approach is to simultaneously optimize the parametric and non-parametric operators.\n",
      "Answer:\n",
      "To decrease the inference time for FPGA deep learning accelerators by adaptively pruning the neural network based on the size of the systolic array used to calculate the convolutions.\n",
      "Answer:\n",
      "An effective filter importance criterion to estimate the contribution of filters to the adversarial training loss.\n",
      "Answer:\n",
      "Yes, according to Liu et al., pruning can be useful as an architecture search paradigm in some cases.\n",
      "Answer:\n",
      "Li et al. [15] proposed a random architecture search to find a good architecture given a pre-defined model by channel pruning.\n",
      "Answer:\n",
      "Li et al. [16] proposed an end-to-end channel pruning method to search out the desired sub-network automatically and efficiently, which learns per-layer sparsity through depth-wise binary convolution.\n",
      "Answer:\n",
      "Ding et al. [17] presented a neural architecture search with pruning method, which derives the most potent model by removing trivial and redundant edges from the whole neural network topology.\n",
      "Answer:\n",
      "Filter pruning.\n",
      "Answer:\n",
      "Early exiting and layer skipping.\n",
      "Answer:\n",
      "That a deep neural network with additional side branch classifiers can both improve accuracy and significantly reduce the inference time of the network.\n",
      "Answer:\n",
      "Adaptive computing is a technique that enables a system to adjust its computing resources according to the conditions at run-time.\n",
      "Answer:\n",
      "Teerapittayanon et al. demonstrated that a deep neural network with additional side branch classifiers can both improve accuracy and significantly reduce the inference time of the network.\n",
      "Answer:\n",
      "Fang et al. presented an input-adaptive framework for video analytics, which adopts an architecture search-based scheme to find the optimal architecture for each early exit branch.\n",
      "Answer:\n",
      "The main contribution of the paper by Li et al. is the design of dynamic layer-skipping mechanisms to suppress unnecessary costs for easy samples and halt inference for all samples to meet resource constraints for the inference of more complicated CNN backbones.\n",
      "Answer:\n",
      "Figurnov et al. studied early termination in each residual unit of ResNets.\n",
      "Answer:\n",
      "The main idea behind the earlyexiting method proposed by Farhadi et al. is to reduce the amount of needed computation on the FPGA platform using partial reconfiguration.\n",
      "Answer:\n",
      "According to the text, the exit position of the sample in the multi-branch network is directly determined by the difficulty of the sample without intermediate trial errors.\n",
      "Answer:\n",
      "The purpose of the low-cost early exit network proposed by Jo et al. is to significantly improve energy efficiencies by reducing the parameters used in inference with efficient branch structures.\n",
      "Answer:\n",
      "The main contribution of the paper is achieving a multi-stage approximate model by early exiting to accelerate model inference for input samples in real-world scenarios.\n",
      "Answer:\n",
      "The main problem addressed in the text is Design Space Exploration (DSE) for neural network models.\n",
      "Answer:\n",
      "Panda et al. [19] and Teerapittayanon et al. [18] empirically set the location and threshold for each exit in the conditional neural network model through experimentation.\n",
      "Answer:\n",
      "Jayakodi et al. [24] found the best thresholds via Bayesian Opt\n",
      "Answer:\n",
      "Park et al. [28] integrated the once-for-all technique and BPNet, which consider architectures of base network and exit branches simultaneously in the same search process.\n",
      "Answer:\n",
      "Li et al. [11] proposed a flexible-rate filter pruning method, which selects the filters to be pruned with a greedy-based strategy.\n",
      "Answer:\n",
      "Qian et al. [30] proposed a hierarchical threshold pruning method, which considers the filter importance within relatively\n",
      "Answer:\n",
      "The main goal of the paper is to achieve efficient on-device inference by automatically finding the (near-)optimal configuration of layerwise pruning for a better network structure.\n",
      "Answer:\n",
      "The focus of the pruning-based approximate strategy is to compress the model by deleting unimportant parameters in the model, which reduces the computation costs.\n",
      "Answer:\n",
      "The main objective of the staging-based approximate strategy is to improve the execution speed of the model, which allows the inference of most simple samples to terminate with a good prediction in the shortest time possible.\n",
      "Answer:\n",
      "The configuration parameters considered in the staging-based approximate strategy include how to place the exits and how to set a threshold for each exit.\n",
      "Answer:\n",
      "The staging-based approximate strategy improves the execution speed of the model by allowing the inference of most simple samples to terminate with a good prediction in the earlier stage by attaching multiple exits in the original model.\n",
      "Answer:\n",
      "The combination of different approximate strategies may affect each other and potentially influences the effect of the model optimization.\n",
      "\n",
      "Note: The numbers in the output are for illustration purposes only and do not reflect the actual values used\n",
      "Answer:\n",
      "The optimization effect of the ResNet-56 using different configuration parameters under the specified requirements of accuracy on the CIFAR-10 dataset.\n",
      "Answer:\n",
      "The number of stages, stage threshold, and pruning rate, respectively.\n",
      "Answer:\n",
      "The relationship is not regular and will be affected by the stage threshold and pruning rate.\n",
      "Answer:\n",
      "The relationship between the number of stages and the computational cost is not regular, which means that the computational cost of a model with more stages may not always be larger than a model with fewer stages.\n",
      "Answer:\n",
      "The staging-based optimization can cause the computational cost of a model to increase or decrease, depending on the pruning rate used.\n",
      "Answer:\n",
      "From the partial experimental results, it can be observed that at a certain accuracy requirement (98.1% in this case), the computational cost of the optimized models using three stages is less than that of the model using two stages.\n",
      "Answer:\n",
      "The paper concludes that the optimization effects of different configuration parameters are distinct and irregular under the specified accuracy requirement, making it difficult to find an optimal model.\n",
      "Answer:\n",
      "The paper observes that at the accuracy requirement of 98.1%, the computation of the optimized models using three stages is less than that of the model using two stages.\n",
      "Answer:\n",
      "The paper shows that the optimization effect of staging-based strategy, pruning-based strategy, and CoAxNN for ResNet-56 on the CIFAR-10.\n",
      "Answer:\n",
      "The main focus of the paper is to combine different approximate strategies to achieve efficient optimization for neural network models.\n",
      "Answer:\n",
      "The normalized computational cost of the staging-based optimization strategy is 0.89.\n",
      "Answer:\n",
      "The computational cost of CoAxNN is 0.64, which is greatly improved compared to the staging-based and pruning-based strategies.\n",
      "Answer:\n",
      "The computational cost of CoAxNN is 0.64.\n",
      "Answer:\n",
      "The pruning rate of CoAxNN is 0.1.\n",
      "Answer:\n",
      "The normalized computational cost of CoAxNN is 0.89.\n",
      "Answer:\n",
      "CoAxNN performs staging-based and pruning-based approximate strategies according to the genes of the chromosome for each individual, which generates a compressed multi-stage model.\n",
      "Answer:\n",
      "According to the availability of stages in the gene, CoAxNN attaches exit branches to the original model to build a multistage conditional activation model.\n",
      "Answer:\n",
      "According to the threshold of each stage, CoAxNN predicts input samples, having distinct difficulties, by multiple stages with different computational complexities, with the entropy-aware activation manner.\n",
      "Answer:\n",
      "The purpose of removing unimportant filters is to reduce computational costs.\n",
      "Answer:\n",
      "The next step is to update the chromosome pool, generating the next generation of individuals.\n",
      "Answer:\n",
      "The general approach of executing a neural network model is a one-staged approach, which processes all the inputs in the same manner, starting from the input operator and performing it operator by operator until the final exit operator.\n",
      "Answer:\n",
      "The purpose of the early exiting strategy in CoAxNN is to give an early exiting opportunity for simple inputs.\n",
      "Answer:\n",
      "The notation  ∗ represents a multi-stage model in CoAxNN.\n",
      "Answer:\n",
      "The purpose of the number of stages 𝜏 in CoAxNN is to formalize the early exiting strategy in CoAxNN.\n",
      "Answer:\n",
      "The main difference between the original neural network model and the approximate model with the staging-based strategy is that the approximate model has additional components, such as the exit checker and the additional exit branch, which are not present in the original model. These additional components allow the approximate model to adaptively condition the inference based on the input data.\n",
      "Answer:\n",
      "The purpose of the exit checker in the staging-based strategy is to determine when to exit the inference process based on the input data. The exit checker uses a threshold and a conditional activation operator to decide when to exit the inference process, allowing the model to adaptively condition the inference based on the input data.\n",
      "Answer:\n",
      "𝜏 represents the original (main) neural network model.\n",
      "Answer:\n",
      "𝑐𝑖 represents the use of threshold 𝜀𝑖.\n",
      "Answer:\n",
      " represents the neural network model.\n",
      "Answer:\n",
      "Exit branches may interfere with computational graph optimization methods, such as operator fusion and memory reuse, provided by deep learning frameworks, increasing operation counts, data movement, and other system overheads.\n",
      "Answer:\n",
      "The confidence threshold is used to determine whether the prediction result of stage 𝑖 has sufficient confidence. With a higher threshold, complex samples may finish predictions from the previous exits with lower accuracy, and using a lower threshold, simple samples may use more complex computations to complete inference, due to cannot end from the previous classifiers, incurring additional computational overheads.\n",
      "Answer:\n",
      "The structure of each exit branch (𝒊) consists of several operators\n",
      "Answer:\n",
      "The different types of operators used for feature extraction in the text are {𝑓 ∗, … , 𝑓 ∗\n",
      "𝑏𝑖−1}.\n",
      "Answer:\n",
      "The purpose of the 𝑓 ∗\n",
      "𝑏𝑖\n",
      "operator in the text is to produce classification results based on the output of 𝑓 ∗\n",
      ".\n",
      "Answer:\n",
      "The difference in the configuration and complexity of the intermediate feature maps for different depths of the\n",
      "Answer:\n",
      "The number (𝜏) and the position (𝒑𝒊) of the exit branches (𝒊) are two factors that will affect each other in the early-exiting method.\n",
      "Answer:\n",
      "To address the problem of increasing the number of exit branches to reduce the computational cost while meeting the model accuracy requirement, CoAxNN puts the availability of each stage into the design space of the GA, and each available stage corresponds to a different solution in the search space.\n",
      "Answer:\n",
      "CoAxNN addresses the problem of increasing the number of exit branches to reduce computational cost while meeting model accuracy requirements.\n",
      "Answer:\n",
      "CoAxNN chooses to attach exit branches at the end of the group of building blocks, and does not attach the exit branch after the last group of building blocks as there is already an existing original exit for the original backbone.\n",
      "Answer:\n",
      "The purpose of introducing a feature extractor and a linear classifier for each exit branch is to retain the original neural network structure and introduce new features for each exit branch.\n",
      "Answer:\n",
      "The purpose of introducing a feature extractor and a linear classifier in CoAxNN is to improve the efficiency of the network for easy samples that exit early.\n",
      "\n",
      "    𝑖. Question: What is the design of the feature extractor in CoAxNN?\n",
      "Answer: The feature extractor in CoAxNN is designed with the building block as granularity, which not only retains the original neural network structure but also provides more opportunities for system-level optimizations.\n",
      "\n",
      "    𝑖. Question: What are the activation operators used in the feature extractor of CoAxNN?\n",
      "Answer: The activation operators used in the feature extractor of CoAxNN are non-linear activation operators such as rectified linear units and normalization operators such as batch\n",
      "Answer:\n",
      "The main problem with linear classifiers is that they have a long latency for easy samples that exit early.\n",
      "Answer:\n",
      "The purpose of adding an extra pooling operator in CoAxNN is to evaluate the confidence of the prediction result for the input sample of the 𝑖th stage classifier.\n",
      "Answer:\n",
      "The design of 𝑖 in CoAxNN is to have\n",
      "Answer:\n",
      "The purpose of using entropy as the entropy-aware activation operator in the CoAxNN model is to evaluate the confidence of the prediction result for the input sample of the 𝑖th stage classifier.\n",
      "Answer:\n",
      "The formula for calculating the entropy of the predicted probability distribution in the CoAxNN model is:\n",
      "\n",
      "entropy( ̂𝑦𝑖) = 𝐶 ∑ ̂𝑦𝑖(𝑐) log ̂𝑦𝑖\n",
      "Answer:\n",
      "It provides realistic performance improvements by removing redundant computations of unimportant filters.\n",
      "Answer:\n",
      "Structured pruning, such as filter pruning, has higher computational efficiency than unstructured pruning.\n",
      "Answer:\n",
      "It is the amount of feature maps that are removed when a filter is deleted.\n",
      "Answer:\n",
      "The purpose of pruning filters in CoAxNN is to remove corresponding feature maps, providing realistic performance improvements.\n",
      "Answer:\n",
      "In CoAxNN, the filter pruning method works by compressing the multi-stage model and quantifying the importance of each filter in a convolutional operator based on the 𝓁2-norm.\n",
      "Answer:\n",
      "The dynamic pruning scheme used in CoAxNN is a st\n",
      "Answer:\n",
      "To keep the model capacity and minimize the loss of accuracy as much as possible.\n",
      "Answer:\n",
      "Joint training defines a loss function for each classifier and minimizes the weighted sum of loss functions for all classifiers during training, which helps to alleviate the overfitting of the model.\n",
      "Answer:\n",
      "The main objective of CoAxNN is to train the backbone neural network and exit branches at the same time and minimize the weighted sum of the cross-entropy loss functions of all stages.\n",
      "Answer:\n",
      "The formula used to calculate the cross-entropy loss function in CoAxNN is given by CE(𝑦, ̄𝑦𝑖) = −𝐶∑𝑐=1𝑦(𝑐)loge𝑓(𝑐) where 𝑓 is the output of the linear classifier of the 𝑖th stage\n",
      "Answer:\n",
      "CoAxNN is a method for compressing and pruning deep neural networks.\n",
      "Answer:\n",
      "The input to CoAxNN is a given training dataset, training epochs, batch size, original deep neural network model, number of stages, weights for loss functions of all stages, and the chromosome pool.\n",
      "Answer:\n",
      "The output of CoAxNN is the number of filters (𝑡) and the 𝓁2-norm of each filter in the approximate multi-stage model.\n",
      "Answer:\n",
      "The dynamic pruning scheme in CoAxNN is used to prune filters with low 𝓁2-norm, thus maintaining the learning ability of the model.\n",
      "\n",
      "    🚀 Question 2: How does CoAxNN update the weights of the traditional backpropagation algorithm?\n",
      "    📝 Answer 2: CoAxNN updates the weights of the traditional backpropagation algorithm according to the loss function according to Eq. (5).\n",
      "\n",
      "    🚀 Question 3: What is the purpose of reordering the importance of filters in the pruning process of each epoch in CoAxNN?\n",
      "    📝 Answer 3: The purpose of reordering the importance of filters in the pruning process\n",
      "Answer:\n",
      "The \"if P[p][i] is available then\" statement is used to check if the loss function is available for the current iteration. If it is available, then the loss function is used to update the model parameters.\n",
      "Answer:\n",
      "The \"𝑙𝑜𝑠𝑠\" variable is used to store the gradients of the loss function with respect to the model parameters.\n",
      "Answer:\n",
      "The line sets the initial value of the output variable 𝑤𝑒𝑖𝑔ℎ𝑡𝑒𝑑_𝑙𝑜𝑠𝑠 to 0, which is used as the input for the next\n",
      "Answer:\n",
      "The search space for determining which stage is available in CoAxNN is 2𝜏.\n",
      "Answer:\n",
      "The search space for thresholds in CoAxNN is 𝑄𝜏, where 𝑄 is the number of candidate thresholds.\n",
      "Answer:\n",
      "The search space for pruning rate in CoAxNN is 𝑅, which indicates the number of candidate pruning rates.\n",
      "Answer:\n",
      "The main idea of the text is that CoAxNN is a genetic algorithm-based dynamic system evolution (GA-DSE) method that uses a chromosome set to find the near-optimal solution in a large search space.\n",
      "Answer:\n",
      "The purpose of Algorithm 2 is to evaluate the accuracy and latency of individuals in the CoAxNN algorithm.\n",
      "Answer:\n",
      "The output\n",
      "Answer:\n",
      "The prediction result is obtained by traversing all available stages and calculating the confidence of corresponding output at each stage according to Equation (3).\n",
      "Answer:\n",
      "The confidence threshold used by CoAxNN is the confidence threshold (𝜀𝑖) of this stage.\n",
      "Answer:\n",
      "The accuracy function returned by CoAxNN is 1 if the prediction is correct, and 0 otherwise.\n",
      "Answer:\n",
      "CoAxNN evaluates the latency score using a similar manner as the accuracy score, which accumulates the latency of the backbone neural network and exit branches until the end of the prediction.\n",
      "Answer:\n",
      "GA-based DSE gets the (near-)optimal solutions about the goal of accuracy and latency by using a Genetic Algorithm to search for the best combination of parameters that maximizes the accuracy and minimizes the latency.\n",
      "Answer:\n",
      "The output of CoAxNN is the average accuracy score (𝛿) and average latency score (𝜇) for all individuals.\n",
      "Answer:\n",
      "The optimization process highlighted in the text is the use of GA-based DSE to obtain near-optimal solutions for the goal of accuracy and latency.\n",
      "Answer:\n",
      "The purpose of removing unimportant filters in the optimization process is to obtain an optimized neural network model.\n",
      "Answer:\n",
      "The significance of the accuracy requirement in the optimization process is that it determines which model is selected according to the user's requirements. If the accuracy requirement is high, the model with the least computation cost is selected under a trivial accuracy loss, while if a\n",
      "Answer:\n",
      "The purpose of Algorithm 2 is to evaluate the performance of different configurations of neural network models on a given dataset.\n",
      "Answer:\n",
      "The output of Algorithm 2 is the accuracy and latency of each configuration of neural network models on the given dataset.\n",
      "Answer:\n",
      "The line \"if P[p][i] is available\n",
      "Answer:\n",
      "The optimized models show a speedup of approximately 1.73 times compared to the original models on the Jetson AGX Orin platform.\n",
      "Answer:\n",
      "The optimized models show an energy consumption reduction of approximately 0.64 times compared to the original models on the Jetson AGX Orin platform.\n",
      "Answer:\n",
      "The proposed method shows an effectiveness of approximately 0.\n",
      "Answer:\n",
      "The two datasets that are part of the CIFAR dataset are CIFAR-10 and CIFAR-100.\n",
      "Answer:\n",
      "The number of classes in the CIFAR-10 dataset is 10.\n",
      "Answer:\n",
      "The number of images in the CINIC-10 dataset is 27,000.\n",
      "Answer:\n",
      "The main goal of the GA-based DSE is to obtain (near-)optimal solutions about accuracy and latency.\n",
      "Answer:\n",
      "The same data argumentation strategies and scheduling settings as [1] are used in the GA-based DSE.\n",
      "Answer:\n",
      "The main conclusion that can be drawn from the results shown in Fig. 4 is that the (near)optimal solutions found by CoAxNN are close to the boundary of the green and red regions, demonstrating the effectiveness of CoAxNN in searching for models with the least computational cost while meeting accuracy requirements.\n",
      "Answer:\n",
      "CoAxNN is a method that demonstrates the effectiveness of CoAxNN.\n",
      "Answer:\n",
      "The purpose of using GA-based DSE is to search for the model having the least computational cost in most cases and meeting the accuracy requirements.\n",
      "Answer:\n",
      "\"ACC. Drop\" represents the accuracy dropping of the model after optimization. A smaller number of \"ACC. Drop\" is better, and a negative number indicates the optimized model has higher accuracy than the baseline model.\n",
      "\n",
      "Note: In the output, the questions and answers are numbered for clarity.\n",
      "Answer:\n",
      "The \"ACC. Drop\" value represents the accuracy dropping of the model after optimization.\n",
      "Answer:\n",
      "A smaller \"ACC. Drop\" value indicates that the optimized model has higher accuracy than the baseline model.\n",
      "Answer:\n",
      "Overfitting refers to the regularization effect of model optimization, which can reduce the overfitting of neural network models.\n",
      "Answer:\n",
      "CoAxNN reduces the computational complexity of the ResNet-20 model by 25.94% while maintaining its accuracy.\n",
      "Answer:\n",
      "CoAxNN consumes less computation cost than other state-of-the-art model optimization methods, such as SFP, while achieving comparable accuracy.\n",
      "Answer:\n",
      "The optimized ResNet-32 model using CoAxNN has a computational cost of 3.44E7 FLOPs.\n",
      "Answer:\n",
      "CoAxNN reduces the computational cost of ResNet-32, ResNet-56, and ResNet-110.\n",
      "Answer:\n",
      "CoAxNN's accuracy drop is 1.39% for ResNet-32, 1.58% for ResNet-56, and 1.22% for ResNet-110.\n",
      "Answer:\n",
      "CoAxNN achieves a similar accuracy loss while reducing computational complexity by automatically searching for a reasonable configuration to effectively optimize the computational complexity while meeting the accuracy requirements.\n",
      "Answer:\n",
      "According to the text, CoAxNN reduces more computations than existing methods, achieving less resource consumption.\n",
      "Answer:\n",
      "The difference in accuracy between the baseline and accelerated models in Table 1 ranges from 0.67% to 1.39% for different models.\n",
      "Answer:\n",
      "According to Table 1, CoAxNN performs better than other state-of-the-art methods, achieving top-1 accuracy of 0.67% and 1.39% for different models.\n",
      "Answer:\n",
      "The top-1 accuracy of the ResNet-56 model on the MNIST dataset is 92.88%.\n",
      "Answer:\n",
      "The FLOPs count of the CoAxNN model on the CIFAR-10 dataset is 2.61E7.\n",
      "Answer:\n",
      "The top-1 accuracy of the SFP model on the TAS dataset is 0.74%.\n",
      "\n",
      "    Note: All answers are in percentage format.\n",
      "Answer:\n",
      "0.67%.\n",
      "Answer:\n",
      "58.71% × 1.93E7 + 41.29% × 4.53E7 = 3.00E7.\n",
      "Answer:\n",
      "Three stages are used in CoAxNN for ResNet-110.\n",
      "\n",
      "Note: All answers are in the format of \"Answer: Value\" where \"Answer\" is the question answer and \"Value\" is the correct answer.\n",
      "Answer:\n",
      "CoAxNN employs distinct stages for different neural network models.\n",
      "Answer:\n",
      "The two stages are used for ResNet-20.\n",
      "Answer:\n",
      "Three stages are used for more complex ResNet-32, ResNet-56, and ResNet-110.\n",
      "Answer:\n",
      "The pruning rate for the optimized ResNet-20 model is 0, meaning no pruning is performed.\n",
      "Answer:\n",
      "The number of stages in the optimized ResNet-20 model is two.\n",
      "Answer:\n",
      "The position of the first stage in the optimized ResNet-20 model is the end of the fourth residual block.\n",
      "Answer:\n",
      "0.67%\n",
      "Answer:\n",
      "The position of three stages is the end of the 10, 19th residual block.\n",
      "Answer:\n",
      "0.07.\n",
      "Answer:\n",
      "The position of three stages in the optimized ResNet-56 is the end of the 10, 19th residual block with the threshold of 0.07.\n",
      "Answer:\n",
      "The position of three stages in the optimized ResNet-110 is the end of the 19, 37th residual block with the threshold of 0.07, 0.017.\n",
      "Answer:\n",
      "CoAxNN outperforms other state-of-the-art methods on the CIFAR-100 dataset by ResNet-\n",
      "Answer:\n",
      "A higher computation reduction of 33.34% was achieved by CoAxNN.\n",
      "Answer:\n",
      "A lower accuracy loss of 1.30% was achieved by CoAxNN.\n",
      "Answer:\n",
      "A higher computational complexity of 1.82E8 FLOPs was used by CoAxNN.\n",
      "\n",
      "Note: All answers are in the format of a number or a percentage.\n",
      "Answer:\n",
      "The number of images predicted by the first few stages of the optimized model on CIFAR-100 is 29.67%, 32.85%, and 37.48%, respectively.\n",
      "Answer:\n",
      "The percentage of images predicted by the first few stages of the optimized model on CIFAR-10 is 44.81%, 36.79%, and 18.40%, respectively.\n",
      "Answer:\n",
      "According to Table 8, the computational cost of the original ResNet-18 model is 5.49E8 FLOPs.\n",
      "Answer:\n",
      "The accuracy of the ResNet-18 model decreased by 1.01% when the FLOPs were reduced from 5.49E8 to 2.21E8.\n",
      "Answer:\n",
      "CoAxNN reduced\n",
      "Answer:\n",
      "CoAxNN achieves a computational complexity reduction of 49.73% (5.93E8 FLOPs) compared to the original ResNet-50 model.\n",
      "Answer:\n",
      "CoAxNN improves the top-1 accuracy of the ResNet-50 model by 0.38% compared to the original model.\n",
      "Answer:\n",
      "FPC reduces the computational complexity of the ResNet-50 model by 40.48% (7.76E8 FLOPs) compared to the original model.\n",
      "Answer:\n",
      "The accuracy drop of the ResNet-18 and ResNet-50 in Table 8 is 1.01% and −0.10%, respectively.\n",
      "Answer:\n",
      "The ResNet-18 and ResNet-50 use four stages in Table 9.\n",
      "Answer:\n",
      "The pruning rate of the ResNet-18 in Table 9 is 0.3.\n",
      "Answer:\n",
      "The top-1 accuracy percentages for the optimized neural network models on CIFAR-100 are 71.33%, 72.75%, and 72.79% for ResNet-56, ResNet-110, and CoAxNN, respectively.\n",
      "Answer:\n",
      "The percentage drop in FLOPs for the optimized models compared to the baseline model is 39.30%, 23.93%, and 40.53% for ResNet-56, ResNet-110, and CoAxNN, respectively.\n",
      "Answer:\n",
      "The percentage of accuracy drop for the ResNet-56 model with the optimal configuration found by CoAxNN is 0.98%.\n",
      "Answer:\n",
      "The number of FLOPs for the ResNet-110 model with the optimal configuration found\n",
      "Answer:\n",
      "CoAxNN is comparable to state-of-the-art methods in terms of performance.\n",
      "Answer:\n",
      "The main advantage of using staging-based approximate strategies is that the inference of simple inputs can be terminated with a good prediction confidence in the earlier stage, thereby avoiding remaining layerwise computations.\n",
      "Answer:\n",
      "The main disadvantage of pruning-based approximate strategies is that the pruning method lacks the ability to configure the neural network dynamically, which will miss the opportunities to optimize the model inference.\n",
      "Answer:\n",
      "The main goal of the paper is to present a new method for efficient model optimization.\n",
      "Answer:\n",
      "The pruning method lacks the ability to configure the neural network dynamically, which will miss the opportunities to optimize the model inference.\n",
      "Answer:\n",
      "CoAxNN automatically finds (near-)optimal configurations by GA-based DSE, making full use of the advantages of both, thus achieving efficient model optimization.\n",
      "Answer:\n",
      "0.67%\n",
      "Answer:\n",
      "1.33×\n",
      "Answer:\n",
      "ResNet-\n",
      "Answer:\n",
      "The percentage of accuracy loss achieved by CoAxNN on the CIFAR-10 dataset is 0.67%, 0.84%, 0.74%, and 0.63%.\n",
      "Answer:\n",
      "CoAxNN can accelerate ResNet-20, ResNet-32, ResNet-56, and ResNet-110 models by 1.33×, 1.34×, 1.53×, and 1.51×, respectively.\n",
      "Answer:\n",
      "CoAxNN reduces\n",
      "Answer:\n",
      "CoAxNN reduced the accuracy loss of 0.67%, 0.84%, 0.74%, and 0.63% on the CIFAR-10 dataset.\n",
      "Answer:\n",
      "CoAxNN reduced the energy consumption of ResNet-20, ResNet-32, ResNet-56, and ResNet-110 by 25.17%, 25.68%, 34.61%, and 33.81%, respectively.\n",
      "Answer:\n",
      "The optimized models have higher execution latency compared to the baseline models, with an increase of 10.2% and 13.6% for ResNet-18 and ResNet-50, respectively.\n",
      "Answer:\n",
      "The increase in execution latency in the optimized models is due to the filter pruning technique used to reduce the computational costs and memory footprint, which results in a higher number of computations required to complete the same task.\n",
      "Answer:\n",
      "The study's findings suggest that filter\n",
      "Answer:\n",
      "The ResNet-18 model with the optimal configuration achieves a FLOPs reduction of 1.01%.\n",
      "Answer:\n",
      "The ResNet-50 model with the optimal configuration achieves a FLOPs reduction of −0.10%.\n",
      "Answer:\n",
      "The ResNet-18 model with the optimal configuration has 39.90% FLOPs.\n",
      "Answer:\n",
      "The energy reduction of the optimized ResNet-20 model is 0.67%.\n",
      "Answer:\n",
      "The energy reduction of the optimized ResNet-32 model is 0.84%.\n",
      "Answer:\n",
      "The energy reduction of the optimized ResNet-56 model is 0.74%.\n",
      "\n",
      "Note: All questions and answers are case-sensitive.\n",
      "Answer:\n",
      "CoAxNN-ACT denotes the accuracy of the model at each stage on the whole dataset and on the images that satisfy the activation condition of the corresponding stage, respectively.\n",
      "Answer:\n",
      "The critical motivation of CoAxNN is to find a satisfying optimization configuration for practical scenarios.\n",
      "Answer:\n",
      "The ablation study in Fig. 5 shows the accuracy of ResNet-56 optimized by CoAxNN at different stages, and it shows that the accuracy of the model in the first few stages is lower than that of the baseline model, and as the computational complexity of the model increases, the accuracy in the later stages increases.\n",
      "Answer:\n",
      "The main conclusion that can be drawn from the visualization results in Fig. 5 is that the accuracy of the CoAxNN-ALL model gradually converges to that of the baseline model as the computational complexity of the model increases.\n",
      "Answer:\n",
      "The CoAxNN-ACT model differs from the CoAxNN-ALL model in that the former has a higher accuracy in the first few stages, indicating that the first few stages have sufficient ability to classify simple images.\n",
      "Answer:\n",
      "From the visualization results in Fig. 6, it can be inferred that the CoAxNN\n",
      "Answer:\n",
      "CoAxNN can separate \"easy\" images consuming less effort from \"hard\" images consuming more computation, significantly reducing computation costs for neural network models.\n",
      "Answer:\n",
      "The accuracy of the last stage of the optimization model is lower than that of the baseline model.\n",
      "Answer:\n",
      "We collect the latency of each operator of the neural network model on the edge device in the profiling phase to be used in GA-based search.\n",
      "Answer:\n",
      "Significantly reducing computation costs for neural network models.\n",
      "Answer:\n",
      "The GA-based DSE takes 1–2 s on the CPU platform, which is greatly less than model training.\n",
      "Answer:\n",
      "Yes, CoAxNN is a generic framework for optimizing on-device deep learning via model approximation, which can be generalized to other intelligent tasks such as object detection.\n",
      "\n",
      "Note: The questions and answers are generated based on the provided input text, and may not be accurate or complete.\n",
      "Answer:\n",
      "The main advantage of CoAxNN is that the runtime overhead is negligible.\n",
      "Answer:\n",
      "Yes, CoAxNN can be applied to other intelligent tasks such as object detection.\n",
      "Answer:\n",
      "The advantage of using CoAxNN is that it can achieve efficient fine-tuning of neural network models without requiring specific software implementations and hardware design support.\n",
      "Answer:\n",
      "The requirements of intelligent tasks include the ability to perform optimization processes efficiently and effectively.\n",
      "Answer:\n",
      "The limitations of CoAxNN include the use of a fixed-rate filter pruning strategy and the inability of the NSGA-III used in GA-based DSE to always find the optimal solutions for increasing accuracy and decreasing latency.\n",
      "Answer:\n",
      "Future studies in CoAxNN will include exploring other genetic algorithms such as NPGA for multiobjective optimization and setting different pruning ratios for different layers to further improve performance.\n",
      "\n",
      "Note: The questions and answers are generated based on the provided text, but they may not be accurate or complete.\n",
      "Answer:\n",
      "Different layers have different sensitives for model accuracy.\n",
      "Answer:\n",
      "Setting different pruning ratios for different layers can potentially further improve the performance, which will be explored in future studies.\n",
      "Answer:\n",
      "In this paper, we proposed an efficient optimization framework, CoAxNN, which effectively combines staging-based with pruning-based approximate strategies for efficient model inference on resource-constrained edge devices.\n",
      "Answer:\n",
      "Reference [1]: K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in:\n",
      "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n",
      "2016, pp. 770–778.\n",
      "Answer:\n",
      "Reference [2]: Y. He, G. Kang, X. Dong, Y. Fu, Y. Yang, Soft filter pruning for accelerating\n",
      "in: Proceedings of the Twenty-Seventh\n",
      "Intelligence (IJCAI), 2018, pp.\n",
      "\n",
      "deep convolutional neural networks,\n",
      "International Joint Conference on Artificial\n",
      "2234–2240.\n",
      "Answer:\n",
      "Reference [3]: S.K. Esser, J.L. McKinstry, D. Bablani, R. Appuswamy, D.S. Modha, Learned\n",
      "step size quantization, in: International Conference on Learning Representations,\n",
      "2020.\n",
      "Answer:\n",
      "Reference [4]: Y. Guo, A. Yao, Y. Chen, Dynamic network surgery for efficient dnns,\n",
      "\n",
      "in:\n",
      "\n",
      "Advances in Neural Information Processing Systems, Vol. 29, 2016.\n",
      "Answer:\n",
      "Reference [5]: S. Han, J. Pool, J. Tran, W. Dally, Learning both weights and connections for\n",
      "efficient neural network, in: Advances in Neural Information Processing Systems,\n",
      "Vol. 28, 2015.\n",
      "Answer:\n",
      "Reference [6]: B. Hassibi, D. Stork, Second order derivatives for network pruning: Optimal brain\n",
      "surgeon, in: Advances in Neural Information Processing Systems, Vol. 5, 1992.\n",
      "Answer:\n",
      "Reference [7]: B. Hassibi, D.G. Stork, G.J. Wolff, Optimal brain surgeon and general network\n",
      "pruning, in: IEEE International Conference on Neural Networks, IEEE, 1993, pp.\n",
      "293–299.\n",
      "Answer:\n",
      "Reference [8]: Y. He, X. Dong, G. Kang, Y. Fu, C. Yan, Y. Yang, Asymptotic soft filter pruning\n",
      "for deep convolutional neural networks, IEEE Trans. Cybern. 50 (8) (2019)\n",
      "3594–3604.\n",
      "Answer:\n",
      "Reference [9]: G. Li, X. Ma, X. Wang, L. Liu, J. Xue, X. Feng, Fusion-catalyzed pruning for\n",
      "optimizing deep learning on intelligent edge devices, IEEE Trans. Comput.-Aided\n",
      "Des. Integr. Circuits Syst. 39 (11) (2020) 3614–3626.\n",
      "Answer:\n",
      "Reference [10]: J.-H. Luo, J. Wu, W. Lin, Thinet: A filter level pruning method for deep neural\n",
      "network compression, in: Proceedings of the IEEE International Conference on\n",
      "Computer Vision, 2017, pp. 5058–5066.\n",
      "Answer:\n",
      "Reference [11]: G. Li, X. Ma, X. Wang, H. Yue, J. Li, L. Liu, X. Feng, J. Xue, Optimizing deep\n",
      "neural networks on intelligent edge accelerators via flexible-rate filter pruning,\n",
      "J. Syst. Archit. (2022) 102431.\n",
      "Answer:\n",
      "Reference [12]: J. Plochaet, T. Goedemé, Hardware-aware pruning for FPGA deep learning\n",
      "accelerators, in: Proceedings of the IEEE/CVF Conference on Computer Vision\n",
      "and Pattern Recognition, 2023, pp. 4481–4489.\n",
      "Answer:\n",
      "Reference [13]: X. Zhuang, Y. Ge, B. Zheng, Q. Wang, Adversarial network pruning by filter\n",
      "robustness estimation, in: ICASSP 2023-2023 IEEE International Conference on\n",
      "Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2023, pp. 1–5.\n",
      "Answer:\n",
      "Reference [14]: Z. Liu, M. Sun, T. Zhou, G. Huang, T. Darrell, Rethinking the value of network\n",
      "pruning, in: International Conference on Learning Representations (ICLR), 2019.\n",
      "Answer:\n",
      "Reference [15]: Y. Li, K. Adamczewski, W. Li, S. Gu, R. Timofte, L. Van Gool, Revisiting\n",
      "random channel pruning for neural network compression, in: Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp.\n",
      "191–201.\n",
      "Answer:\n",
      "Reference [16]: Y. Li, P. Zhao, G. Yuan, X. Lin, Y. Wang, X. Chen, Pruning-as-search: Efficient\n",
      "neural architecture search via channel pruning and structural reparameterization,\n",
      "in: Proceedings of the Thirty-First International Joint Conference on Artificial\n",
      "Intelligence, 2022, pp. 3236–3242.\n",
      "Answer:\n",
      "Reference [17]: Y. Ding, Y. Wu, C. Huang, S. Tang, F. Wu, Y. Yang, W. Zhu, Y. Zhuang, NAP:\n",
      "\n",
      "Neural architecture search with pruning, Neurocomputing 477 (2022) 85–95.\n",
      "Answer:\n",
      "Reference [18]: S. Teerapittayanon, B. McDanel, H.-T. Kung, Branchynet: Fast inference via early\n",
      "exiting from deep neural networks, in: 2016 23rd International Conference on\n",
      "Pattern Recognition (ICPR), IEEE, 2016, pp. 2464–2469.\n",
      "Answer:\n",
      "Reference [19]: P. Panda, A. Sengupta, K. Roy, Conditional deep learning for energy-efficient\n",
      "and enhanced pattern recognition, in: 2016 Design, Automation & Test in Europe\n",
      "Conference & Exhibition (DATE), IEEE, 2016, pp. 475–480.\n",
      "Answer:\n",
      "Reference [20]: Y. Yang, D. Liu, H. Fang, Y.-X. Huang, Y. Sun, Z.-Y. Zhang, Once for all skip:\n",
      "efficient adaptive deep neural networks, in: 2022 Design, Automation & Test in\n",
      "Europe Conference & Exhibition (DATE), IEEE, 2022, pp. 568–571.\n",
      "Answer:\n",
      "Reference [21]: B. Fang, X. Zeng, F. Zhang, H. Xu, M. Zhang, FlexDNN: Input-adaptive on-device\n",
      "deep learning for efficient mobile vision, in: 2020 IEEE/ACM Symposium on Edge\n",
      "Computing (SEC), IEEE, 2020, pp. 84–95.\n",
      "Answer:\n",
      "Reference [22]: Y. Wang, J. Shen, T.-K. Hu, P. Xu, T. Nguyen, R. Baraniuk, Z. Wang, Y. Lin,\n",
      "Dual dynamic inference: Enabling more efficient, adaptive, and controllable deep\n",
      "inference, IEEE J. Sel. Top. Sign. Proces. 14 (4) (2020) 623–633.\n",
      "Answer:\n",
      "Reference [23]: M. Figurnov, M.D. Collins, Y. Zhu, L. Zhang, J. Huang, D. Vetrov, R. Salakhutdi-\n",
      "nov, Spatially adaptive computation time for residual networks, in: Proceedings\n",
      "of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp.\n",
      "1039–1048.\n",
      "Answer:\n",
      "Reference [24]: N.K. Jayakodi, A. Chatterjee, W. Choi, J.R. Doppa, P.P. Pande, Trading-off\n",
      "accuracy and energy of deep inference on embedded systems: A co-design\n",
      "approach, IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst. 37 (11) (2018)\n",
      "2881–2893.\n",
      "Answer:\n",
      "Reference [25]: Z. Liang, Y. Zhou, Dispense mode for inference to accelerate branchynet, in:\n",
      "2022 IEEE International Conference on Image Processing (ICIP), IEEE, 2022, pp.\n",
      "1246–1250.\n",
      "Answer:\n",
      "Reference [26]: J. Jo, G. Kim, S. Kim, J. Park, LoCoExNet: Low-cost early exit network for energy\n",
      "efficient CNN accelerator design, IEEE Trans. Comput.-Aided Des. Integr. Circuits\n",
      "Syst. (2023).\n",
      "Answer:\n",
      "Reference [27]: K. Park, C. Oh, Y. Yi, Bpnet: branch-pruned conditional neural network for\n",
      "systematic time-accuracy tradeoff, in: 2020 57th ACM/IEEE Design Automation\n",
      "Conference (DAC), IEEE, 2020, pp. 1–6.\n",
      "Answer:\n",
      "Reference [28]: G. Park, Y. Yi, Condnas: neural architecture search for conditional CNNs,\n",
      "\n",
      "Electronics 11 (7) (2022) 1101.\n",
      "Answer:\n",
      "Reference [29]: Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, S. Han, Amc: Automl for model\n",
      "compression and acceleration on mobile devices, in: Proceedings of the European\n",
      "Conference on Computer Vision (ECCV), 2018, pp. 784–800.\n",
      "Answer:\n",
      "Reference [30]: Y. Qian, Z. He, Y. Wang, B. Wang, X. Ling, Z. Gu, H. Wang, S. Zeng, W. Swaileh,\n",
      "Hierarchical threshold pruning based on uniform response criterion, IEEE Trans.\n",
      "Neural Netw. Learn. Syst. (2023).\n",
      "Answer:\n",
      "Reference [31]: K. Wang, D. Zhang, Y. Li, R. Zhang, L. Lin, Cost-effective active learning for deep\n",
      "image classification, IEEE Trans. Circuits Syst. Video Technol. 27 (12) (2016)\n",
      "2591–2600.\n",
      "Answer:\n",
      "Reference [32]: S. Anwar, K. Hwang, W. Sung, Structured pruning of deep convolutional neural\n",
      "\n",
      "networks, ACM J. Emerg. Technol. Comput. Syst. (JETC) 13 (3) (2017) 1–18.\n",
      "Answer:\n",
      "Reference [33]: J.H. Holland, Adaptation in Natural and Artificial Systems: An Introductory\n",
      "Analysis with Applications To Biology, Control, and Artificial Intelligence, MIT\n",
      "Press, 1992.\n",
      "Answer:\n",
      "Reference [34]: A. Mohammadi, H. Asadi, S. Mohamed, K. Nelson, S. Nahavandi, OpenGA, a C++\n",
      "genetic algorithm library, in: 2017 IEEE International Conference on Systems,\n",
      "Man, and Cybernetics (SMC), IEEE, 2017, pp. 2051–2056.\n",
      "Answer:\n",
      "Reference [35]: K. Deb, H. Jain, An evolutionary many-objective optimization algorithm using\n",
      "reference-point-based nondominated sorting approach, part I: solving problems\n",
      "with box constraints, IEEE Trans. Evol. Comput. 18 (4) (2013) 577–601.\n",
      "Answer:\n",
      "Reference [36]: A. Krizhevsky, G. Hinton, et al., Learning multiple layers of features from tiny\n",
      "\n",
      "images, 2009.\n",
      "Answer:\n",
      "Reference [37]: L.N. Darlow, E.J. Crowley, A. Antoniou, A.J. Storkey, Cinic-10 is not imagenet\n",
      "\n",
      "or cifar-10, 2018, arXiv preprint arXiv:1810.03505.\n",
      "Answer:\n",
      "Reference [38]: L. Cai, Z. An, C. Yang, Y. Xu, Softer pruning, incremental regularization, in:\n",
      "2020 25th International Conference on Pattern Recognition (ICPR), IEEE, 2021,\n",
      "pp. 224–230.\n",
      "Answer:\n",
      "Reference [39]: X. Dong, J. Huang, Y. Yang, S. Yan, More is less: A more complicated network\n",
      "in: Proceedings of the IEEE Conference on\n",
      "\n",
      "with less inference complexity,\n",
      "Computer Vision and Pattern Recognition, 2017, pp. 5840–5848.\n",
      "Answer:\n",
      "Reference [40]: Y. He, P. Liu, Z. Wang, Z. Hu, Y. Yang, Filter pruning via geometric median\n",
      "for deep convolutional neural networks acceleration,\n",
      "in: Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp.\n",
      "4340–4349.\n",
      "Answer:\n",
      "Reference [41]: X. Dong, Y. Yang, Network pruning via transformable architecture search, Adv.\n",
      "\n",
      "Neural Inf. Process. Syst. 32 (2019).\n",
      "Answer:\n",
      "Reference [42]: Y. He, X. Zhang, J. Sun, Channel pruning for accelerating very deep neural\n",
      "networks, in: Proceedings of the IEEE International Conference on Computer\n",
      "Vision, 2017, pp. 1389–1397.\n",
      "Answer:\n",
      "Reference [43]: S. Lin, R. Ji, C. Yan, B. Zhang, L. Cao, Q. Ye, F. Huang, D. Doermann,\n",
      "Towards optimal structured cnn pruning via generative adversarial\n",
      "learning,\n",
      "in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, 2019, pp. 2790–2799.\n",
      "Answer:\n",
      "Reference [44]: L. Cai, Z. An, C. Yang, Y. Xu, Soft and hard filter pruning via dimension\n",
      "reduction, in: 2021 International Joint Conference on Neural Networks (IJCNN),\n",
      "IEEE, 2021, pp. 1–8.\n",
      "Answer:\n",
      "Reference [45]: X. Yang, H. Lu, H. Shuai, X.-T. Yuan, Pruning convolutional neural networks\n",
      "via stochastic gradient hard thresholding, in: Pattern Recognition and Computer\n",
      "Vision: Second Chinese Conference, PRCV 2019, Xi’an, China, November 8–11,\n",
      "2019, Proceedings, Part I, Springer, 2019, pp. 373–385.\n",
      "Answer:\n",
      "Reference [46]: O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A.\n",
      "Karpathy, A. Khosla, M. Bernstein, et al., Imagenet large scale visual recognition\n",
      "challenge, Int. J. Comput. Vis. 115 (3) (2015) 211–252.\n",
      "Answer:\n",
      "Reference [47]: Y. Chen, X. Wen, Y. Zhang, Q. He, FPC: Filter pruning via the contribution\n",
      "of output feature map for deep convolutional neural networks acceleration,\n",
      "Knowl.-Based Syst. 238 (2022) 107876.\n",
      "Answer:\n",
      "Reference [48]: Y. Chen, X. Wen, Y. Zhang, W. Shi, CCPrune: Collaborative channel pruning for\n",
      "\n",
      "learning compact convolutional networks, Neurocomputing 451 (2021) 35–45.\n",
      "Answer:\n",
      "Reference [49]: X. Chen, H. Ma, J. Wan, B. Li, T. Xia, Multi-view 3d object detection network\n",
      "for autonomous driving, in: Proceedings of the IEEE Conference on Computer\n",
      "Vision and Pattern Recognition, 2017, pp. 1907–1915.\n",
      "Answer:\n",
      "Reference [50]: G. Hinton, O. Vinyals, J. Dean, et al., Distilling the knowledge in a neural\n",
      "\n",
      "network, arXiv preprint arXiv:1503.02531 2 (7) (2015).\n",
      "Answer:\n",
      "Reference [51]: J. Horn, N. Nafpliotis, D.E. Goldberg, Multiobjective optimization using the\n",
      "\n",
      "niched Pareto genetic algorithm, 1993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for input_question in tqdm(df['text_answer']):\n",
    "    print(input_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question:\\nWhat is the conclusion of the paper?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_question'].iloc[256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Question:\\nWhat is the main contribution of the paper?',\n",
       " 'Question:\\nWhat are the keywords associated with this article?',\n",
       " 'Question:\\nWhat is the main goal of the paper?',\n",
       " 'Question:\\nWhat are the approximate strategies used in the study?',\n",
       " 'Question:\\nWhat is the purpose of combining different approximate strategies?',\n",
       " 'Question:\\nWhat is the proposed model optimization framework called?',\n",
       " 'Question:\\nWhat is the main goal of the proposed model optimization framework?',\n",
       " 'Question:\\nWhat are the principles of different approximate optimizations used in the proposed approach?',\n",
       " 'Question:\\nWhat is the effectiveness of CoAxNN according to the experimental results?',\n",
       " 'Question:\\nWhat is the main goal of the text?',\n",
       " 'Question:\\nWhat are the limitations of current deep learning models?',\n",
       " 'Question:\\nWhat are the potential benefits of on-device deep learning?',\n",
       " 'Question:\\nWhat are the efforts made to enable efficient on-device deep learning?',\n",
       " 'Question:\\nWhat are the challenges in combining different optimization strategies?',\n",
       " 'Question:\\nWhat are the approximate strategies based on distinct principles?',\n",
       " 'Question:\\nWhat is the main aim of this paper?',\n",
       " 'Question:\\nWhat is the key issue in combining different strategies?',\n",
       " 'Question:\\nWho are the corresponding authors of this paper?',\n",
       " 'Question:\\nWho contributed equally to this work?',\n",
       " 'Question:\\nWhen was this work received in revised form?',\n",
       " 'Question:\\nWhat is the novel neural network optimization framework presented in this paper?',\n",
       " 'Question:\\nWhat is the main idea of the text?',\n",
       " 'Question:\\nWhat are the two main optimization techniques used in CoAxNN?',\n",
       " 'Question:\\nWhat is the key novelty of CoAxNN compared to previous work?',\n",
       " 'Question:\\nWhat are the optimization techniques studied in the paper?',\n",
       " 'Question:\\nWhat is the main contribution of the paper?',\n",
       " 'Question:\\nHow does the framework construct the design space?',\n",
       " 'Question:\\nWhat is the main goal of CoAxNN?',\n",
       " 'Question:\\nWhat does the phrase \"trivial accuracy loss\" mean in the context of the paper?',\n",
       " 'Question:\\nWhat is the main contribution of the paper?',\n",
       " 'Question:\\nWhat is the main idea of the text?',\n",
       " 'Question:\\nWhat are the details of the optimization framework described in Section 3?',\n",
       " 'Question:\\nWhat is the purpose of the experimental evaluation in Section 4?',\n",
       " 'Question:\\nWhat are the two methods of pruning used in the text?',\n",
       " 'Question:\\nWhat is the purpose of pruning according to the text?',\n",
       " 'Question:\\nWhat are two approaches used for pruning according to the text?',\n",
       " 'Question:\\nWhat are the main challenges in optimizing the OBD method?',\n",
       " 'Question:\\nWhat are unstructured sparse models?',\n",
       " 'Question:\\nWhat are filter pruning methods?',\n",
       " 'Question:\\nWhat is the main focus of much recent work in deep learning?',\n",
       " 'Question:\\nWhat is the purpose of dynamically pruning filters in a soft manner?',\n",
       " \"Question:\\nWhat is the goal of Li et al.'s fusioncatalyzed filter pruning approach?\",\n",
       " 'Question:\\nWhat is the goal of the hardware-aware pruning method introduced by Plochaet et al.?',\n",
       " 'Question:\\nWhat is the importance criterion proposed by Zhuang et al. to evaluate the importance of filters in structured pruning?',\n",
       " 'Question:\\nCan pruning be useful as an architecture search paradigm?',\n",
       " 'Question:\\nWhat is the main contribution of Li et al. [15]?',\n",
       " 'Question:\\nWhat is the proposed method in Li et al. [16]?',\n",
       " 'Question:\\nWhat is the main contribution of Ding et al. [17]?',\n",
       " 'Question:\\nWhat technique do the authors adopt to realize practical performance improvement for neural network models?',\n",
       " 'Question:\\nWhat are staging-based approximation techniques?',\n",
       " \"Question:\\nWhat is the main idea of Teerapittayanon et al.'s work?\",\n",
       " 'Question:\\nWhat is adaptive computing?',\n",
       " 'Question:\\nWhat did Teerapittayanon et al. demonstrate?',\n",
       " 'Question:\\nWhat did Fang et al. present?',\n",
       " 'Question:\\nWhat is the main contribution of the paper by Li et al.?',\n",
       " 'Question:\\nWhat did Figurnov et al. study in their paper?',\n",
       " 'Question:\\nWhat is the main idea behind the earlyexiting method proposed by Farhadi et al.?',\n",
       " 'Question:\\nWhat does the text say about the exit position of the sample in the multi-branch network?',\n",
       " 'Question:\\nWhat is the purpose of the low-cost early exit network proposed by Jo et al.?',\n",
       " 'Question:\\nWhat is the main contribution of the paper?',\n",
       " 'Question:\\nWhat is the main problem addressed in the text?',\n",
       " 'Question:\\nHow did Panda et al. [19] and Teerapittayanon et al. [18] set the location and threshold for each exit in the conditional neural network model?',\n",
       " 'Question:\\nHow did Jayakodi et al. [24] find the best thresholds for the specified trade-off between accuracy and energy consumption of inference?',\n",
       " 'Question:\\nWhat is the main contribution of Park et al. [28] in the field of DSE?',\n",
       " \"Question:\\nWhat is the key idea of Li et al. [11]'s proposed method for filter pruning?\",\n",
       " \"Question:\\nWhat is the main advantage of Qian et al. [30]'s proposed method for DSE?\",\n",
       " 'Question:\\nWhat is the main goal of the paper?',\n",
       " 'Question:\\nWhat is the focus of the pruning-based approximate strategy?',\n",
       " 'Question:\\nWhat is the main objective of the staging-based approximate strategy?',\n",
       " 'Question:\\nWhat are the configuration parameters considered in the staging-based approximate strategy?',\n",
       " 'Question:\\nHow does the staging-based approximate strategy improve the execution speed of the model?',\n",
       " 'Question:\\nHow does the combination of different approximate strategies affect the effect of the model optimization?',\n",
       " 'Question:\\nWhat is shown in Fig. 1?',\n",
       " 'Question:\\nWhat do the triples (𝑥, 𝑦, 𝑧) represent in Fig. 1?',\n",
       " 'Question:\\nWhat is the relationship between the number of stages and the computational cost shown in Fig. 1?',\n",
       " 'Question:\\nWhat is the relationship between the number of stages and the computational cost in the context of the text?',\n",
       " 'Question:\\nHow does the staging-based optimization affect the computational cost of a model?',\n",
       " 'Question:\\nWhat can be observed from the partial experimental results in Fig. 1?',\n",
       " 'Question:\\nWhat does the paper conclude about the optimization effect of different configuration parameters?',\n",
       " 'Question:\\nWhat does the paper observe in Fig. 1(a)?',\n",
       " 'Question:\\nWhat does the paper show in Fig. 2?',\n",
       " 'Question:\\nWhat is the main focus of the paper?',\n",
       " 'Question:\\nWhat is the normalized computational cost of the staging-based optimization strategy?',\n",
       " 'Question:\\nWhat is the computational cost of CoAxNN?',\n",
       " 'Question:\\nWhat is the computational cost of CoAxNN?',\n",
       " 'Question:\\nWhat is the pruning rate of CoAxNN?',\n",
       " 'Question:\\nWhat is the normalized computational cost of CoAxNN?',\n",
       " 'Question:\\nWhat does CoAxNN perform according to the genes of the chromosome for each individual?',\n",
       " 'Question:\\nWhat does CoAxNN attach to the original model?',\n",
       " 'Question:\\nWhat does CoAxNN predict according to the threshold of each stage?',\n",
       " 'Question:\\nWhat is the purpose of removing unimportant filters in the text?',\n",
       " 'Question:\\nWhat is the next step after evaluating the fitness of the corresponding individuals according to the accuracy and latency of the compressed multi-stage model?',\n",
       " 'Question:\\nWhat is the general approach of executing a neural network model?',\n",
       " 'Question:\\nWhat is the purpose of the early exiting strategy in CoAxNN?',\n",
       " 'Question:\\nWhat does the notation \\ue23a ∗ represent in CoAxNN?',\n",
       " 'Question:\\nWhat is the purpose of the number of stages 𝜏 in CoAxNN?',\n",
       " 'Question:\\nWhat is the main difference between the original neural network model and the approximate model with the staging-based strategy?',\n",
       " 'Question:\\nWhat is the purpose of the exit checker in the staging-based strategy?',\n",
       " 'Question:\\nWhat does 𝜏 represent in the given text?',\n",
       " 'Question:\\nWhat is the significance of 𝑐𝑖 in the given text?',\n",
       " 'Question:\\nWhat is the meaning of \\ue23f in the given text?',\n",
       " 'Question:\\nWhat is the main problem with exit branches in deep learning?',\n",
       " 'Question:\\nWhat is the confidence threshold used for in deep learning?',\n",
       " 'Question:\\nWhat is the structure of each exit branch in deep learning?',\n",
       " 'Question:\\nWhat are the different types of operators used for feature extraction in the text?',\n",
       " 'Question:\\nWhat is the purpose of the 𝑓 ∗\\n𝑏𝑖\\noperator in the text?',\n",
       " 'Question:\\nWhat is the difference in the configuration and complexity of the intermediate feature maps for different depths of the main neural networks?',\n",
       " 'Question:\\nWhat are the two factors that will affect each other in the early-exiting method?',\n",
       " 'Question:\\nWhat is the purpose of putting the availability of each stage into the design space of the GA in CoAxNN?',\n",
       " 'Question:\\nWhat is the main problem that CoAxNN addresses in the text?',\n",
       " 'Question:\\nHow does CoAxNN choose to attach exit branches?',\n",
       " 'Question:\\nWhat is the purpose of introducing a feature extractor and a linear classifier for each exit branch?',\n",
       " 'Question:\\nWhat is the purpose of introducing a feature extractor and a linear classifier in CoAxNN?',\n",
       " 'Question:\\nWhat is the main problem with linear classifiers?',\n",
       " 'Question:\\nWhat is the purpose of adding an extra pooling operator in CoAxNN?',\n",
       " 'Question:\\nWhat is the design of 𝑖 in CoAxNN?',\n",
       " 'Question:\\nWhat is the purpose of using entropy as the entropy-aware activation operator in the CoAxNN model?',\n",
       " 'Question:\\nWhat is the formula for calculating the entropy of the predicted probability distribution in the CoAxNN model?',\n",
       " 'Question:\\nWhat is the main advantage of using pruning-based approximate optimization in CoAxNN?',\n",
       " 'Question:\\nWhat is the difference between structured and unstructured pruning in the neural network pruning technique?',\n",
       " 'Question:\\nWhat is the importance of each filter in a convolutional operator based on the 𝓁2-norm?',\n",
       " 'Question:\\nWhat is the purpose of pruning filters in CoAxNN?',\n",
       " 'Question:\\nHow does the filter pruning method work in CoAxNN?',\n",
       " 'Question:\\nWhat is the dynamic pruning scheme used in CoAxNN?',\n",
       " 'Question:\\nWhat is the purpose of utilizing a dynamic pruning scheme for staging-based approximate CNNs?',\n",
       " 'Question:\\nWhat is the purpose of joint training in the training process of neural network models with exit branches?',\n",
       " 'Question:\\nWhat is the main objective of CoAxNN?\\n    2.',\n",
       " 'Question:\\nWhat is the formula used to calculate the cross-entropy loss function in CoAxNN?\\n    4.',\n",
       " 'Question:\\nWhat is the purpose of CoAxNN?',\n",
       " 'Question:\\nWhat is the input to CoAxNN?',\n",
       " 'Question:\\nWhat is the output of CoAxNN?',\n",
       " 'Question:\\nWhat is the purpose of the dynamic pruning scheme in CoAxNN?\\n    📝',\n",
       " 'Question:\\nWhat is the purpose of the \"if P[p][i] is available then\" statement in the code?',\n",
       " 'Question:\\nWhat is the purpose of the \"𝑙𝑜𝑠𝑠\" variable in the code?',\n",
       " 'Question:\\nWhat is the purpose of the line \"𝑤𝑒𝑖𝑔ℎ𝑡𝑒𝑑_𝑙𝑜𝑠𝑠 = 0;\" in the code?',\n",
       " 'Question:\\nWhat is the search space for determining which stage is available in CoAxNN?',\n",
       " 'Question:\\nWhat is the search space for thresholds in CoAxNN?',\n",
       " 'Question:\\nWhat is the search space for pruning rate in CoAxNN?',\n",
       " 'Question:\\nWhat is the main idea of the text?',\n",
       " 'Question:\\nWhat is the purpose of the algorithm shown in Algorithm 2?',\n",
       " 'Question:\\nWhat is the output of the CoAxNN algorithm?',\n",
       " 'Question:\\nHow is the prediction result obtained by CoAxNN?',\n",
       " 'Question:\\nWhat is the confidence threshold used by CoAxNN?',\n",
       " 'Question:\\nWhat is the accuracy function returned by CoAxNN?',\n",
       " 'Question:\\nHow does CoAxNN evaluate the latency score?',\n",
       " 'Question:\\nHow does GA-based DSE get the (near-)optimal solutions about the goal of accuracy and latency?',\n",
       " 'Question:\\nWhat is the output of CoAxNN?',\n",
       " 'Question:\\nWhat does the text highlight about the optimization process?',\n",
       " 'Question:\\nWhat is the purpose of removing unimportant filters in the optimization process?',\n",
       " 'Question:\\nWhat is the significance of the accuracy requirement in the optimization process?',\n",
       " 'Question:\\nWhat is the purpose of the Algorithm 2 in the given text?',\n",
       " 'Question:\\nWhat is the output of the Algorithm 2 in the given text?',\n",
       " 'Question:\\nWhat is the significance of the line \"if P[p][i] is available then\" in the given text?',\n",
       " 'Question:\\nWhat is the speedup of the optimized models compared to the original models on the Jetson AGX Orin platform?',\n",
       " 'Question:\\nWhat is the energy consumption of the optimized models compared to the original models on the Jetson AGX Orin platform?',\n",
       " 'Question:\\nWhat is the effectiveness of the proposed method on the CIFAR dataset?',\n",
       " 'Question:\\nWhat are the two datasets that are part of the CIFAR dataset?',\n",
       " 'Question:\\nWhat is the number of classes in the CIFAR-10 dataset?',\n",
       " 'Question:\\nWhat is the number of images in the CINIC-10 dataset?',\n",
       " 'Question:\\nWhat is the main goal of the GA-based DSE in the text?',\n",
       " 'Question:\\nWhat is the data argumentation strategy used in the GA-based DSE?',\n",
       " 'Question:\\nWhat is the main conclusion that can be drawn from the results shown in Fig. 4?',\n",
       " 'Question:\\nWhat is CoAxNN?',\n",
       " 'Question:\\nWhat is the purpose of using GA-based DSE in most cases?',\n",
       " 'Question:\\nWhat is the meaning of \"ACC. Drop\" in the context of the text?',\n",
       " 'Question:\\nWhat does the \"ACC. Drop\" value represent in the text?',\n",
       " 'Question:\\nWhat does a smaller \"ACC. Drop\" value indicate in the text?',\n",
       " 'Question:\\nWhat does the term \"overfitting\" refer to in the text?',\n",
       " 'Question:\\nHow does CoAxNN reduce the computational complexity of the ResNet-20 model while maintaining its accuracy?',\n",
       " 'Question:\\nHow does CoAxNN compare to other state-of-the-art model optimization methods in terms of computational complexity and accuracy?',\n",
       " 'Question:\\nWhat is the computational cost of the optimized ResNet-32 model using CoAxNN?',\n",
       " 'Question:\\nWhat does CoAxNN reduce the computational cost of?',\n",
       " 'Question:\\nWhat is the accuracy drop of CoAxNN?',\n",
       " 'Question:\\nHow does CoAxNN achieve a similar accuracy loss while reducing computational complexity?',\n",
       " 'Question:\\nWhat does the text say about the computational complexity of existing methods compared to CoAxNN?',\n",
       " 'Question:\\nWhat is the difference in accuracy between the baseline and accelerated models in Table 1?',\n",
       " 'Question:\\nHow does CoAxNN perform compared to other state-of-the-art methods in Table 1?',\n",
       " 'Question:\\nWhat is the top-1 accuracy of the ResNet-56 model on the MNIST dataset?',\n",
       " 'Question:\\nWhat is the FLOPs count of the CoAxNN model on the CIFAR-10 dataset?',\n",
       " 'Question:\\nWhat is the top-1 accuracy of the SFP model on the TAS dataset?',\n",
       " 'Question:\\nWhat is the accuracy loss of ResNet-20?',\n",
       " 'Question:\\nWhat is the weighted average FLOPs of ResNet-32?',\n",
       " 'Question:\\nHow many stages are used in CoAxNN for ResNet-110?',\n",
       " 'Question:\\nWhat does CoAxNN employ?',\n",
       " 'Question:\\nWhat are the two stages used for?',\n",
       " 'Question:\\nHow many stages are used for more complex models?',\n",
       " 'Question:\\nWhat is the pruning rate for the optimized ResNet-20 model?',\n",
       " 'Question:\\nWhat is the number of stages in the optimized ResNet-20 model?',\n",
       " 'Question:\\nWhat is the position of the first stage in the optimized ResNet-20 model?',\n",
       " 'Question:\\nWhat is the pruning rate of ResNet-32?',\n",
       " 'Question:\\nWhat is the position of the threshold for ResNet-56?',\n",
       " 'Question:\\nWhat is the threshold of ResNet-110 for the last stage?',\n",
       " 'Question:\\nWhat is the position of three stages in the optimized ResNet-56?',\n",
       " 'Question:\\nWhat is the position of three stages in the optimized ResNet-110?',\n",
       " 'Question:\\nHow does CoAxNN perform on the CIFAR-100 dataset?',\n",
       " 'Question:\\nWhat is the computation reduction achieved by CoAxNN?',\n",
       " 'Question:\\nWhat is the accuracy loss of the optimized model with CoAxNN?',\n",
       " 'Question:\\nWhat is the computational complexity of the optimized model with CoAxNN?',\n",
       " 'Question:\\nWhat is the number of images predicted by the first few stages of the optimized model on CIFAR-100?',\n",
       " 'Question:\\nWhat is the percentage of images predicted by the first few stages of the optimized model on CIFAR-10?',\n",
       " 'Question:\\nWhat is the computational cost of the original ResNet-18 model?',\n",
       " 'Question:\\nHow much did the accuracy of the ResNet-18 model decrease when the FLOPs were reduced from 5.49E8 to 2.21E8?',\n",
       " 'Question:\\nHow much did the computational complexity of CoAxNN reduce while achieving a 0.50% accuracy gain for the ResNet-18 model?',\n",
       " 'Question:\\nWhat is the computational complexity reduction achieved by CoAxNN?',\n",
       " 'Question:\\nHow does CoAxNN improve the top-1 accuracy of the ResNet-50 model?',\n",
       " 'Question:\\nWhat is the computational complexity reduction achieved by FPC?',\n",
       " 'Question:\\nWhat is the accuracy drop of the ResNet-18 and ResNet-50 in Table 8?',\n",
       " 'Question:\\nWhat is the number of stages used by the ResNet-18 and ResNet-50 in Table 9?',\n",
       " 'Question:\\nWhat is the pruning rate of the ResNet-18 in Table 9?',\n",
       " 'Question:\\nWhat are the top-1 accuracy percentages for the optimized neural network models on CIFAR-100?',\n",
       " 'Question:\\nWhat is the percentage drop in FLOPs for the optimized models compared to the baseline model?',\n",
       " 'Question:\\nWhat is the percentage of accuracy drop for the ResNet-56 model with the optimal configuration found by CoAxNN?',\n",
       " 'Question:\\nWhat is the number of FLOPs for the ResNet-110 model with the optimal configuration found by CoAxNN?',\n",
       " 'Question:\\nHow does CoAxNN compare to state-of-the-art methods?',\n",
       " 'Question:\\nWhat is the main advantage of using staging-based approximate strategies?',\n",
       " 'Question:\\nWhat is the main disadvantage of pruning-based approximate strategies?',\n",
       " 'Question:\\nWhat is the main goal of the paper?',\n",
       " 'Question:\\nWhat is the problem with the pruning method?',\n",
       " 'Question:\\nWhat is the advantage of CoAxNN compared to other methods?',\n",
       " 'Question:\\nWhat is the average inference latency for CoAxNN on the CIFAR10 dataset?',\n",
       " 'Question:\\nWhat is the speedup of CoAxNN for the ResNet-20 model on the CIFAR10 dataset?',\n",
       " 'Question:\\nWhat is the largest model accelerated by CoAxNN in the table?',\n",
       " 'Question:\\nWhat is the percentage of accuracy loss achieved by CoAxNN on the CIFAR-10 dataset?',\n",
       " 'Question:\\nWhat is the speedup achieved by CoAxNN on different models?',\n",
       " 'Question:\\nHow much energy consumption is reduced by CoAxNN on different models?',\n",
       " 'Question:\\nWhat percentage of accuracy loss did CoAxNN reduce on the CIFAR-10 dataset?',\n",
       " 'Question:\\nWhat percentage of energy consumption did CoAxNN reduce on the ResNet-20, ResNet-32, ResNet-56, and ResNet-110 models?',\n",
       " 'Question:\\nWhat is the difference in execution latency between the baseline models and the optimized models in the study?',\n",
       " 'Question:\\nWhat is the reason for the increase in execution latency in the optimized models, according to the study?',\n",
       " \"Question:\\nWhat is the implication of the study's findings regarding the use of filter pruning for neural network acceleration?\",\n",
       " 'Question:\\nWhat is the percentage of FLOPs reduction for the ResNet-18 model with the optimal configuration?',\n",
       " 'Question:\\nWhat is the percentage of FLOPs reduction for the ResNet-50 model with the optimal configuration?',\n",
       " 'Question:\\nWhat is the number of FLOPs for the ResNet-18 model with the optimal configuration?',\n",
       " 'Question:\\nWhat is the energy reduction of the optimized ResNet-20 model?',\n",
       " 'Question:\\nWhat is the energy reduction of the optimized ResNet-32 model?',\n",
       " 'Question:\\nWhat is the energy reduction of the optimized ResNet-56 model?',\n",
       " 'Question:\\nWhat does \"CoAxNN-ACT\" denote in the given text?',\n",
       " 'Question:\\nWhat is the critical motivation of CoAxNN?',\n",
       " 'Question:\\nWhat does the ablation study in Fig. 5 show?',\n",
       " 'Question:\\nWhat is the main conclusion that can be drawn from the visualization results in Fig. 5?',\n",
       " 'Question:\\nHow does the CoAxNN-ACT model differ from the CoAxNN-ALL model?',\n",
       " 'Question:\\nWhat can be inferred from the visualization results in Fig. 6?',\n",
       " 'Question:\\nWhat is the main advantage of using CoAxNN?',\n",
       " 'Question:\\nWhat is the main disadvantage of using the baseline model?',\n",
       " 'Question:\\nWhat is the purpose of collecting the latency of each operator in the profiling phase?',\n",
       " 'Question:\\nWhat are the benefits of GA-based DSE?',\n",
       " 'Question:\\nWhat are the overheads of GA-based DSE?',\n",
       " 'Question:\\nCan CoAxNN be used for other intelligent tasks?',\n",
       " 'Question:\\nWhat is the main advantage of CoAxNN?',\n",
       " 'Question:\\nCan CoAxNN be applied to other intelligent tasks?',\n",
       " 'Question:\\nWhat is the advantage of using CoAxNN?',\n",
       " 'Question:\\nWhat are the requirements of intelligent tasks?',\n",
       " 'Question:\\nWhat are the limitations of CoAxNN?',\n",
       " 'Question:\\nWhat future studies will be explored in CoAxNN?',\n",
       " 'Question:\\nWhat are the different sensitives for model accuracy?',\n",
       " 'Question:\\nWhat will be explored in future studies?']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df['text_question'].values[:256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Answer:\\nThe paper proposes a new method for optimizing on-device deep learning with conditional approximate neural networks.',\n",
       " 'Answer:\\nThe keywords associated with this article are on-device deep learning, efficient neural networks, model approximation and optimization.',\n",
       " 'Answer:\\nThe main goal of the paper is to develop a new method for deploying deep neural networks on resource-constrained devices while reducing their computational complexity.',\n",
       " 'Answer:\\nNeural network pruning.',\n",
       " 'Answer:\\nTo improve the performance of on-device inference.',\n",
       " 'Answer:\\nCoAxNN.',\n",
       " 'Answer:\\nThe main goal of the proposed model optimization framework is to effectively combine different approximate strategies to facilitate on-device deep learning via model approximation.',\n",
       " 'Answer:\\nThe principles of different approximate optimizations used in the proposed approach include the use of different approximation methods such as quantization, pruning, and knowledge distillation.',\n",
       " 'Answer:\\nThe effectiveness of CoAxNN is demonstrated through experimental results, which show that it achieves up to 1.53× speedup while reducing energy by up to 34.61%, with trivial accuracy loss on CIFAR-10/100 and CINIC-10 datasets.',\n",
       " 'Answer:\\nThe main goal of the text is to introduce a new approach for efficient on-device deep learning.',\n",
       " 'Answer:\\nCurrent deep learning models are becoming wider and deeper, leading to tremendous computational costs and expensive energy consumption for model execution.',\n",
       " 'Answer:\\nOn-device deep learning has the potential for privacy protection and real-time response.',\n",
       " 'Answer:\\nPruning-based strategies and quantization-based methods.',\n",
       " 'Answer:\\nIt is still a challenging problem to effectively combine them.',\n",
       " 'Answer:\\nEmerging staging-based approximate strategies.',\n",
       " 'Answer:\\nThe main aim of this paper is to address the challenging problem of designing an efficient model optimization framework to make full advantage of the superiority of different optimization strategies.',\n",
       " 'Answer:\\nThe key issue in combining different strategies is that the configuration parameters of different strategies may affect each other, influencing the optimization effect of the model, and even leading to poor optimization results.',\n",
       " 'Answer:\\nThe corresponding authors of this paper are G. Li, X. Ma, Q. Yu, L. Liu, H. Liu, and W. Xueying.',\n",
       " 'Answer:\\nGuangli Li and Xiu Ma.',\n",
       " 'Answer:\\nReceived in revised form on 18 July 2023.',\n",
       " 'Answer:\\nCoAxNN (Conditional Approximate Neural Networks).',\n",
       " 'Answer:\\nThe main idea of the text is that the authors have developed a new deep learning framework called CoAxNN, which combines staging-based and pruning-based approximate strategies for efficient on-device deep learning.',\n",
       " 'Answer:\\nThe two main optimization techniques used in CoAxNN are pruning and staging.',\n",
       " 'Answer:\\nThe key novelty of CoAxNN is that it provides an effective and efficient mechanism to combine pruning and staging, allowing for more efficient model optimization.',\n",
       " 'Answer:\\nThe optimization techniques studied in the paper include pruning and staging.',\n",
       " 'Answer:\\nThe main contribution of the paper is the presentation of a novel neural network optimization framework, CoAxNN, which effectively combines staging-based and pruning-based approximate strategies to improve actual performance while meeting accuracy requirements for efficient on-device model inference.',\n",
       " 'Answer:\\nAccording to the principles of staging-based and pruning-based approximate strategies, the framework constructs the design space and automatically searches for reasonable configuration parameters, including the number of stages, the position of stages, the threshold of stages, and the pruning rate, so as to make efficient on-device model inference.',\n",
       " 'Answer:\\nThe main goal of CoAxNN is to achieve efficient model optimization by making full use of the advantages of both approximate strategies and the design space.',\n",
       " 'Answer:\\nIn the context of the paper, the phrase \"trivial accuracy loss\" means that the performance of the optimized model is very close to the performance of the state-of-the-art model, with only a small difference in accuracy.',\n",
       " 'Answer:\\nThe main contribution of the paper is the development of a novel optimization framework that can significantly improve the performance of model inference on commercial edge devices, such as Jetson AGX Orin, by leveraging approximate strategies and the design space.',\n",
       " 'Answer:\\nThe main idea of the text is to introduce a new optimization framework for neural network models.',\n",
       " 'Answer:\\nThe details of the optimization framework described in Section 3 are the pruning-based approximation and the structured pruning.',\n",
       " 'Answer:\\nThe purpose of the experimental evaluation in Section 4 is to evaluate the performance of the optimized models.',\n",
       " 'Answer:\\nMagnitude-based pruning and dynamic network surgery.',\n",
       " 'Answer:\\nTo reduce the storage and computation demands by an order of magnitude.',\n",
       " 'Answer:\\nOptimal Brain Damage (OBD) and Optimal Brain Surgeon (OBS).',\n",
       " 'Answer:\\nThe main challenges in optimizing the OBD method are the condition that the Hessian matrix is non-diagonal and the difficulty in supporting these approaches with existing software and hardware.',\n",
       " 'Answer:\\nUnstructured sparse models are models that require specific matrix multiplication calculations and storage formats, which can hardly leverage existing high-efficiency BLAS libraries.',\n",
       " 'Answer:\\nFilter pruning methods are methods that dynamically prune filters in a soft manner, which zeroizes the unimportant filters and keeps updating them in the training stage.',\n",
       " 'Answer:\\nThe main focus of much recent work in deep learning is on filter pruning methods.',\n",
       " 'Answer:\\nThe purpose of dynamically pruning filters in a soft manner is to zeroize the unimportant filters and keep updating them in the training stage.',\n",
       " \"Answer:\\nThe goal of Li et al.'s fusioncatalyzed filter pruning approach is to simultaneously optimize the parametric and non-parametric operators.\",\n",
       " 'Answer:\\nTo decrease the inference time for FPGA deep learning accelerators by adaptively pruning the neural network based on the size of the systolic array used to calculate the convolutions.',\n",
       " 'Answer:\\nAn effective filter importance criterion to estimate the contribution of filters to the adversarial training loss.',\n",
       " 'Answer:\\nYes, according to Liu et al., pruning can be useful as an architecture search paradigm in some cases.',\n",
       " 'Answer:\\nLi et al. [15] proposed a random architecture search to find a good architecture given a pre-defined model by channel pruning.',\n",
       " 'Answer:\\nLi et al. [16] proposed an end-to-end channel pruning method to search out the desired sub-network automatically and efficiently, which learns per-layer sparsity through depth-wise binary convolution.',\n",
       " 'Answer:\\nDing et al. [17] presented a neural architecture search with pruning method, which derives the most potent model by removing trivial and redundant edges from the whole neural network topology.',\n",
       " 'Answer:\\nFilter pruning.',\n",
       " 'Answer:\\nEarly exiting and layer skipping.',\n",
       " 'Answer:\\nThat a deep neural network with additional side branch classifiers can both improve accuracy and significantly reduce the inference time of the network.',\n",
       " 'Answer:\\nAdaptive computing is a technique that enables a system to adjust its computing resources according to the conditions at run-time.',\n",
       " 'Answer:\\nTeerapittayanon et al. demonstrated that a deep neural network with additional side branch classifiers can both improve accuracy and significantly reduce the inference time of the network.',\n",
       " 'Answer:\\nFang et al. presented an input-adaptive framework for video analytics, which adopts an architecture search-based scheme to find the optimal architecture for each early exit branch.',\n",
       " 'Answer:\\nThe main contribution of the paper by Li et al. is the design of dynamic layer-skipping mechanisms to suppress unnecessary costs for easy samples and halt inference for all samples to meet resource constraints for the inference of more complicated CNN backbones.',\n",
       " 'Answer:\\nFigurnov et al. studied early termination in each residual unit of ResNets.',\n",
       " 'Answer:\\nThe main idea behind the earlyexiting method proposed by Farhadi et al. is to reduce the amount of needed computation on the FPGA platform using partial reconfiguration.',\n",
       " 'Answer:\\nAccording to the text, the exit position of the sample in the multi-branch network is directly determined by the difficulty of the sample without intermediate trial errors.',\n",
       " 'Answer:\\nThe purpose of the low-cost early exit network proposed by Jo et al. is to significantly improve energy efficiencies by reducing the parameters used in inference with efficient branch structures.',\n",
       " 'Answer:\\nThe main contribution of the paper is achieving a multi-stage approximate model by early exiting to accelerate model inference for input samples in real-world scenarios.',\n",
       " 'Answer:\\nThe main problem addressed in the text is Design Space Exploration (DSE) for neural network models.',\n",
       " 'Answer:\\nPanda et al. [19] and Teerapittayanon et al. [18] empirically set the location and threshold for each exit in the conditional neural network model through experimentation.',\n",
       " 'Answer:\\nJayakodi et al. [24] found the best thresholds via Bayesian Opt',\n",
       " 'Answer:\\nPark et al. [28] integrated the once-for-all technique and BPNet, which consider architectures of base network and exit branches simultaneously in the same search process.',\n",
       " 'Answer:\\nLi et al. [11] proposed a flexible-rate filter pruning method, which selects the filters to be pruned with a greedy-based strategy.',\n",
       " 'Answer:\\nQian et al. [30] proposed a hierarchical threshold pruning method, which considers the filter importance within relatively',\n",
       " 'Answer:\\nThe main goal of the paper is to achieve efficient on-device inference by automatically finding the (near-)optimal configuration of layerwise pruning for a better network structure.',\n",
       " 'Answer:\\nThe focus of the pruning-based approximate strategy is to compress the model by deleting unimportant parameters in the model, which reduces the computation costs.',\n",
       " 'Answer:\\nThe main objective of the staging-based approximate strategy is to improve the execution speed of the model, which allows the inference of most simple samples to terminate with a good prediction in the shortest time possible.',\n",
       " 'Answer:\\nThe configuration parameters considered in the staging-based approximate strategy include how to place the exits and how to set a threshold for each exit.',\n",
       " 'Answer:\\nThe staging-based approximate strategy improves the execution speed of the model by allowing the inference of most simple samples to terminate with a good prediction in the earlier stage by attaching multiple exits in the original model.',\n",
       " 'Answer:\\nThe combination of different approximate strategies may affect each other and potentially influences the effect of the model optimization.\\n\\nNote: The numbers in the output are for illustration purposes only and do not reflect the actual values used',\n",
       " 'Answer:\\nThe optimization effect of the ResNet-56 using different configuration parameters under the specified requirements of accuracy on the CIFAR-10 dataset.',\n",
       " 'Answer:\\nThe number of stages, stage threshold, and pruning rate, respectively.',\n",
       " 'Answer:\\nThe relationship is not regular and will be affected by the stage threshold and pruning rate.',\n",
       " 'Answer:\\nThe relationship between the number of stages and the computational cost is not regular, which means that the computational cost of a model with more stages may not always be larger than a model with fewer stages.',\n",
       " 'Answer:\\nThe staging-based optimization can cause the computational cost of a model to increase or decrease, depending on the pruning rate used.',\n",
       " 'Answer:\\nFrom the partial experimental results, it can be observed that at a certain accuracy requirement (98.1% in this case), the computational cost of the optimized models using three stages is less than that of the model using two stages.',\n",
       " 'Answer:\\nThe paper concludes that the optimization effects of different configuration parameters are distinct and irregular under the specified accuracy requirement, making it difficult to find an optimal model.',\n",
       " 'Answer:\\nThe paper observes that at the accuracy requirement of 98.1%, the computation of the optimized models using three stages is less than that of the model using two stages.',\n",
       " 'Answer:\\nThe paper shows that the optimization effect of staging-based strategy, pruning-based strategy, and CoAxNN for ResNet-56 on the CIFAR-10.',\n",
       " 'Answer:\\nThe main focus of the paper is to combine different approximate strategies to achieve efficient optimization for neural network models.',\n",
       " 'Answer:\\nThe normalized computational cost of the staging-based optimization strategy is 0.89.',\n",
       " 'Answer:\\nThe computational cost of CoAxNN is 0.64, which is greatly improved compared to the staging-based and pruning-based strategies.',\n",
       " 'Answer:\\nThe computational cost of CoAxNN is 0.64.',\n",
       " 'Answer:\\nThe pruning rate of CoAxNN is 0.1.',\n",
       " 'Answer:\\nThe normalized computational cost of CoAxNN is 0.89.',\n",
       " 'Answer:\\nCoAxNN performs staging-based and pruning-based approximate strategies according to the genes of the chromosome for each individual, which generates a compressed multi-stage model.',\n",
       " 'Answer:\\nAccording to the availability of stages in the gene, CoAxNN attaches exit branches to the original model to build a multistage conditional activation model.',\n",
       " 'Answer:\\nAccording to the threshold of each stage, CoAxNN predicts input samples, having distinct difficulties, by multiple stages with different computational complexities, with the entropy-aware activation manner.',\n",
       " 'Answer:\\nThe purpose of removing unimportant filters is to reduce computational costs.',\n",
       " 'Answer:\\nThe next step is to update the chromosome pool, generating the next generation of individuals.',\n",
       " 'Answer:\\nThe general approach of executing a neural network model is a one-staged approach, which processes all the inputs in the same manner, starting from the input operator and performing it operator by operator until the final exit operator.',\n",
       " 'Answer:\\nThe purpose of the early exiting strategy in CoAxNN is to give an early exiting opportunity for simple inputs.',\n",
       " 'Answer:\\nThe notation \\ue23a ∗ represents a multi-stage model in CoAxNN.',\n",
       " 'Answer:\\nThe purpose of the number of stages 𝜏 in CoAxNN is to formalize the early exiting strategy in CoAxNN.',\n",
       " 'Answer:\\nThe main difference between the original neural network model and the approximate model with the staging-based strategy is that the approximate model has additional components, such as the exit checker and the additional exit branch, which are not present in the original model. These additional components allow the approximate model to adaptively condition the inference based on the input data.',\n",
       " 'Answer:\\nThe purpose of the exit checker in the staging-based strategy is to determine when to exit the inference process based on the input data. The exit checker uses a threshold and a conditional activation operator to decide when to exit the inference process, allowing the model to adaptively condition the inference based on the input data.',\n",
       " 'Answer:\\n𝜏 represents the original (main) neural network model.',\n",
       " 'Answer:\\n𝑐𝑖 represents the use of threshold 𝜀𝑖.',\n",
       " 'Answer:\\n\\ue23f represents the neural network model.',\n",
       " 'Answer:\\nExit branches may interfere with computational graph optimization methods, such as operator fusion and memory reuse, provided by deep learning frameworks, increasing operation counts, data movement, and other system overheads.',\n",
       " 'Answer:\\nThe confidence threshold is used to determine whether the prediction result of stage 𝑖 has sufficient confidence. With a higher threshold, complex samples may finish predictions from the previous exits with lower accuracy, and using a lower threshold, simple samples may use more complex computations to complete inference, due to cannot end from the previous classifiers, incurring additional computational overheads.',\n",
       " 'Answer:\\nThe structure of each exit branch (𝒊) consists of several operators',\n",
       " 'Answer:\\nThe different types of operators used for feature extraction in the text are {𝑓 ∗, … , 𝑓 ∗\\n𝑏𝑖−1}.',\n",
       " 'Answer:\\nThe purpose of the 𝑓 ∗\\n𝑏𝑖\\noperator in the text is to produce classification results based on the output of 𝑓 ∗\\n.',\n",
       " 'Answer:\\nThe difference in the configuration and complexity of the intermediate feature maps for different depths of the',\n",
       " 'Answer:\\nThe number (𝜏) and the position (𝒑𝒊) of the exit branches (𝒊) are two factors that will affect each other in the early-exiting method.',\n",
       " 'Answer:\\nTo address the problem of increasing the number of exit branches to reduce the computational cost while meeting the model accuracy requirement, CoAxNN puts the availability of each stage into the design space of the GA, and each available stage corresponds to a different solution in the search space.',\n",
       " 'Answer:\\nCoAxNN addresses the problem of increasing the number of exit branches to reduce computational cost while meeting model accuracy requirements.',\n",
       " 'Answer:\\nCoAxNN chooses to attach exit branches at the end of the group of building blocks, and does not attach the exit branch after the last group of building blocks as there is already an existing original exit for the original backbone.',\n",
       " 'Answer:\\nThe purpose of introducing a feature extractor and a linear classifier for each exit branch is to retain the original neural network structure and introduce new features for each exit branch.',\n",
       " 'Answer:\\nThe purpose of introducing a feature extractor and a linear classifier in CoAxNN is to improve the efficiency of the network for easy samples that exit early.\\n\\n    𝑖. Question: What is the design of the feature extractor in CoAxNN?\\nAnswer: The feature extractor in CoAxNN is designed with the building block as granularity, which not only retains the original neural network structure but also provides more opportunities for system-level optimizations.\\n\\n    𝑖. Question: What are the activation operators used in the feature extractor of CoAxNN?\\nAnswer: The activation operators used in the feature extractor of CoAxNN are non-linear activation operators such as rectified linear units and normalization operators such as batch',\n",
       " 'Answer:\\nThe main problem with linear classifiers is that they have a long latency for easy samples that exit early.',\n",
       " 'Answer:\\nThe purpose of adding an extra pooling operator in CoAxNN is to evaluate the confidence of the prediction result for the input sample of the 𝑖th stage classifier.',\n",
       " 'Answer:\\nThe design of 𝑖 in CoAxNN is to have',\n",
       " 'Answer:\\nThe purpose of using entropy as the entropy-aware activation operator in the CoAxNN model is to evaluate the confidence of the prediction result for the input sample of the 𝑖th stage classifier.',\n",
       " 'Answer:\\nThe formula for calculating the entropy of the predicted probability distribution in the CoAxNN model is:\\n\\nentropy( ̂𝑦𝑖) = 𝐶 ∑ ̂𝑦𝑖(𝑐) log ̂𝑦𝑖',\n",
       " 'Answer:\\nIt provides realistic performance improvements by removing redundant computations of unimportant filters.',\n",
       " 'Answer:\\nStructured pruning, such as filter pruning, has higher computational efficiency than unstructured pruning.',\n",
       " 'Answer:\\nIt is the amount of feature maps that are removed when a filter is deleted.',\n",
       " 'Answer:\\nThe purpose of pruning filters in CoAxNN is to remove corresponding feature maps, providing realistic performance improvements.',\n",
       " 'Answer:\\nIn CoAxNN, the filter pruning method works by compressing the multi-stage model and quantifying the importance of each filter in a convolutional operator based on the 𝓁2-norm.',\n",
       " 'Answer:\\nThe dynamic pruning scheme used in CoAxNN is a st',\n",
       " 'Answer:\\nTo keep the model capacity and minimize the loss of accuracy as much as possible.',\n",
       " 'Answer:\\nJoint training defines a loss function for each classifier and minimizes the weighted sum of loss functions for all classifiers during training, which helps to alleviate the overfitting of the model.',\n",
       " 'Answer:\\nThe main objective of CoAxNN is to train the backbone neural network and exit branches at the same time and minimize the weighted sum of the cross-entropy loss functions of all stages.',\n",
       " 'Answer:\\nThe formula used to calculate the cross-entropy loss function in CoAxNN is given by CE(𝑦, ̄𝑦𝑖) = −𝐶∑𝑐=1𝑦(𝑐)loge𝑓(𝑐) where 𝑓 is the output of the linear classifier of the 𝑖th stage',\n",
       " 'Answer:\\nCoAxNN is a method for compressing and pruning deep neural networks.',\n",
       " 'Answer:\\nThe input to CoAxNN is a given training dataset, training epochs, batch size, original deep neural network model, number of stages, weights for loss functions of all stages, and the chromosome pool.',\n",
       " 'Answer:\\nThe output of CoAxNN is the number of filters (𝑡) and the 𝓁2-norm of each filter in the approximate multi-stage model.',\n",
       " 'Answer:\\nThe dynamic pruning scheme in CoAxNN is used to prune filters with low 𝓁2-norm, thus maintaining the learning ability of the model.\\n\\n    🚀 Question 2: How does CoAxNN update the weights of the traditional backpropagation algorithm?\\n    📝 Answer 2: CoAxNN updates the weights of the traditional backpropagation algorithm according to the loss function according to Eq. (5).\\n\\n    🚀 Question 3: What is the purpose of reordering the importance of filters in the pruning process of each epoch in CoAxNN?\\n    📝 Answer 3: The purpose of reordering the importance of filters in the pruning process',\n",
       " 'Answer:\\nThe \"if P[p][i] is available then\" statement is used to check if the loss function is available for the current iteration. If it is available, then the loss function is used to update the model parameters.',\n",
       " 'Answer:\\nThe \"𝑙𝑜𝑠𝑠\" variable is used to store the gradients of the loss function with respect to the model parameters.',\n",
       " 'Answer:\\nThe line sets the initial value of the output variable 𝑤𝑒𝑖𝑔ℎ𝑡𝑒𝑑_𝑙𝑜𝑠𝑠 to 0, which is used as the input for the next',\n",
       " 'Answer:\\nThe search space for determining which stage is available in CoAxNN is 2𝜏.',\n",
       " 'Answer:\\nThe search space for thresholds in CoAxNN is 𝑄𝜏, where 𝑄 is the number of candidate thresholds.',\n",
       " 'Answer:\\nThe search space for pruning rate in CoAxNN is 𝑅, which indicates the number of candidate pruning rates.',\n",
       " 'Answer:\\nThe main idea of the text is that CoAxNN is a genetic algorithm-based dynamic system evolution (GA-DSE) method that uses a chromosome set to find the near-optimal solution in a large search space.',\n",
       " 'Answer:\\nThe purpose of Algorithm 2 is to evaluate the accuracy and latency of individuals in the CoAxNN algorithm.',\n",
       " 'Answer:\\nThe output',\n",
       " 'Answer:\\nThe prediction result is obtained by traversing all available stages and calculating the confidence of corresponding output at each stage according to Equation (3).',\n",
       " 'Answer:\\nThe confidence threshold used by CoAxNN is the confidence threshold (𝜀𝑖) of this stage.',\n",
       " 'Answer:\\nThe accuracy function returned by CoAxNN is 1 if the prediction is correct, and 0 otherwise.',\n",
       " 'Answer:\\nCoAxNN evaluates the latency score using a similar manner as the accuracy score, which accumulates the latency of the backbone neural network and exit branches until the end of the prediction.',\n",
       " 'Answer:\\nGA-based DSE gets the (near-)optimal solutions about the goal of accuracy and latency by using a Genetic Algorithm to search for the best combination of parameters that maximizes the accuracy and minimizes the latency.',\n",
       " 'Answer:\\nThe output of CoAxNN is the average accuracy score (𝛿) and average latency score (𝜇) for all individuals.',\n",
       " 'Answer:\\nThe optimization process highlighted in the text is the use of GA-based DSE to obtain near-optimal solutions for the goal of accuracy and latency.',\n",
       " 'Answer:\\nThe purpose of removing unimportant filters in the optimization process is to obtain an optimized neural network model.',\n",
       " \"Answer:\\nThe significance of the accuracy requirement in the optimization process is that it determines which model is selected according to the user's requirements. If the accuracy requirement is high, the model with the least computation cost is selected under a trivial accuracy loss, while if a\",\n",
       " 'Answer:\\nThe purpose of Algorithm 2 is to evaluate the performance of different configurations of neural network models on a given dataset.',\n",
       " 'Answer:\\nThe output of Algorithm 2 is the accuracy and latency of each configuration of neural network models on the given dataset.',\n",
       " 'Answer:\\nThe line \"if P[p][i] is available',\n",
       " 'Answer:\\nThe optimized models show a speedup of approximately 1.73 times compared to the original models on the Jetson AGX Orin platform.',\n",
       " 'Answer:\\nThe optimized models show an energy consumption reduction of approximately 0.64 times compared to the original models on the Jetson AGX Orin platform.',\n",
       " 'Answer:\\nThe proposed method shows an effectiveness of approximately 0.',\n",
       " 'Answer:\\nThe two datasets that are part of the CIFAR dataset are CIFAR-10 and CIFAR-100.',\n",
       " 'Answer:\\nThe number of classes in the CIFAR-10 dataset is 10.',\n",
       " 'Answer:\\nThe number of images in the CINIC-10 dataset is 27,000.',\n",
       " 'Answer:\\nThe main goal of the GA-based DSE is to obtain (near-)optimal solutions about accuracy and latency.',\n",
       " 'Answer:\\nThe same data argumentation strategies and scheduling settings as [1] are used in the GA-based DSE.',\n",
       " 'Answer:\\nThe main conclusion that can be drawn from the results shown in Fig. 4 is that the (near)optimal solutions found by CoAxNN are close to the boundary of the green and red regions, demonstrating the effectiveness of CoAxNN in searching for models with the least computational cost while meeting accuracy requirements.',\n",
       " 'Answer:\\nCoAxNN is a method that demonstrates the effectiveness of CoAxNN.',\n",
       " 'Answer:\\nThe purpose of using GA-based DSE is to search for the model having the least computational cost in most cases and meeting the accuracy requirements.',\n",
       " 'Answer:\\n\"ACC. Drop\" represents the accuracy dropping of the model after optimization. A smaller number of \"ACC. Drop\" is better, and a negative number indicates the optimized model has higher accuracy than the baseline model.\\n\\nNote: In the output, the questions and answers are numbered for clarity.',\n",
       " 'Answer:\\nThe \"ACC. Drop\" value represents the accuracy dropping of the model after optimization.',\n",
       " 'Answer:\\nA smaller \"ACC. Drop\" value indicates that the optimized model has higher accuracy than the baseline model.',\n",
       " 'Answer:\\nOverfitting refers to the regularization effect of model optimization, which can reduce the overfitting of neural network models.',\n",
       " 'Answer:\\nCoAxNN reduces the computational complexity of the ResNet-20 model by 25.94% while maintaining its accuracy.',\n",
       " 'Answer:\\nCoAxNN consumes less computation cost than other state-of-the-art model optimization methods, such as SFP, while achieving comparable accuracy.',\n",
       " 'Answer:\\nThe optimized ResNet-32 model using CoAxNN has a computational cost of 3.44E7 FLOPs.',\n",
       " 'Answer:\\nCoAxNN reduces the computational cost of ResNet-32, ResNet-56, and ResNet-110.',\n",
       " \"Answer:\\nCoAxNN's accuracy drop is 1.39% for ResNet-32, 1.58% for ResNet-56, and 1.22% for ResNet-110.\",\n",
       " 'Answer:\\nCoAxNN achieves a similar accuracy loss while reducing computational complexity by automatically searching for a reasonable configuration to effectively optimize the computational complexity while meeting the accuracy requirements.',\n",
       " 'Answer:\\nAccording to the text, CoAxNN reduces more computations than existing methods, achieving less resource consumption.',\n",
       " 'Answer:\\nThe difference in accuracy between the baseline and accelerated models in Table 1 ranges from 0.67% to 1.39% for different models.',\n",
       " 'Answer:\\nAccording to Table 1, CoAxNN performs better than other state-of-the-art methods, achieving top-1 accuracy of 0.67% and 1.39% for different models.',\n",
       " 'Answer:\\nThe top-1 accuracy of the ResNet-56 model on the MNIST dataset is 92.88%.',\n",
       " 'Answer:\\nThe FLOPs count of the CoAxNN model on the CIFAR-10 dataset is 2.61E7.',\n",
       " 'Answer:\\nThe top-1 accuracy of the SFP model on the TAS dataset is 0.74%.\\n\\n    Note: All answers are in percentage format.',\n",
       " 'Answer:\\n0.67%.',\n",
       " 'Answer:\\n58.71% × 1.93E7 + 41.29% × 4.53E7 = 3.00E7.',\n",
       " 'Answer:\\nThree stages are used in CoAxNN for ResNet-110.\\n\\nNote: All answers are in the format of \"Answer: Value\" where \"Answer\" is the question answer and \"Value\" is the correct answer.',\n",
       " 'Answer:\\nCoAxNN employs distinct stages for different neural network models.',\n",
       " 'Answer:\\nThe two stages are used for ResNet-20.',\n",
       " 'Answer:\\nThree stages are used for more complex ResNet-32, ResNet-56, and ResNet-110.',\n",
       " 'Answer:\\nThe pruning rate for the optimized ResNet-20 model is 0, meaning no pruning is performed.',\n",
       " 'Answer:\\nThe number of stages in the optimized ResNet-20 model is two.',\n",
       " 'Answer:\\nThe position of the first stage in the optimized ResNet-20 model is the end of the fourth residual block.',\n",
       " 'Answer:\\n0.67%',\n",
       " 'Answer:\\nThe position of three stages is the end of the 10, 19th residual block.',\n",
       " 'Answer:\\n0.07.',\n",
       " 'Answer:\\nThe position of three stages in the optimized ResNet-56 is the end of the 10, 19th residual block with the threshold of 0.07.',\n",
       " 'Answer:\\nThe position of three stages in the optimized ResNet-110 is the end of the 19, 37th residual block with the threshold of 0.07, 0.017.',\n",
       " 'Answer:\\nCoAxNN outperforms other state-of-the-art methods on the CIFAR-100 dataset by ResNet-',\n",
       " 'Answer:\\nA higher computation reduction of 33.34% was achieved by CoAxNN.',\n",
       " 'Answer:\\nA lower accuracy loss of 1.30% was achieved by CoAxNN.',\n",
       " 'Answer:\\nA higher computational complexity of 1.82E8 FLOPs was used by CoAxNN.\\n\\nNote: All answers are in the format of a number or a percentage.',\n",
       " 'Answer:\\nThe number of images predicted by the first few stages of the optimized model on CIFAR-100 is 29.67%, 32.85%, and 37.48%, respectively.',\n",
       " 'Answer:\\nThe percentage of images predicted by the first few stages of the optimized model on CIFAR-10 is 44.81%, 36.79%, and 18.40%, respectively.',\n",
       " 'Answer:\\nAccording to Table 8, the computational cost of the original ResNet-18 model is 5.49E8 FLOPs.',\n",
       " 'Answer:\\nThe accuracy of the ResNet-18 model decreased by 1.01% when the FLOPs were reduced from 5.49E8 to 2.21E8.',\n",
       " 'Answer:\\nCoAxNN reduced',\n",
       " 'Answer:\\nCoAxNN achieves a computational complexity reduction of 49.73% (5.93E8 FLOPs) compared to the original ResNet-50 model.',\n",
       " 'Answer:\\nCoAxNN improves the top-1 accuracy of the ResNet-50 model by 0.38% compared to the original model.',\n",
       " 'Answer:\\nFPC reduces the computational complexity of the ResNet-50 model by 40.48% (7.76E8 FLOPs) compared to the original model.',\n",
       " 'Answer:\\nThe accuracy drop of the ResNet-18 and ResNet-50 in Table 8 is 1.01% and −0.10%, respectively.',\n",
       " 'Answer:\\nThe ResNet-18 and ResNet-50 use four stages in Table 9.',\n",
       " 'Answer:\\nThe pruning rate of the ResNet-18 in Table 9 is 0.3.',\n",
       " 'Answer:\\nThe top-1 accuracy percentages for the optimized neural network models on CIFAR-100 are 71.33%, 72.75%, and 72.79% for ResNet-56, ResNet-110, and CoAxNN, respectively.',\n",
       " 'Answer:\\nThe percentage drop in FLOPs for the optimized models compared to the baseline model is 39.30%, 23.93%, and 40.53% for ResNet-56, ResNet-110, and CoAxNN, respectively.',\n",
       " 'Answer:\\nThe percentage of accuracy drop for the ResNet-56 model with the optimal configuration found by CoAxNN is 0.98%.',\n",
       " 'Answer:\\nThe number of FLOPs for the ResNet-110 model with the optimal configuration found',\n",
       " 'Answer:\\nCoAxNN is comparable to state-of-the-art methods in terms of performance.',\n",
       " 'Answer:\\nThe main advantage of using staging-based approximate strategies is that the inference of simple inputs can be terminated with a good prediction confidence in the earlier stage, thereby avoiding remaining layerwise computations.',\n",
       " 'Answer:\\nThe main disadvantage of pruning-based approximate strategies is that the pruning method lacks the ability to configure the neural network dynamically, which will miss the opportunities to optimize the model inference.',\n",
       " 'Answer:\\nThe main goal of the paper is to present a new method for efficient model optimization.',\n",
       " 'Answer:\\nThe pruning method lacks the ability to configure the neural network dynamically, which will miss the opportunities to optimize the model inference.',\n",
       " 'Answer:\\nCoAxNN automatically finds (near-)optimal configurations by GA-based DSE, making full use of the advantages of both, thus achieving efficient model optimization.',\n",
       " 'Answer:\\n0.67%',\n",
       " 'Answer:\\n1.33×',\n",
       " 'Answer:\\nResNet-',\n",
       " 'Answer:\\nThe percentage of accuracy loss achieved by CoAxNN on the CIFAR-10 dataset is 0.67%, 0.84%, 0.74%, and 0.63%.',\n",
       " 'Answer:\\nCoAxNN can accelerate ResNet-20, ResNet-32, ResNet-56, and ResNet-110 models by 1.33×, 1.34×, 1.53×, and 1.51×, respectively.',\n",
       " 'Answer:\\nCoAxNN reduces',\n",
       " 'Answer:\\nCoAxNN reduced the accuracy loss of 0.67%, 0.84%, 0.74%, and 0.63% on the CIFAR-10 dataset.',\n",
       " 'Answer:\\nCoAxNN reduced the energy consumption of ResNet-20, ResNet-32, ResNet-56, and ResNet-110 by 25.17%, 25.68%, 34.61%, and 33.81%, respectively.',\n",
       " 'Answer:\\nThe optimized models have higher execution latency compared to the baseline models, with an increase of 10.2% and 13.6% for ResNet-18 and ResNet-50, respectively.',\n",
       " 'Answer:\\nThe increase in execution latency in the optimized models is due to the filter pruning technique used to reduce the computational costs and memory footprint, which results in a higher number of computations required to complete the same task.',\n",
       " \"Answer:\\nThe study's findings suggest that filter\",\n",
       " 'Answer:\\nThe ResNet-18 model with the optimal configuration achieves a FLOPs reduction of 1.01%.',\n",
       " 'Answer:\\nThe ResNet-50 model with the optimal configuration achieves a FLOPs reduction of −0.10%.',\n",
       " 'Answer:\\nThe ResNet-18 model with the optimal configuration has 39.90% FLOPs.',\n",
       " 'Answer:\\nThe energy reduction of the optimized ResNet-20 model is 0.67%.',\n",
       " 'Answer:\\nThe energy reduction of the optimized ResNet-32 model is 0.84%.',\n",
       " 'Answer:\\nThe energy reduction of the optimized ResNet-56 model is 0.74%.\\n\\nNote: All questions and answers are case-sensitive.',\n",
       " 'Answer:\\nCoAxNN-ACT denotes the accuracy of the model at each stage on the whole dataset and on the images that satisfy the activation condition of the corresponding stage, respectively.',\n",
       " 'Answer:\\nThe critical motivation of CoAxNN is to find a satisfying optimization configuration for practical scenarios.',\n",
       " 'Answer:\\nThe ablation study in Fig. 5 shows the accuracy of ResNet-56 optimized by CoAxNN at different stages, and it shows that the accuracy of the model in the first few stages is lower than that of the baseline model, and as the computational complexity of the model increases, the accuracy in the later stages increases.',\n",
       " 'Answer:\\nThe main conclusion that can be drawn from the visualization results in Fig. 5 is that the accuracy of the CoAxNN-ALL model gradually converges to that of the baseline model as the computational complexity of the model increases.',\n",
       " 'Answer:\\nThe CoAxNN-ACT model differs from the CoAxNN-ALL model in that the former has a higher accuracy in the first few stages, indicating that the first few stages have sufficient ability to classify simple images.',\n",
       " 'Answer:\\nFrom the visualization results in Fig. 6, it can be inferred that the CoAxNN',\n",
       " 'Answer:\\nCoAxNN can separate \"easy\" images consuming less effort from \"hard\" images consuming more computation, significantly reducing computation costs for neural network models.',\n",
       " 'Answer:\\nThe accuracy of the last stage of the optimization model is lower than that of the baseline model.',\n",
       " 'Answer:\\nWe collect the latency of each operator of the neural network model on the edge device in the profiling phase to be used in GA-based search.',\n",
       " 'Answer:\\nSignificantly reducing computation costs for neural network models.',\n",
       " 'Answer:\\nThe GA-based DSE takes 1–2 s on the CPU platform, which is greatly less than model training.',\n",
       " 'Answer:\\nYes, CoAxNN is a generic framework for optimizing on-device deep learning via model approximation, which can be generalized to other intelligent tasks such as object detection.\\n\\nNote: The questions and answers are generated based on the provided input text, and may not be accurate or complete.',\n",
       " 'Answer:\\nThe main advantage of CoAxNN is that the runtime overhead is negligible.',\n",
       " 'Answer:\\nYes, CoAxNN can be applied to other intelligent tasks such as object detection.',\n",
       " 'Answer:\\nThe advantage of using CoAxNN is that it can achieve efficient fine-tuning of neural network models without requiring specific software implementations and hardware design support.',\n",
       " 'Answer:\\nThe requirements of intelligent tasks include the ability to perform optimization processes efficiently and effectively.',\n",
       " 'Answer:\\nThe limitations of CoAxNN include the use of a fixed-rate filter pruning strategy and the inability of the NSGA-III used in GA-based DSE to always find the optimal solutions for increasing accuracy and decreasing latency.',\n",
       " 'Answer:\\nFuture studies in CoAxNN will include exploring other genetic algorithms such as NPGA for multiobjective optimization and setting different pruning ratios for different layers to further improve performance.\\n\\nNote: The questions and answers are generated based on the provided text, but they may not be accurate or complete.',\n",
       " 'Answer:\\nDifferent layers have different sensitives for model accuracy.',\n",
       " 'Answer:\\nSetting different pruning ratios for different layers can potentially further improve the performance, which will be explored in future studies.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df['text_answer'].values[:256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>text_question</th>\n",
       "      <th>text_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the main contribution of the paper?</td>\n",
       "      <td>The paper proposes a new method for optimizing...</td>\n",
       "      <td>Question:\\nWhat is the main contribution of th...</td>\n",
       "      <td>Answer:\\nThe paper proposes a new method for o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the keywords associated with this art...</td>\n",
       "      <td>The keywords associated with this article are ...</td>\n",
       "      <td>Question:\\nWhat are the keywords associated wi...</td>\n",
       "      <td>Answer:\\nThe keywords associated with this art...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the main goal of the paper?</td>\n",
       "      <td>The main goal of the paper is to develop a new...</td>\n",
       "      <td>Question:\\nWhat is the main goal of the paper?</td>\n",
       "      <td>Answer:\\nThe main goal of the paper is to deve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the approximate strategies used in th...</td>\n",
       "      <td>Neural network pruning.</td>\n",
       "      <td>Question:\\nWhat are the approximate strategies...</td>\n",
       "      <td>Answer:\\nNeural network pruning.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the purpose of combining different app...</td>\n",
       "      <td>To improve the performance of on-device infere...</td>\n",
       "      <td>Question:\\nWhat is the purpose of combining di...</td>\n",
       "      <td>Answer:\\nTo improve the performance of on-devi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>What are the requirements of intelligent tasks?</td>\n",
       "      <td>The requirements of intelligent tasks include ...</td>\n",
       "      <td>Question:\\nWhat are the requirements of intell...</td>\n",
       "      <td>Answer:\\nThe requirements of intelligent tasks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>What are the limitations of CoAxNN?</td>\n",
       "      <td>The limitations of CoAxNN include the use of a...</td>\n",
       "      <td>Question:\\nWhat are the limitations of CoAxNN?</td>\n",
       "      <td>Answer:\\nThe limitations of CoAxNN include the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>What future studies will be explored in CoAxNN?</td>\n",
       "      <td>Future studies in CoAxNN will include explorin...</td>\n",
       "      <td>Question:\\nWhat future studies will be explore...</td>\n",
       "      <td>Answer:\\nFuture studies in CoAxNN will include...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>What are the different sensitives for model ac...</td>\n",
       "      <td>Different layers have different sensitives for...</td>\n",
       "      <td>Question:\\nWhat are the different sensitives f...</td>\n",
       "      <td>Answer:\\nDifferent layers have different sensi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>What will be explored in future studies?</td>\n",
       "      <td>Setting different pruning ratios for different...</td>\n",
       "      <td>Question:\\nWhat will be explored in future stu...</td>\n",
       "      <td>Answer:\\nSetting different pruning ratios for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Question  \\\n",
       "0          What is the main contribution of the paper?   \n",
       "1    What are the keywords associated with this art...   \n",
       "2                  What is the main goal of the paper?   \n",
       "3    What are the approximate strategies used in th...   \n",
       "4    What is the purpose of combining different app...   \n",
       "..                                                 ...   \n",
       "251    What are the requirements of intelligent tasks?   \n",
       "252                What are the limitations of CoAxNN?   \n",
       "253    What future studies will be explored in CoAxNN?   \n",
       "254  What are the different sensitives for model ac...   \n",
       "255           What will be explored in future studies?   \n",
       "\n",
       "                                                Answer  \\\n",
       "0    The paper proposes a new method for optimizing...   \n",
       "1    The keywords associated with this article are ...   \n",
       "2    The main goal of the paper is to develop a new...   \n",
       "3                              Neural network pruning.   \n",
       "4    To improve the performance of on-device infere...   \n",
       "..                                                 ...   \n",
       "251  The requirements of intelligent tasks include ...   \n",
       "252  The limitations of CoAxNN include the use of a...   \n",
       "253  Future studies in CoAxNN will include explorin...   \n",
       "254  Different layers have different sensitives for...   \n",
       "255  Setting different pruning ratios for different...   \n",
       "\n",
       "                                         text_question  \\\n",
       "0    Question:\\nWhat is the main contribution of th...   \n",
       "1    Question:\\nWhat are the keywords associated wi...   \n",
       "2       Question:\\nWhat is the main goal of the paper?   \n",
       "3    Question:\\nWhat are the approximate strategies...   \n",
       "4    Question:\\nWhat is the purpose of combining di...   \n",
       "..                                                 ...   \n",
       "251  Question:\\nWhat are the requirements of intell...   \n",
       "252     Question:\\nWhat are the limitations of CoAxNN?   \n",
       "253  Question:\\nWhat future studies will be explore...   \n",
       "254  Question:\\nWhat are the different sensitives f...   \n",
       "255  Question:\\nWhat will be explored in future stu...   \n",
       "\n",
       "                                           text_answer  \n",
       "0    Answer:\\nThe paper proposes a new method for o...  \n",
       "1    Answer:\\nThe keywords associated with this art...  \n",
       "2    Answer:\\nThe main goal of the paper is to deve...  \n",
       "3                     Answer:\\nNeural network pruning.  \n",
       "4    Answer:\\nTo improve the performance of on-devi...  \n",
       "..                                                 ...  \n",
       "251  Answer:\\nThe requirements of intelligent tasks...  \n",
       "252  Answer:\\nThe limitations of CoAxNN include the...  \n",
       "253  Answer:\\nFuture studies in CoAxNN will include...  \n",
       "254  Answer:\\nDifferent layers have different sensi...  \n",
       "255  Answer:\\nSetting different pruning ratios for ...  \n",
       "\n",
       "[256 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /home/aribra/.local/lib/python3.10/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /home/aribra/.local/lib/python3.10/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/aribra/.local/lib/python3.10/site-packages (from rouge_score) (1.26.2)\n",
      "Collecting six>=1.14.0 (from rouge_score)\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: click in /home/aribra/.local/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/aribra/.local/lib/python3.10/site-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/aribra/.local/lib/python3.10/site-packages (from nltk->rouge_score) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/aribra/.local/lib/python3.10/site-packages (from nltk->rouge_score) (4.65.2)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=8629f48e85b36deedd1e3d3668a48765fe7de9d7de6dda940275caa4d3b5cb3e\n",
      "  Stored in directory: /home/aribra/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: six, rouge_score\n",
      "Successfully installed rouge_score-0.1.2 six-1.16.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ! pip3 install evaluate\n",
    "! pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "eval_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = list( df['text_answer'].values )\n",
    "predictions = list( df['text_answer'].values )\n",
    "m = eval_metric.compute(predictions = predictions, references = references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp[roug]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "308it [00:00, 44982.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What is the main contribution of the paper? Answer:\n",
      "The paper proposes a new method for optimizing on-device deep learning with conditional approximate neural networks.\n",
      "Question:\n",
      "What are the keywords associated with this article? Answer:\n",
      "The keywords associated with this article are on-device deep learning, efficient neural networks, model approximation and optimization.\n",
      "Question:\n",
      "What is the main goal of the paper? Answer:\n",
      "The main goal of the paper is to develop a new method for deploying deep neural networks on resource-constrained devices while reducing their computational complexity.\n",
      "Question:\n",
      "What are the approximate strategies used in the study? Answer:\n",
      "Neural network pruning.\n",
      "Question:\n",
      "What is the purpose of combining different approximate strategies? Answer:\n",
      "To improve the performance of on-device inference.\n",
      "Question:\n",
      "What is the proposed model optimization framework called? Answer:\n",
      "CoAxNN.\n",
      "Question:\n",
      "What is the main goal of the proposed model optimization framework? Answer:\n",
      "The main goal of the proposed model optimization framework is to effectively combine different approximate strategies to facilitate on-device deep learning via model approximation.\n",
      "Question:\n",
      "What are the principles of different approximate optimizations used in the proposed approach? Answer:\n",
      "The principles of different approximate optimizations used in the proposed approach include the use of different approximation methods such as quantization, pruning, and knowledge distillation.\n",
      "Question:\n",
      "What is the effectiveness of CoAxNN according to the experimental results? Answer:\n",
      "The effectiveness of CoAxNN is demonstrated through experimental results, which show that it achieves up to 1.53× speedup while reducing energy by up to 34.61%, with trivial accuracy loss on CIFAR-10/100 and CINIC-10 datasets.\n",
      "Question:\n",
      "What is the main goal of the text? Answer:\n",
      "The main goal of the text is to introduce a new approach for efficient on-device deep learning.\n",
      "Question:\n",
      "What are the limitations of current deep learning models? Answer:\n",
      "Current deep learning models are becoming wider and deeper, leading to tremendous computational costs and expensive energy consumption for model execution.\n",
      "Question:\n",
      "What are the potential benefits of on-device deep learning? Answer:\n",
      "On-device deep learning has the potential for privacy protection and real-time response.\n",
      "Question:\n",
      "What are the efforts made to enable efficient on-device deep learning? Answer:\n",
      "Pruning-based strategies and quantization-based methods.\n",
      "Question:\n",
      "What are the challenges in combining different optimization strategies? Answer:\n",
      "It is still a challenging problem to effectively combine them.\n",
      "Question:\n",
      "What are the approximate strategies based on distinct principles? Answer:\n",
      "Emerging staging-based approximate strategies.\n",
      "Question:\n",
      "What is the main aim of this paper? Answer:\n",
      "The main aim of this paper is to address the challenging problem of designing an efficient model optimization framework to make full advantage of the superiority of different optimization strategies.\n",
      "Question:\n",
      "What is the key issue in combining different strategies? Answer:\n",
      "The key issue in combining different strategies is that the configuration parameters of different strategies may affect each other, influencing the optimization effect of the model, and even leading to poor optimization results.\n",
      "Question:\n",
      "Who are the corresponding authors of this paper? Answer:\n",
      "The corresponding authors of this paper are G. Li, X. Ma, Q. Yu, L. Liu, H. Liu, and W. Xueying.\n",
      "Question:\n",
      "Who contributed equally to this work? Answer:\n",
      "Guangli Li and Xiu Ma.\n",
      "Question:\n",
      "When was this work received in revised form? Answer:\n",
      "Received in revised form on 18 July 2023.\n",
      "Question:\n",
      "What is the novel neural network optimization framework presented in this paper? Answer:\n",
      "CoAxNN (Conditional Approximate Neural Networks).\n",
      "Question:\n",
      "What is the main idea of the text? Answer:\n",
      "The main idea of the text is that the authors have developed a new deep learning framework called CoAxNN, which combines staging-based and pruning-based approximate strategies for efficient on-device deep learning.\n",
      "Question:\n",
      "What are the two main optimization techniques used in CoAxNN? Answer:\n",
      "The two main optimization techniques used in CoAxNN are pruning and staging.\n",
      "Question:\n",
      "What is the key novelty of CoAxNN compared to previous work? Answer:\n",
      "The key novelty of CoAxNN is that it provides an effective and efficient mechanism to combine pruning and staging, allowing for more efficient model optimization.\n",
      "Question:\n",
      "What are the optimization techniques studied in the paper? Answer:\n",
      "The optimization techniques studied in the paper include pruning and staging.\n",
      "Question:\n",
      "What is the main contribution of the paper? Answer:\n",
      "The main contribution of the paper is the presentation of a novel neural network optimization framework, CoAxNN, which effectively combines staging-based and pruning-based approximate strategies to improve actual performance while meeting accuracy requirements for efficient on-device model inference.\n",
      "Question:\n",
      "How does the framework construct the design space? Answer:\n",
      "According to the principles of staging-based and pruning-based approximate strategies, the framework constructs the design space and automatically searches for reasonable configuration parameters, including the number of stages, the position of stages, the threshold of stages, and the pruning rate, so as to make efficient on-device model inference.\n",
      "Question:\n",
      "What is the main goal of CoAxNN? Answer:\n",
      "The main goal of CoAxNN is to achieve efficient model optimization by making full use of the advantages of both approximate strategies and the design space.\n",
      "Question:\n",
      "What does the phrase \"trivial accuracy loss\" mean in the context of the paper? Answer:\n",
      "In the context of the paper, the phrase \"trivial accuracy loss\" means that the performance of the optimized model is very close to the performance of the state-of-the-art model, with only a small difference in accuracy.\n",
      "Question:\n",
      "What is the main contribution of the paper? Answer:\n",
      "The main contribution of the paper is the development of a novel optimization framework that can significantly improve the performance of model inference on commercial edge devices, such as Jetson AGX Orin, by leveraging approximate strategies and the design space.\n",
      "Question:\n",
      "What is the main idea of the text? Answer:\n",
      "The main idea of the text is to introduce a new optimization framework for neural network models.\n",
      "Question:\n",
      "What are the details of the optimization framework described in Section 3? Answer:\n",
      "The details of the optimization framework described in Section 3 are the pruning-based approximation and the structured pruning.\n",
      "Question:\n",
      "What is the purpose of the experimental evaluation in Section 4? Answer:\n",
      "The purpose of the experimental evaluation in Section 4 is to evaluate the performance of the optimized models.\n",
      "Question:\n",
      "What are the two methods of pruning used in the text? Answer:\n",
      "Magnitude-based pruning and dynamic network surgery.\n",
      "Question:\n",
      "What is the purpose of pruning according to the text? Answer:\n",
      "To reduce the storage and computation demands by an order of magnitude.\n",
      "Question:\n",
      "What are two approaches used for pruning according to the text? Answer:\n",
      "Optimal Brain Damage (OBD) and Optimal Brain Surgeon (OBS).\n",
      "Question:\n",
      "What are the main challenges in optimizing the OBD method? Answer:\n",
      "The main challenges in optimizing the OBD method are the condition that the Hessian matrix is non-diagonal and the difficulty in supporting these approaches with existing software and hardware.\n",
      "Question:\n",
      "What are unstructured sparse models? Answer:\n",
      "Unstructured sparse models are models that require specific matrix multiplication calculations and storage formats, which can hardly leverage existing high-efficiency BLAS libraries.\n",
      "Question:\n",
      "What are filter pruning methods? Answer:\n",
      "Filter pruning methods are methods that dynamically prune filters in a soft manner, which zeroizes the unimportant filters and keeps updating them in the training stage.\n",
      "Question:\n",
      "What is the main focus of much recent work in deep learning? Answer:\n",
      "The main focus of much recent work in deep learning is on filter pruning methods.\n",
      "Question:\n",
      "What is the purpose of dynamically pruning filters in a soft manner? Answer:\n",
      "The purpose of dynamically pruning filters in a soft manner is to zeroize the unimportant filters and keep updating them in the training stage.\n",
      "Question:\n",
      "What is the goal of Li et al.'s fusioncatalyzed filter pruning approach? Answer:\n",
      "The goal of Li et al.'s fusioncatalyzed filter pruning approach is to simultaneously optimize the parametric and non-parametric operators.\n",
      "Question:\n",
      "What is the goal of the hardware-aware pruning method introduced by Plochaet et al.? Answer:\n",
      "To decrease the inference time for FPGA deep learning accelerators by adaptively pruning the neural network based on the size of the systolic array used to calculate the convolutions.\n",
      "Question:\n",
      "What is the importance criterion proposed by Zhuang et al. to evaluate the importance of filters in structured pruning? Answer:\n",
      "An effective filter importance criterion to estimate the contribution of filters to the adversarial training loss.\n",
      "Question:\n",
      "Can pruning be useful as an architecture search paradigm? Answer:\n",
      "Yes, according to Liu et al., pruning can be useful as an architecture search paradigm in some cases.\n",
      "Question:\n",
      "What is the main contribution of Li et al. [15]? Answer:\n",
      "Li et al. [15] proposed a random architecture search to find a good architecture given a pre-defined model by channel pruning.\n",
      "Question:\n",
      "What is the proposed method in Li et al. [16]? Answer:\n",
      "Li et al. [16] proposed an end-to-end channel pruning method to search out the desired sub-network automatically and efficiently, which learns per-layer sparsity through depth-wise binary convolution.\n",
      "Question:\n",
      "What is the main contribution of Ding et al. [17]? Answer:\n",
      "Ding et al. [17] presented a neural architecture search with pruning method, which derives the most potent model by removing trivial and redundant edges from the whole neural network topology.\n",
      "Question:\n",
      "What technique do the authors adopt to realize practical performance improvement for neural network models? Answer:\n",
      "Filter pruning.\n",
      "Question:\n",
      "What are staging-based approximation techniques? Answer:\n",
      "Early exiting and layer skipping.\n",
      "Question:\n",
      "What is the main idea of Teerapittayanon et al.'s work? Answer:\n",
      "That a deep neural network with additional side branch classifiers can both improve accuracy and significantly reduce the inference time of the network.\n",
      "Question:\n",
      "What is adaptive computing? Answer:\n",
      "Adaptive computing is a technique that enables a system to adjust its computing resources according to the conditions at run-time.\n",
      "Question:\n",
      "What did Teerapittayanon et al. demonstrate? Answer:\n",
      "Teerapittayanon et al. demonstrated that a deep neural network with additional side branch classifiers can both improve accuracy and significantly reduce the inference time of the network.\n",
      "Question:\n",
      "What did Fang et al. present? Answer:\n",
      "Fang et al. presented an input-adaptive framework for video analytics, which adopts an architecture search-based scheme to find the optimal architecture for each early exit branch.\n",
      "Question:\n",
      "What is the main contribution of the paper by Li et al.? Answer:\n",
      "The main contribution of the paper by Li et al. is the design of dynamic layer-skipping mechanisms to suppress unnecessary costs for easy samples and halt inference for all samples to meet resource constraints for the inference of more complicated CNN backbones.\n",
      "Question:\n",
      "What did Figurnov et al. study in their paper? Answer:\n",
      "Figurnov et al. studied early termination in each residual unit of ResNets.\n",
      "Question:\n",
      "What is the main idea behind the earlyexiting method proposed by Farhadi et al.? Answer:\n",
      "The main idea behind the earlyexiting method proposed by Farhadi et al. is to reduce the amount of needed computation on the FPGA platform using partial reconfiguration.\n",
      "Question:\n",
      "What does the text say about the exit position of the sample in the multi-branch network? Answer:\n",
      "According to the text, the exit position of the sample in the multi-branch network is directly determined by the difficulty of the sample without intermediate trial errors.\n",
      "Question:\n",
      "What is the purpose of the low-cost early exit network proposed by Jo et al.? Answer:\n",
      "The purpose of the low-cost early exit network proposed by Jo et al. is to significantly improve energy efficiencies by reducing the parameters used in inference with efficient branch structures.\n",
      "Question:\n",
      "What is the main contribution of the paper? Answer:\n",
      "The main contribution of the paper is achieving a multi-stage approximate model by early exiting to accelerate model inference for input samples in real-world scenarios.\n",
      "Question:\n",
      "What is the main problem addressed in the text? Answer:\n",
      "The main problem addressed in the text is Design Space Exploration (DSE) for neural network models.\n",
      "Question:\n",
      "How did Panda et al. [19] and Teerapittayanon et al. [18] set the location and threshold for each exit in the conditional neural network model? Answer:\n",
      "Panda et al. [19] and Teerapittayanon et al. [18] empirically set the location and threshold for each exit in the conditional neural network model through experimentation.\n",
      "Question:\n",
      "How did Jayakodi et al. [24] find the best thresholds for the specified trade-off between accuracy and energy consumption of inference? Answer:\n",
      "Jayakodi et al. [24] found the best thresholds via Bayesian Opt\n",
      "Question:\n",
      "What is the main contribution of Park et al. [28] in the field of DSE? Answer:\n",
      "Park et al. [28] integrated the once-for-all technique and BPNet, which consider architectures of base network and exit branches simultaneously in the same search process.\n",
      "Question:\n",
      "What is the key idea of Li et al. [11]'s proposed method for filter pruning? Answer:\n",
      "Li et al. [11] proposed a flexible-rate filter pruning method, which selects the filters to be pruned with a greedy-based strategy.\n",
      "Question:\n",
      "What is the main advantage of Qian et al. [30]'s proposed method for DSE? Answer:\n",
      "Qian et al. [30] proposed a hierarchical threshold pruning method, which considers the filter importance within relatively\n",
      "Question:\n",
      "What is the main goal of the paper? Answer:\n",
      "The main goal of the paper is to achieve efficient on-device inference by automatically finding the (near-)optimal configuration of layerwise pruning for a better network structure.\n",
      "Question:\n",
      "What is the focus of the pruning-based approximate strategy? Answer:\n",
      "The focus of the pruning-based approximate strategy is to compress the model by deleting unimportant parameters in the model, which reduces the computation costs.\n",
      "Question:\n",
      "What is the main objective of the staging-based approximate strategy? Answer:\n",
      "The main objective of the staging-based approximate strategy is to improve the execution speed of the model, which allows the inference of most simple samples to terminate with a good prediction in the shortest time possible.\n",
      "Question:\n",
      "What are the configuration parameters considered in the staging-based approximate strategy? Answer:\n",
      "The configuration parameters considered in the staging-based approximate strategy include how to place the exits and how to set a threshold for each exit.\n",
      "Question:\n",
      "How does the staging-based approximate strategy improve the execution speed of the model? Answer:\n",
      "The staging-based approximate strategy improves the execution speed of the model by allowing the inference of most simple samples to terminate with a good prediction in the earlier stage by attaching multiple exits in the original model.\n",
      "Question:\n",
      "How does the combination of different approximate strategies affect the effect of the model optimization? Answer:\n",
      "The combination of different approximate strategies may affect each other and potentially influences the effect of the model optimization.\n",
      "\n",
      "Note: The numbers in the output are for illustration purposes only and do not reflect the actual values used\n",
      "Question:\n",
      "What is shown in Fig. 1? Answer:\n",
      "The optimization effect of the ResNet-56 using different configuration parameters under the specified requirements of accuracy on the CIFAR-10 dataset.\n",
      "Question:\n",
      "What do the triples (𝑥, 𝑦, 𝑧) represent in Fig. 1? Answer:\n",
      "The number of stages, stage threshold, and pruning rate, respectively.\n",
      "Question:\n",
      "What is the relationship between the number of stages and the computational cost shown in Fig. 1? Answer:\n",
      "The relationship is not regular and will be affected by the stage threshold and pruning rate.\n",
      "Question:\n",
      "What is the relationship between the number of stages and the computational cost in the context of the text? Answer:\n",
      "The relationship between the number of stages and the computational cost is not regular, which means that the computational cost of a model with more stages may not always be larger than a model with fewer stages.\n",
      "Question:\n",
      "How does the staging-based optimization affect the computational cost of a model? Answer:\n",
      "The staging-based optimization can cause the computational cost of a model to increase or decrease, depending on the pruning rate used.\n",
      "Question:\n",
      "What can be observed from the partial experimental results in Fig. 1? Answer:\n",
      "From the partial experimental results, it can be observed that at a certain accuracy requirement (98.1% in this case), the computational cost of the optimized models using three stages is less than that of the model using two stages.\n",
      "Question:\n",
      "What does the paper conclude about the optimization effect of different configuration parameters? Answer:\n",
      "The paper concludes that the optimization effects of different configuration parameters are distinct and irregular under the specified accuracy requirement, making it difficult to find an optimal model.\n",
      "Question:\n",
      "What does the paper observe in Fig. 1(a)? Answer:\n",
      "The paper observes that at the accuracy requirement of 98.1%, the computation of the optimized models using three stages is less than that of the model using two stages.\n",
      "Question:\n",
      "What does the paper show in Fig. 2? Answer:\n",
      "The paper shows that the optimization effect of staging-based strategy, pruning-based strategy, and CoAxNN for ResNet-56 on the CIFAR-10.\n",
      "Question:\n",
      "What is the main focus of the paper? Answer:\n",
      "The main focus of the paper is to combine different approximate strategies to achieve efficient optimization for neural network models.\n",
      "Question:\n",
      "What is the normalized computational cost of the staging-based optimization strategy? Answer:\n",
      "The normalized computational cost of the staging-based optimization strategy is 0.89.\n",
      "Question:\n",
      "What is the computational cost of CoAxNN? Answer:\n",
      "The computational cost of CoAxNN is 0.64, which is greatly improved compared to the staging-based and pruning-based strategies.\n",
      "Question:\n",
      "What is the computational cost of CoAxNN? Answer:\n",
      "The computational cost of CoAxNN is 0.64.\n",
      "Question:\n",
      "What is the pruning rate of CoAxNN? Answer:\n",
      "The pruning rate of CoAxNN is 0.1.\n",
      "Question:\n",
      "What is the normalized computational cost of CoAxNN? Answer:\n",
      "The normalized computational cost of CoAxNN is 0.89.\n",
      "Question:\n",
      "What does CoAxNN perform according to the genes of the chromosome for each individual? Answer:\n",
      "CoAxNN performs staging-based and pruning-based approximate strategies according to the genes of the chromosome for each individual, which generates a compressed multi-stage model.\n",
      "Question:\n",
      "What does CoAxNN attach to the original model? Answer:\n",
      "According to the availability of stages in the gene, CoAxNN attaches exit branches to the original model to build a multistage conditional activation model.\n",
      "Question:\n",
      "What does CoAxNN predict according to the threshold of each stage? Answer:\n",
      "According to the threshold of each stage, CoAxNN predicts input samples, having distinct difficulties, by multiple stages with different computational complexities, with the entropy-aware activation manner.\n",
      "Question:\n",
      "What is the purpose of removing unimportant filters in the text? Answer:\n",
      "The purpose of removing unimportant filters is to reduce computational costs.\n",
      "Question:\n",
      "What is the next step after evaluating the fitness of the corresponding individuals according to the accuracy and latency of the compressed multi-stage model? Answer:\n",
      "The next step is to update the chromosome pool, generating the next generation of individuals.\n",
      "Question:\n",
      "What is the general approach of executing a neural network model? Answer:\n",
      "The general approach of executing a neural network model is a one-staged approach, which processes all the inputs in the same manner, starting from the input operator and performing it operator by operator until the final exit operator.\n",
      "Question:\n",
      "What is the purpose of the early exiting strategy in CoAxNN? Answer:\n",
      "The purpose of the early exiting strategy in CoAxNN is to give an early exiting opportunity for simple inputs.\n",
      "Question:\n",
      "What does the notation  ∗ represent in CoAxNN? Answer:\n",
      "The notation  ∗ represents a multi-stage model in CoAxNN.\n",
      "Question:\n",
      "What is the purpose of the number of stages 𝜏 in CoAxNN? Answer:\n",
      "The purpose of the number of stages 𝜏 in CoAxNN is to formalize the early exiting strategy in CoAxNN.\n",
      "Question:\n",
      "What is the main difference between the original neural network model and the approximate model with the staging-based strategy? Answer:\n",
      "The main difference between the original neural network model and the approximate model with the staging-based strategy is that the approximate model has additional components, such as the exit checker and the additional exit branch, which are not present in the original model. These additional components allow the approximate model to adaptively condition the inference based on the input data.\n",
      "Question:\n",
      "What is the purpose of the exit checker in the staging-based strategy? Answer:\n",
      "The purpose of the exit checker in the staging-based strategy is to determine when to exit the inference process based on the input data. The exit checker uses a threshold and a conditional activation operator to decide when to exit the inference process, allowing the model to adaptively condition the inference based on the input data.\n",
      "Question:\n",
      "What does 𝜏 represent in the given text? Answer:\n",
      "𝜏 represents the original (main) neural network model.\n",
      "Question:\n",
      "What is the significance of 𝑐𝑖 in the given text? Answer:\n",
      "𝑐𝑖 represents the use of threshold 𝜀𝑖.\n",
      "Question:\n",
      "What is the meaning of  in the given text? Answer:\n",
      " represents the neural network model.\n",
      "Question:\n",
      "What is the main problem with exit branches in deep learning? Answer:\n",
      "Exit branches may interfere with computational graph optimization methods, such as operator fusion and memory reuse, provided by deep learning frameworks, increasing operation counts, data movement, and other system overheads.\n",
      "Question:\n",
      "What is the confidence threshold used for in deep learning? Answer:\n",
      "The confidence threshold is used to determine whether the prediction result of stage 𝑖 has sufficient confidence. With a higher threshold, complex samples may finish predictions from the previous exits with lower accuracy, and using a lower threshold, simple samples may use more complex computations to complete inference, due to cannot end from the previous classifiers, incurring additional computational overheads.\n",
      "Question:\n",
      "What is the structure of each exit branch in deep learning? Answer:\n",
      "The structure of each exit branch (𝒊) consists of several operators\n",
      "Question:\n",
      "What are the different types of operators used for feature extraction in the text? Answer:\n",
      "The different types of operators used for feature extraction in the text are {𝑓 ∗, … , 𝑓 ∗\n",
      "𝑏𝑖−1}.\n",
      "Question:\n",
      "What is the purpose of the 𝑓 ∗\n",
      "𝑏𝑖\n",
      "operator in the text? Answer:\n",
      "The purpose of the 𝑓 ∗\n",
      "𝑏𝑖\n",
      "operator in the text is to produce classification results based on the output of 𝑓 ∗\n",
      ".\n",
      "Question:\n",
      "What is the difference in the configuration and complexity of the intermediate feature maps for different depths of the main neural networks? Answer:\n",
      "The difference in the configuration and complexity of the intermediate feature maps for different depths of the\n",
      "Question:\n",
      "What are the two factors that will affect each other in the early-exiting method? Answer:\n",
      "The number (𝜏) and the position (𝒑𝒊) of the exit branches (𝒊) are two factors that will affect each other in the early-exiting method.\n",
      "Question:\n",
      "What is the purpose of putting the availability of each stage into the design space of the GA in CoAxNN? Answer:\n",
      "To address the problem of increasing the number of exit branches to reduce the computational cost while meeting the model accuracy requirement, CoAxNN puts the availability of each stage into the design space of the GA, and each available stage corresponds to a different solution in the search space.\n",
      "Question:\n",
      "What is the main problem that CoAxNN addresses in the text? Answer:\n",
      "CoAxNN addresses the problem of increasing the number of exit branches to reduce computational cost while meeting model accuracy requirements.\n",
      "Question:\n",
      "How does CoAxNN choose to attach exit branches? Answer:\n",
      "CoAxNN chooses to attach exit branches at the end of the group of building blocks, and does not attach the exit branch after the last group of building blocks as there is already an existing original exit for the original backbone.\n",
      "Question:\n",
      "What is the purpose of introducing a feature extractor and a linear classifier for each exit branch? Answer:\n",
      "The purpose of introducing a feature extractor and a linear classifier for each exit branch is to retain the original neural network structure and introduce new features for each exit branch.\n",
      "Question:\n",
      "What is the purpose of introducing a feature extractor and a linear classifier in CoAxNN? Answer:\n",
      "The purpose of introducing a feature extractor and a linear classifier in CoAxNN is to improve the efficiency of the network for easy samples that exit early.\n",
      "\n",
      "    𝑖. Question: What is the design of the feature extractor in CoAxNN?\n",
      "Answer: The feature extractor in CoAxNN is designed with the building block as granularity, which not only retains the original neural network structure but also provides more opportunities for system-level optimizations.\n",
      "\n",
      "    𝑖. Question: What are the activation operators used in the feature extractor of CoAxNN?\n",
      "Answer: The activation operators used in the feature extractor of CoAxNN are non-linear activation operators such as rectified linear units and normalization operators such as batch\n",
      "Question:\n",
      "What is the main problem with linear classifiers? Answer:\n",
      "The main problem with linear classifiers is that they have a long latency for easy samples that exit early.\n",
      "Question:\n",
      "What is the purpose of adding an extra pooling operator in CoAxNN? Answer:\n",
      "The purpose of adding an extra pooling operator in CoAxNN is to evaluate the confidence of the prediction result for the input sample of the 𝑖th stage classifier.\n",
      "Question:\n",
      "What is the design of 𝑖 in CoAxNN? Answer:\n",
      "The design of 𝑖 in CoAxNN is to have\n",
      "Question:\n",
      "What is the purpose of using entropy as the entropy-aware activation operator in the CoAxNN model? Answer:\n",
      "The purpose of using entropy as the entropy-aware activation operator in the CoAxNN model is to evaluate the confidence of the prediction result for the input sample of the 𝑖th stage classifier.\n",
      "Question:\n",
      "What is the formula for calculating the entropy of the predicted probability distribution in the CoAxNN model? Answer:\n",
      "The formula for calculating the entropy of the predicted probability distribution in the CoAxNN model is:\n",
      "\n",
      "entropy( ̂𝑦𝑖) = 𝐶 ∑ ̂𝑦𝑖(𝑐) log ̂𝑦𝑖\n",
      "Question:\n",
      "What is the main advantage of using pruning-based approximate optimization in CoAxNN? Answer:\n",
      "It provides realistic performance improvements by removing redundant computations of unimportant filters.\n",
      "Question:\n",
      "What is the difference between structured and unstructured pruning in the neural network pruning technique? Answer:\n",
      "Structured pruning, such as filter pruning, has higher computational efficiency than unstructured pruning.\n",
      "Question:\n",
      "What is the importance of each filter in a convolutional operator based on the 𝓁2-norm? Answer:\n",
      "It is the amount of feature maps that are removed when a filter is deleted.\n",
      "Question:\n",
      "What is the purpose of pruning filters in CoAxNN? Answer:\n",
      "The purpose of pruning filters in CoAxNN is to remove corresponding feature maps, providing realistic performance improvements.\n",
      "Question:\n",
      "How does the filter pruning method work in CoAxNN? Answer:\n",
      "In CoAxNN, the filter pruning method works by compressing the multi-stage model and quantifying the importance of each filter in a convolutional operator based on the 𝓁2-norm.\n",
      "Question:\n",
      "What is the dynamic pruning scheme used in CoAxNN? Answer:\n",
      "The dynamic pruning scheme used in CoAxNN is a st\n",
      "Question:\n",
      "What is the purpose of utilizing a dynamic pruning scheme for staging-based approximate CNNs? Answer:\n",
      "To keep the model capacity and minimize the loss of accuracy as much as possible.\n",
      "Question:\n",
      "What is the purpose of joint training in the training process of neural network models with exit branches? Answer:\n",
      "Joint training defines a loss function for each classifier and minimizes the weighted sum of loss functions for all classifiers during training, which helps to alleviate the overfitting of the model.\n",
      "Question:\n",
      "What is the main objective of CoAxNN?\n",
      "    2. Answer:\n",
      "The main objective of CoAxNN is to train the backbone neural network and exit branches at the same time and minimize the weighted sum of the cross-entropy loss functions of all stages.\n",
      "Question:\n",
      "What is the formula used to calculate the cross-entropy loss function in CoAxNN?\n",
      "    4. Answer:\n",
      "The formula used to calculate the cross-entropy loss function in CoAxNN is given by CE(𝑦, ̄𝑦𝑖) = −𝐶∑𝑐=1𝑦(𝑐)loge𝑓(𝑐) where 𝑓 is the output of the linear classifier of the 𝑖th stage\n",
      "Question:\n",
      "What is the purpose of CoAxNN? Answer:\n",
      "CoAxNN is a method for compressing and pruning deep neural networks.\n",
      "Question:\n",
      "What is the input to CoAxNN? Answer:\n",
      "The input to CoAxNN is a given training dataset, training epochs, batch size, original deep neural network model, number of stages, weights for loss functions of all stages, and the chromosome pool.\n",
      "Question:\n",
      "What is the output of CoAxNN? Answer:\n",
      "The output of CoAxNN is the number of filters (𝑡) and the 𝓁2-norm of each filter in the approximate multi-stage model.\n",
      "Question:\n",
      "What is the purpose of the dynamic pruning scheme in CoAxNN?\n",
      "    📝 Answer:\n",
      "The dynamic pruning scheme in CoAxNN is used to prune filters with low 𝓁2-norm, thus maintaining the learning ability of the model.\n",
      "\n",
      "    🚀 Question 2: How does CoAxNN update the weights of the traditional backpropagation algorithm?\n",
      "    📝 Answer 2: CoAxNN updates the weights of the traditional backpropagation algorithm according to the loss function according to Eq. (5).\n",
      "\n",
      "    🚀 Question 3: What is the purpose of reordering the importance of filters in the pruning process of each epoch in CoAxNN?\n",
      "    📝 Answer 3: The purpose of reordering the importance of filters in the pruning process\n",
      "Question:\n",
      "What is the purpose of the \"if P[p][i] is available then\" statement in the code? Answer:\n",
      "The \"if P[p][i] is available then\" statement is used to check if the loss function is available for the current iteration. If it is available, then the loss function is used to update the model parameters.\n",
      "Question:\n",
      "What is the purpose of the \"𝑙𝑜𝑠𝑠\" variable in the code? Answer:\n",
      "The \"𝑙𝑜𝑠𝑠\" variable is used to store the gradients of the loss function with respect to the model parameters.\n",
      "Question:\n",
      "What is the purpose of the line \"𝑤𝑒𝑖𝑔ℎ𝑡𝑒𝑑_𝑙𝑜𝑠𝑠 = 0;\" in the code? Answer:\n",
      "The line sets the initial value of the output variable 𝑤𝑒𝑖𝑔ℎ𝑡𝑒𝑑_𝑙𝑜𝑠𝑠 to 0, which is used as the input for the next\n",
      "Question:\n",
      "What is the search space for determining which stage is available in CoAxNN? Answer:\n",
      "The search space for determining which stage is available in CoAxNN is 2𝜏.\n",
      "Question:\n",
      "What is the search space for thresholds in CoAxNN? Answer:\n",
      "The search space for thresholds in CoAxNN is 𝑄𝜏, where 𝑄 is the number of candidate thresholds.\n",
      "Question:\n",
      "What is the search space for pruning rate in CoAxNN? Answer:\n",
      "The search space for pruning rate in CoAxNN is 𝑅, which indicates the number of candidate pruning rates.\n",
      "Question:\n",
      "What is the main idea of the text? Answer:\n",
      "The main idea of the text is that CoAxNN is a genetic algorithm-based dynamic system evolution (GA-DSE) method that uses a chromosome set to find the near-optimal solution in a large search space.\n",
      "Question:\n",
      "What is the purpose of the algorithm shown in Algorithm 2? Answer:\n",
      "The purpose of Algorithm 2 is to evaluate the accuracy and latency of individuals in the CoAxNN algorithm.\n",
      "Question:\n",
      "What is the output of the CoAxNN algorithm? Answer:\n",
      "The output\n",
      "Question:\n",
      "How is the prediction result obtained by CoAxNN? Answer:\n",
      "The prediction result is obtained by traversing all available stages and calculating the confidence of corresponding output at each stage according to Equation (3).\n",
      "Question:\n",
      "What is the confidence threshold used by CoAxNN? Answer:\n",
      "The confidence threshold used by CoAxNN is the confidence threshold (𝜀𝑖) of this stage.\n",
      "Question:\n",
      "What is the accuracy function returned by CoAxNN? Answer:\n",
      "The accuracy function returned by CoAxNN is 1 if the prediction is correct, and 0 otherwise.\n",
      "Question:\n",
      "How does CoAxNN evaluate the latency score? Answer:\n",
      "CoAxNN evaluates the latency score using a similar manner as the accuracy score, which accumulates the latency of the backbone neural network and exit branches until the end of the prediction.\n",
      "Question:\n",
      "How does GA-based DSE get the (near-)optimal solutions about the goal of accuracy and latency? Answer:\n",
      "GA-based DSE gets the (near-)optimal solutions about the goal of accuracy and latency by using a Genetic Algorithm to search for the best combination of parameters that maximizes the accuracy and minimizes the latency.\n",
      "Question:\n",
      "What is the output of CoAxNN? Answer:\n",
      "The output of CoAxNN is the average accuracy score (𝛿) and average latency score (𝜇) for all individuals.\n",
      "Question:\n",
      "What does the text highlight about the optimization process? Answer:\n",
      "The optimization process highlighted in the text is the use of GA-based DSE to obtain near-optimal solutions for the goal of accuracy and latency.\n",
      "Question:\n",
      "What is the purpose of removing unimportant filters in the optimization process? Answer:\n",
      "The purpose of removing unimportant filters in the optimization process is to obtain an optimized neural network model.\n",
      "Question:\n",
      "What is the significance of the accuracy requirement in the optimization process? Answer:\n",
      "The significance of the accuracy requirement in the optimization process is that it determines which model is selected according to the user's requirements. If the accuracy requirement is high, the model with the least computation cost is selected under a trivial accuracy loss, while if a\n",
      "Question:\n",
      "What is the purpose of the Algorithm 2 in the given text? Answer:\n",
      "The purpose of Algorithm 2 is to evaluate the performance of different configurations of neural network models on a given dataset.\n",
      "Question:\n",
      "What is the output of the Algorithm 2 in the given text? Answer:\n",
      "The output of Algorithm 2 is the accuracy and latency of each configuration of neural network models on the given dataset.\n",
      "Question:\n",
      "What is the significance of the line \"if P[p][i] is available then\" in the given text? Answer:\n",
      "The line \"if P[p][i] is available\n",
      "Question:\n",
      "What is the speedup of the optimized models compared to the original models on the Jetson AGX Orin platform? Answer:\n",
      "The optimized models show a speedup of approximately 1.73 times compared to the original models on the Jetson AGX Orin platform.\n",
      "Question:\n",
      "What is the energy consumption of the optimized models compared to the original models on the Jetson AGX Orin platform? Answer:\n",
      "The optimized models show an energy consumption reduction of approximately 0.64 times compared to the original models on the Jetson AGX Orin platform.\n",
      "Question:\n",
      "What is the effectiveness of the proposed method on the CIFAR dataset? Answer:\n",
      "The proposed method shows an effectiveness of approximately 0.\n",
      "Question:\n",
      "What are the two datasets that are part of the CIFAR dataset? Answer:\n",
      "The two datasets that are part of the CIFAR dataset are CIFAR-10 and CIFAR-100.\n",
      "Question:\n",
      "What is the number of classes in the CIFAR-10 dataset? Answer:\n",
      "The number of classes in the CIFAR-10 dataset is 10.\n",
      "Question:\n",
      "What is the number of images in the CINIC-10 dataset? Answer:\n",
      "The number of images in the CINIC-10 dataset is 27,000.\n",
      "Question:\n",
      "What is the main goal of the GA-based DSE in the text? Answer:\n",
      "The main goal of the GA-based DSE is to obtain (near-)optimal solutions about accuracy and latency.\n",
      "Question:\n",
      "What is the data argumentation strategy used in the GA-based DSE? Answer:\n",
      "The same data argumentation strategies and scheduling settings as [1] are used in the GA-based DSE.\n",
      "Question:\n",
      "What is the main conclusion that can be drawn from the results shown in Fig. 4? Answer:\n",
      "The main conclusion that can be drawn from the results shown in Fig. 4 is that the (near)optimal solutions found by CoAxNN are close to the boundary of the green and red regions, demonstrating the effectiveness of CoAxNN in searching for models with the least computational cost while meeting accuracy requirements.\n",
      "Question:\n",
      "What is CoAxNN? Answer:\n",
      "CoAxNN is a method that demonstrates the effectiveness of CoAxNN.\n",
      "Question:\n",
      "What is the purpose of using GA-based DSE in most cases? Answer:\n",
      "The purpose of using GA-based DSE is to search for the model having the least computational cost in most cases and meeting the accuracy requirements.\n",
      "Question:\n",
      "What is the meaning of \"ACC. Drop\" in the context of the text? Answer:\n",
      "\"ACC. Drop\" represents the accuracy dropping of the model after optimization. A smaller number of \"ACC. Drop\" is better, and a negative number indicates the optimized model has higher accuracy than the baseline model.\n",
      "\n",
      "Note: In the output, the questions and answers are numbered for clarity.\n",
      "Question:\n",
      "What does the \"ACC. Drop\" value represent in the text? Answer:\n",
      "The \"ACC. Drop\" value represents the accuracy dropping of the model after optimization.\n",
      "Question:\n",
      "What does a smaller \"ACC. Drop\" value indicate in the text? Answer:\n",
      "A smaller \"ACC. Drop\" value indicates that the optimized model has higher accuracy than the baseline model.\n",
      "Question:\n",
      "What does the term \"overfitting\" refer to in the text? Answer:\n",
      "Overfitting refers to the regularization effect of model optimization, which can reduce the overfitting of neural network models.\n",
      "Question:\n",
      "How does CoAxNN reduce the computational complexity of the ResNet-20 model while maintaining its accuracy? Answer:\n",
      "CoAxNN reduces the computational complexity of the ResNet-20 model by 25.94% while maintaining its accuracy.\n",
      "Question:\n",
      "How does CoAxNN compare to other state-of-the-art model optimization methods in terms of computational complexity and accuracy? Answer:\n",
      "CoAxNN consumes less computation cost than other state-of-the-art model optimization methods, such as SFP, while achieving comparable accuracy.\n",
      "Question:\n",
      "What is the computational cost of the optimized ResNet-32 model using CoAxNN? Answer:\n",
      "The optimized ResNet-32 model using CoAxNN has a computational cost of 3.44E7 FLOPs.\n",
      "Question:\n",
      "What does CoAxNN reduce the computational cost of? Answer:\n",
      "CoAxNN reduces the computational cost of ResNet-32, ResNet-56, and ResNet-110.\n",
      "Question:\n",
      "What is the accuracy drop of CoAxNN? Answer:\n",
      "CoAxNN's accuracy drop is 1.39% for ResNet-32, 1.58% for ResNet-56, and 1.22% for ResNet-110.\n",
      "Question:\n",
      "How does CoAxNN achieve a similar accuracy loss while reducing computational complexity? Answer:\n",
      "CoAxNN achieves a similar accuracy loss while reducing computational complexity by automatically searching for a reasonable configuration to effectively optimize the computational complexity while meeting the accuracy requirements.\n",
      "Question:\n",
      "What does the text say about the computational complexity of existing methods compared to CoAxNN? Answer:\n",
      "According to the text, CoAxNN reduces more computations than existing methods, achieving less resource consumption.\n",
      "Question:\n",
      "What is the difference in accuracy between the baseline and accelerated models in Table 1? Answer:\n",
      "The difference in accuracy between the baseline and accelerated models in Table 1 ranges from 0.67% to 1.39% for different models.\n",
      "Question:\n",
      "How does CoAxNN perform compared to other state-of-the-art methods in Table 1? Answer:\n",
      "According to Table 1, CoAxNN performs better than other state-of-the-art methods, achieving top-1 accuracy of 0.67% and 1.39% for different models.\n",
      "Question:\n",
      "What is the top-1 accuracy of the ResNet-56 model on the MNIST dataset? Answer:\n",
      "The top-1 accuracy of the ResNet-56 model on the MNIST dataset is 92.88%.\n",
      "Question:\n",
      "What is the FLOPs count of the CoAxNN model on the CIFAR-10 dataset? Answer:\n",
      "The FLOPs count of the CoAxNN model on the CIFAR-10 dataset is 2.61E7.\n",
      "Question:\n",
      "What is the top-1 accuracy of the SFP model on the TAS dataset? Answer:\n",
      "The top-1 accuracy of the SFP model on the TAS dataset is 0.74%.\n",
      "\n",
      "    Note: All answers are in percentage format.\n",
      "Question:\n",
      "What is the accuracy loss of ResNet-20? Answer:\n",
      "0.67%.\n",
      "Question:\n",
      "What is the weighted average FLOPs of ResNet-32? Answer:\n",
      "58.71% × 1.93E7 + 41.29% × 4.53E7 = 3.00E7.\n",
      "Question:\n",
      "How many stages are used in CoAxNN for ResNet-110? Answer:\n",
      "Three stages are used in CoAxNN for ResNet-110.\n",
      "\n",
      "Note: All answers are in the format of \"Answer: Value\" where \"Answer\" is the question answer and \"Value\" is the correct answer.\n",
      "Question:\n",
      "What does CoAxNN employ? Answer:\n",
      "CoAxNN employs distinct stages for different neural network models.\n",
      "Question:\n",
      "What are the two stages used for? Answer:\n",
      "The two stages are used for ResNet-20.\n",
      "Question:\n",
      "How many stages are used for more complex models? Answer:\n",
      "Three stages are used for more complex ResNet-32, ResNet-56, and ResNet-110.\n",
      "Question:\n",
      "What is the pruning rate for the optimized ResNet-20 model? Answer:\n",
      "The pruning rate for the optimized ResNet-20 model is 0, meaning no pruning is performed.\n",
      "Question:\n",
      "What is the number of stages in the optimized ResNet-20 model? Answer:\n",
      "The number of stages in the optimized ResNet-20 model is two.\n",
      "Question:\n",
      "What is the position of the first stage in the optimized ResNet-20 model? Answer:\n",
      "The position of the first stage in the optimized ResNet-20 model is the end of the fourth residual block.\n",
      "Question:\n",
      "What is the pruning rate of ResNet-32? Answer:\n",
      "0.67%\n",
      "Question:\n",
      "What is the position of the threshold for ResNet-56? Answer:\n",
      "The position of three stages is the end of the 10, 19th residual block.\n",
      "Question:\n",
      "What is the threshold of ResNet-110 for the last stage? Answer:\n",
      "0.07.\n",
      "Question:\n",
      "What is the position of three stages in the optimized ResNet-56? Answer:\n",
      "The position of three stages in the optimized ResNet-56 is the end of the 10, 19th residual block with the threshold of 0.07.\n",
      "Question:\n",
      "What is the position of three stages in the optimized ResNet-110? Answer:\n",
      "The position of three stages in the optimized ResNet-110 is the end of the 19, 37th residual block with the threshold of 0.07, 0.017.\n",
      "Question:\n",
      "How does CoAxNN perform on the CIFAR-100 dataset? Answer:\n",
      "CoAxNN outperforms other state-of-the-art methods on the CIFAR-100 dataset by ResNet-\n",
      "Question:\n",
      "What is the computation reduction achieved by CoAxNN? Answer:\n",
      "A higher computation reduction of 33.34% was achieved by CoAxNN.\n",
      "Question:\n",
      "What is the accuracy loss of the optimized model with CoAxNN? Answer:\n",
      "A lower accuracy loss of 1.30% was achieved by CoAxNN.\n",
      "Question:\n",
      "What is the computational complexity of the optimized model with CoAxNN? Answer:\n",
      "A higher computational complexity of 1.82E8 FLOPs was used by CoAxNN.\n",
      "\n",
      "Note: All answers are in the format of a number or a percentage.\n",
      "Question:\n",
      "What is the number of images predicted by the first few stages of the optimized model on CIFAR-100? Answer:\n",
      "The number of images predicted by the first few stages of the optimized model on CIFAR-100 is 29.67%, 32.85%, and 37.48%, respectively.\n",
      "Question:\n",
      "What is the percentage of images predicted by the first few stages of the optimized model on CIFAR-10? Answer:\n",
      "The percentage of images predicted by the first few stages of the optimized model on CIFAR-10 is 44.81%, 36.79%, and 18.40%, respectively.\n",
      "Question:\n",
      "What is the computational cost of the original ResNet-18 model? Answer:\n",
      "According to Table 8, the computational cost of the original ResNet-18 model is 5.49E8 FLOPs.\n",
      "Question:\n",
      "How much did the accuracy of the ResNet-18 model decrease when the FLOPs were reduced from 5.49E8 to 2.21E8? Answer:\n",
      "The accuracy of the ResNet-18 model decreased by 1.01% when the FLOPs were reduced from 5.49E8 to 2.21E8.\n",
      "Question:\n",
      "How much did the computational complexity of CoAxNN reduce while achieving a 0.50% accuracy gain for the ResNet-18 model? Answer:\n",
      "CoAxNN reduced\n",
      "Question:\n",
      "What is the computational complexity reduction achieved by CoAxNN? Answer:\n",
      "CoAxNN achieves a computational complexity reduction of 49.73% (5.93E8 FLOPs) compared to the original ResNet-50 model.\n",
      "Question:\n",
      "How does CoAxNN improve the top-1 accuracy of the ResNet-50 model? Answer:\n",
      "CoAxNN improves the top-1 accuracy of the ResNet-50 model by 0.38% compared to the original model.\n",
      "Question:\n",
      "What is the computational complexity reduction achieved by FPC? Answer:\n",
      "FPC reduces the computational complexity of the ResNet-50 model by 40.48% (7.76E8 FLOPs) compared to the original model.\n",
      "Question:\n",
      "What is the accuracy drop of the ResNet-18 and ResNet-50 in Table 8? Answer:\n",
      "The accuracy drop of the ResNet-18 and ResNet-50 in Table 8 is 1.01% and −0.10%, respectively.\n",
      "Question:\n",
      "What is the number of stages used by the ResNet-18 and ResNet-50 in Table 9? Answer:\n",
      "The ResNet-18 and ResNet-50 use four stages in Table 9.\n",
      "Question:\n",
      "What is the pruning rate of the ResNet-18 in Table 9? Answer:\n",
      "The pruning rate of the ResNet-18 in Table 9 is 0.3.\n",
      "Question:\n",
      "What are the top-1 accuracy percentages for the optimized neural network models on CIFAR-100? Answer:\n",
      "The top-1 accuracy percentages for the optimized neural network models on CIFAR-100 are 71.33%, 72.75%, and 72.79% for ResNet-56, ResNet-110, and CoAxNN, respectively.\n",
      "Question:\n",
      "What is the percentage drop in FLOPs for the optimized models compared to the baseline model? Answer:\n",
      "The percentage drop in FLOPs for the optimized models compared to the baseline model is 39.30%, 23.93%, and 40.53% for ResNet-56, ResNet-110, and CoAxNN, respectively.\n",
      "Question:\n",
      "What is the percentage of accuracy drop for the ResNet-56 model with the optimal configuration found by CoAxNN? Answer:\n",
      "The percentage of accuracy drop for the ResNet-56 model with the optimal configuration found by CoAxNN is 0.98%.\n",
      "Question:\n",
      "What is the number of FLOPs for the ResNet-110 model with the optimal configuration found by CoAxNN? Answer:\n",
      "The number of FLOPs for the ResNet-110 model with the optimal configuration found\n",
      "Question:\n",
      "How does CoAxNN compare to state-of-the-art methods? Answer:\n",
      "CoAxNN is comparable to state-of-the-art methods in terms of performance.\n",
      "Question:\n",
      "What is the main advantage of using staging-based approximate strategies? Answer:\n",
      "The main advantage of using staging-based approximate strategies is that the inference of simple inputs can be terminated with a good prediction confidence in the earlier stage, thereby avoiding remaining layerwise computations.\n",
      "Question:\n",
      "What is the main disadvantage of pruning-based approximate strategies? Answer:\n",
      "The main disadvantage of pruning-based approximate strategies is that the pruning method lacks the ability to configure the neural network dynamically, which will miss the opportunities to optimize the model inference.\n",
      "Question:\n",
      "What is the main goal of the paper? Answer:\n",
      "The main goal of the paper is to present a new method for efficient model optimization.\n",
      "Question:\n",
      "What is the problem with the pruning method? Answer:\n",
      "The pruning method lacks the ability to configure the neural network dynamically, which will miss the opportunities to optimize the model inference.\n",
      "Question:\n",
      "What is the advantage of CoAxNN compared to other methods? Answer:\n",
      "CoAxNN automatically finds (near-)optimal configurations by GA-based DSE, making full use of the advantages of both, thus achieving efficient model optimization.\n",
      "Question:\n",
      "What is the average inference latency for CoAxNN on the CIFAR10 dataset? Answer:\n",
      "0.67%\n",
      "Question:\n",
      "What is the speedup of CoAxNN for the ResNet-20 model on the CIFAR10 dataset? Answer:\n",
      "1.33×\n",
      "Question:\n",
      "What is the largest model accelerated by CoAxNN in the table? Answer:\n",
      "ResNet-\n",
      "Question:\n",
      "What is the percentage of accuracy loss achieved by CoAxNN on the CIFAR-10 dataset? Answer:\n",
      "The percentage of accuracy loss achieved by CoAxNN on the CIFAR-10 dataset is 0.67%, 0.84%, 0.74%, and 0.63%.\n",
      "Question:\n",
      "What is the speedup achieved by CoAxNN on different models? Answer:\n",
      "CoAxNN can accelerate ResNet-20, ResNet-32, ResNet-56, and ResNet-110 models by 1.33×, 1.34×, 1.53×, and 1.51×, respectively.\n",
      "Question:\n",
      "How much energy consumption is reduced by CoAxNN on different models? Answer:\n",
      "CoAxNN reduces\n",
      "Question:\n",
      "What percentage of accuracy loss did CoAxNN reduce on the CIFAR-10 dataset? Answer:\n",
      "CoAxNN reduced the accuracy loss of 0.67%, 0.84%, 0.74%, and 0.63% on the CIFAR-10 dataset.\n",
      "Question:\n",
      "What percentage of energy consumption did CoAxNN reduce on the ResNet-20, ResNet-32, ResNet-56, and ResNet-110 models? Answer:\n",
      "CoAxNN reduced the energy consumption of ResNet-20, ResNet-32, ResNet-56, and ResNet-110 by 25.17%, 25.68%, 34.61%, and 33.81%, respectively.\n",
      "Question:\n",
      "What is the difference in execution latency between the baseline models and the optimized models in the study? Answer:\n",
      "The optimized models have higher execution latency compared to the baseline models, with an increase of 10.2% and 13.6% for ResNet-18 and ResNet-50, respectively.\n",
      "Question:\n",
      "What is the reason for the increase in execution latency in the optimized models, according to the study? Answer:\n",
      "The increase in execution latency in the optimized models is due to the filter pruning technique used to reduce the computational costs and memory footprint, which results in a higher number of computations required to complete the same task.\n",
      "Question:\n",
      "What is the implication of the study's findings regarding the use of filter pruning for neural network acceleration? Answer:\n",
      "The study's findings suggest that filter\n",
      "Question:\n",
      "What is the percentage of FLOPs reduction for the ResNet-18 model with the optimal configuration? Answer:\n",
      "The ResNet-18 model with the optimal configuration achieves a FLOPs reduction of 1.01%.\n",
      "Question:\n",
      "What is the percentage of FLOPs reduction for the ResNet-50 model with the optimal configuration? Answer:\n",
      "The ResNet-50 model with the optimal configuration achieves a FLOPs reduction of −0.10%.\n",
      "Question:\n",
      "What is the number of FLOPs for the ResNet-18 model with the optimal configuration? Answer:\n",
      "The ResNet-18 model with the optimal configuration has 39.90% FLOPs.\n",
      "Question:\n",
      "What is the energy reduction of the optimized ResNet-20 model? Answer:\n",
      "The energy reduction of the optimized ResNet-20 model is 0.67%.\n",
      "Question:\n",
      "What is the energy reduction of the optimized ResNet-32 model? Answer:\n",
      "The energy reduction of the optimized ResNet-32 model is 0.84%.\n",
      "Question:\n",
      "What is the energy reduction of the optimized ResNet-56 model? Answer:\n",
      "The energy reduction of the optimized ResNet-56 model is 0.74%.\n",
      "\n",
      "Note: All questions and answers are case-sensitive.\n",
      "Question:\n",
      "What does \"CoAxNN-ACT\" denote in the given text? Answer:\n",
      "CoAxNN-ACT denotes the accuracy of the model at each stage on the whole dataset and on the images that satisfy the activation condition of the corresponding stage, respectively.\n",
      "Question:\n",
      "What is the critical motivation of CoAxNN? Answer:\n",
      "The critical motivation of CoAxNN is to find a satisfying optimization configuration for practical scenarios.\n",
      "Question:\n",
      "What does the ablation study in Fig. 5 show? Answer:\n",
      "The ablation study in Fig. 5 shows the accuracy of ResNet-56 optimized by CoAxNN at different stages, and it shows that the accuracy of the model in the first few stages is lower than that of the baseline model, and as the computational complexity of the model increases, the accuracy in the later stages increases.\n",
      "Question:\n",
      "What is the main conclusion that can be drawn from the visualization results in Fig. 5? Answer:\n",
      "The main conclusion that can be drawn from the visualization results in Fig. 5 is that the accuracy of the CoAxNN-ALL model gradually converges to that of the baseline model as the computational complexity of the model increases.\n",
      "Question:\n",
      "How does the CoAxNN-ACT model differ from the CoAxNN-ALL model? Answer:\n",
      "The CoAxNN-ACT model differs from the CoAxNN-ALL model in that the former has a higher accuracy in the first few stages, indicating that the first few stages have sufficient ability to classify simple images.\n",
      "Question:\n",
      "What can be inferred from the visualization results in Fig. 6? Answer:\n",
      "From the visualization results in Fig. 6, it can be inferred that the CoAxNN\n",
      "Question:\n",
      "What is the main advantage of using CoAxNN? Answer:\n",
      "CoAxNN can separate \"easy\" images consuming less effort from \"hard\" images consuming more computation, significantly reducing computation costs for neural network models.\n",
      "Question:\n",
      "What is the main disadvantage of using the baseline model? Answer:\n",
      "The accuracy of the last stage of the optimization model is lower than that of the baseline model.\n",
      "Question:\n",
      "What is the purpose of collecting the latency of each operator in the profiling phase? Answer:\n",
      "We collect the latency of each operator of the neural network model on the edge device in the profiling phase to be used in GA-based search.\n",
      "Question:\n",
      "What are the benefits of GA-based DSE? Answer:\n",
      "Significantly reducing computation costs for neural network models.\n",
      "Question:\n",
      "What are the overheads of GA-based DSE? Answer:\n",
      "The GA-based DSE takes 1–2 s on the CPU platform, which is greatly less than model training.\n",
      "Question:\n",
      "Can CoAxNN be used for other intelligent tasks? Answer:\n",
      "Yes, CoAxNN is a generic framework for optimizing on-device deep learning via model approximation, which can be generalized to other intelligent tasks such as object detection.\n",
      "\n",
      "Note: The questions and answers are generated based on the provided input text, and may not be accurate or complete.\n",
      "Question:\n",
      "What is the main advantage of CoAxNN? Answer:\n",
      "The main advantage of CoAxNN is that the runtime overhead is negligible.\n",
      "Question:\n",
      "Can CoAxNN be applied to other intelligent tasks? Answer:\n",
      "Yes, CoAxNN can be applied to other intelligent tasks such as object detection.\n",
      "Question:\n",
      "What is the advantage of using CoAxNN? Answer:\n",
      "The advantage of using CoAxNN is that it can achieve efficient fine-tuning of neural network models without requiring specific software implementations and hardware design support.\n",
      "Question:\n",
      "What are the requirements of intelligent tasks? Answer:\n",
      "The requirements of intelligent tasks include the ability to perform optimization processes efficiently and effectively.\n",
      "Question:\n",
      "What are the limitations of CoAxNN? Answer:\n",
      "The limitations of CoAxNN include the use of a fixed-rate filter pruning strategy and the inability of the NSGA-III used in GA-based DSE to always find the optimal solutions for increasing accuracy and decreasing latency.\n",
      "Question:\n",
      "What future studies will be explored in CoAxNN? Answer:\n",
      "Future studies in CoAxNN will include exploring other genetic algorithms such as NPGA for multiobjective optimization and setting different pruning ratios for different layers to further improve performance.\n",
      "\n",
      "Note: The questions and answers are generated based on the provided text, but they may not be accurate or complete.\n",
      "Question:\n",
      "What are the different sensitives for model accuracy? Answer:\n",
      "Different layers have different sensitives for model accuracy.\n",
      "Question:\n",
      "What will be explored in future studies? Answer:\n",
      "Setting different pruning ratios for different layers can potentially further improve the performance, which will be explored in future studies.\n",
      "Question:\n",
      "What is the conclusion of the paper? Answer:\n",
      "In this paper, we proposed an efficient optimization framework, CoAxNN, which effectively combines staging-based with pruning-based approximate strategies for efficient model inference on resource-constrained edge devices.\n",
      "Question:\n",
      "What is reference [1]? Answer:\n",
      "Reference [1]: K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in:\n",
      "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n",
      "2016, pp. 770–778.\n",
      "Question:\n",
      "What is reference [2]? Answer:\n",
      "Reference [2]: Y. He, G. Kang, X. Dong, Y. Fu, Y. Yang, Soft filter pruning for accelerating\n",
      "in: Proceedings of the Twenty-Seventh\n",
      "Intelligence (IJCAI), 2018, pp.\n",
      "\n",
      "deep convolutional neural networks,\n",
      "International Joint Conference on Artificial\n",
      "2234–2240.\n",
      "Question:\n",
      "What is reference [3]? Answer:\n",
      "Reference [3]: S.K. Esser, J.L. McKinstry, D. Bablani, R. Appuswamy, D.S. Modha, Learned\n",
      "step size quantization, in: International Conference on Learning Representations,\n",
      "2020.\n",
      "Question:\n",
      "What is reference [4]? Answer:\n",
      "Reference [4]: Y. Guo, A. Yao, Y. Chen, Dynamic network surgery for efficient dnns,\n",
      "\n",
      "in:\n",
      "\n",
      "Advances in Neural Information Processing Systems, Vol. 29, 2016.\n",
      "Question:\n",
      "What is reference [5]? Answer:\n",
      "Reference [5]: S. Han, J. Pool, J. Tran, W. Dally, Learning both weights and connections for\n",
      "efficient neural network, in: Advances in Neural Information Processing Systems,\n",
      "Vol. 28, 2015.\n",
      "Question:\n",
      "What is reference [6]? Answer:\n",
      "Reference [6]: B. Hassibi, D. Stork, Second order derivatives for network pruning: Optimal brain\n",
      "surgeon, in: Advances in Neural Information Processing Systems, Vol. 5, 1992.\n",
      "Question:\n",
      "What is reference [7]? Answer:\n",
      "Reference [7]: B. Hassibi, D.G. Stork, G.J. Wolff, Optimal brain surgeon and general network\n",
      "pruning, in: IEEE International Conference on Neural Networks, IEEE, 1993, pp.\n",
      "293–299.\n",
      "Question:\n",
      "What is reference [8]? Answer:\n",
      "Reference [8]: Y. He, X. Dong, G. Kang, Y. Fu, C. Yan, Y. Yang, Asymptotic soft filter pruning\n",
      "for deep convolutional neural networks, IEEE Trans. Cybern. 50 (8) (2019)\n",
      "3594–3604.\n",
      "Question:\n",
      "What is reference [9]? Answer:\n",
      "Reference [9]: G. Li, X. Ma, X. Wang, L. Liu, J. Xue, X. Feng, Fusion-catalyzed pruning for\n",
      "optimizing deep learning on intelligent edge devices, IEEE Trans. Comput.-Aided\n",
      "Des. Integr. Circuits Syst. 39 (11) (2020) 3614–3626.\n",
      "Question:\n",
      "What is reference [10]? Answer:\n",
      "Reference [10]: J.-H. Luo, J. Wu, W. Lin, Thinet: A filter level pruning method for deep neural\n",
      "network compression, in: Proceedings of the IEEE International Conference on\n",
      "Computer Vision, 2017, pp. 5058–5066.\n",
      "Question:\n",
      "What is reference [11]? Answer:\n",
      "Reference [11]: G. Li, X. Ma, X. Wang, H. Yue, J. Li, L. Liu, X. Feng, J. Xue, Optimizing deep\n",
      "neural networks on intelligent edge accelerators via flexible-rate filter pruning,\n",
      "J. Syst. Archit. (2022) 102431.\n",
      "Question:\n",
      "What is reference [12]? Answer:\n",
      "Reference [12]: J. Plochaet, T. Goedemé, Hardware-aware pruning for FPGA deep learning\n",
      "accelerators, in: Proceedings of the IEEE/CVF Conference on Computer Vision\n",
      "and Pattern Recognition, 2023, pp. 4481–4489.\n",
      "Question:\n",
      "What is reference [13]? Answer:\n",
      "Reference [13]: X. Zhuang, Y. Ge, B. Zheng, Q. Wang, Adversarial network pruning by filter\n",
      "robustness estimation, in: ICASSP 2023-2023 IEEE International Conference on\n",
      "Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2023, pp. 1–5.\n",
      "Question:\n",
      "What is reference [14]? Answer:\n",
      "Reference [14]: Z. Liu, M. Sun, T. Zhou, G. Huang, T. Darrell, Rethinking the value of network\n",
      "pruning, in: International Conference on Learning Representations (ICLR), 2019.\n",
      "Question:\n",
      "What is reference [15]? Answer:\n",
      "Reference [15]: Y. Li, K. Adamczewski, W. Li, S. Gu, R. Timofte, L. Van Gool, Revisiting\n",
      "random channel pruning for neural network compression, in: Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp.\n",
      "191–201.\n",
      "Question:\n",
      "What is reference [16]? Answer:\n",
      "Reference [16]: Y. Li, P. Zhao, G. Yuan, X. Lin, Y. Wang, X. Chen, Pruning-as-search: Efficient\n",
      "neural architecture search via channel pruning and structural reparameterization,\n",
      "in: Proceedings of the Thirty-First International Joint Conference on Artificial\n",
      "Intelligence, 2022, pp. 3236–3242.\n",
      "Question:\n",
      "What is reference [17]? Answer:\n",
      "Reference [17]: Y. Ding, Y. Wu, C. Huang, S. Tang, F. Wu, Y. Yang, W. Zhu, Y. Zhuang, NAP:\n",
      "\n",
      "Neural architecture search with pruning, Neurocomputing 477 (2022) 85–95.\n",
      "Question:\n",
      "What is reference [18]? Answer:\n",
      "Reference [18]: S. Teerapittayanon, B. McDanel, H.-T. Kung, Branchynet: Fast inference via early\n",
      "exiting from deep neural networks, in: 2016 23rd International Conference on\n",
      "Pattern Recognition (ICPR), IEEE, 2016, pp. 2464–2469.\n",
      "Question:\n",
      "What is reference [19]? Answer:\n",
      "Reference [19]: P. Panda, A. Sengupta, K. Roy, Conditional deep learning for energy-efficient\n",
      "and enhanced pattern recognition, in: 2016 Design, Automation & Test in Europe\n",
      "Conference & Exhibition (DATE), IEEE, 2016, pp. 475–480.\n",
      "Question:\n",
      "What is reference [20]? Answer:\n",
      "Reference [20]: Y. Yang, D. Liu, H. Fang, Y.-X. Huang, Y. Sun, Z.-Y. Zhang, Once for all skip:\n",
      "efficient adaptive deep neural networks, in: 2022 Design, Automation & Test in\n",
      "Europe Conference & Exhibition (DATE), IEEE, 2022, pp. 568–571.\n",
      "Question:\n",
      "What is reference [21]? Answer:\n",
      "Reference [21]: B. Fang, X. Zeng, F. Zhang, H. Xu, M. Zhang, FlexDNN: Input-adaptive on-device\n",
      "deep learning for efficient mobile vision, in: 2020 IEEE/ACM Symposium on Edge\n",
      "Computing (SEC), IEEE, 2020, pp. 84–95.\n",
      "Question:\n",
      "What is reference [22]? Answer:\n",
      "Reference [22]: Y. Wang, J. Shen, T.-K. Hu, P. Xu, T. Nguyen, R. Baraniuk, Z. Wang, Y. Lin,\n",
      "Dual dynamic inference: Enabling more efficient, adaptive, and controllable deep\n",
      "inference, IEEE J. Sel. Top. Sign. Proces. 14 (4) (2020) 623–633.\n",
      "Question:\n",
      "What is reference [23]? Answer:\n",
      "Reference [23]: M. Figurnov, M.D. Collins, Y. Zhu, L. Zhang, J. Huang, D. Vetrov, R. Salakhutdi-\n",
      "nov, Spatially adaptive computation time for residual networks, in: Proceedings\n",
      "of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp.\n",
      "1039–1048.\n",
      "Question:\n",
      "What is reference [24]? Answer:\n",
      "Reference [24]: N.K. Jayakodi, A. Chatterjee, W. Choi, J.R. Doppa, P.P. Pande, Trading-off\n",
      "accuracy and energy of deep inference on embedded systems: A co-design\n",
      "approach, IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst. 37 (11) (2018)\n",
      "2881–2893.\n",
      "Question:\n",
      "What is reference [25]? Answer:\n",
      "Reference [25]: Z. Liang, Y. Zhou, Dispense mode for inference to accelerate branchynet, in:\n",
      "2022 IEEE International Conference on Image Processing (ICIP), IEEE, 2022, pp.\n",
      "1246–1250.\n",
      "Question:\n",
      "What is reference [26]? Answer:\n",
      "Reference [26]: J. Jo, G. Kim, S. Kim, J. Park, LoCoExNet: Low-cost early exit network for energy\n",
      "efficient CNN accelerator design, IEEE Trans. Comput.-Aided Des. Integr. Circuits\n",
      "Syst. (2023).\n",
      "Question:\n",
      "What is reference [27]? Answer:\n",
      "Reference [27]: K. Park, C. Oh, Y. Yi, Bpnet: branch-pruned conditional neural network for\n",
      "systematic time-accuracy tradeoff, in: 2020 57th ACM/IEEE Design Automation\n",
      "Conference (DAC), IEEE, 2020, pp. 1–6.\n",
      "Question:\n",
      "What is reference [28]? Answer:\n",
      "Reference [28]: G. Park, Y. Yi, Condnas: neural architecture search for conditional CNNs,\n",
      "\n",
      "Electronics 11 (7) (2022) 1101.\n",
      "Question:\n",
      "What is reference [29]? Answer:\n",
      "Reference [29]: Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, S. Han, Amc: Automl for model\n",
      "compression and acceleration on mobile devices, in: Proceedings of the European\n",
      "Conference on Computer Vision (ECCV), 2018, pp. 784–800.\n",
      "Question:\n",
      "What is reference [30]? Answer:\n",
      "Reference [30]: Y. Qian, Z. He, Y. Wang, B. Wang, X. Ling, Z. Gu, H. Wang, S. Zeng, W. Swaileh,\n",
      "Hierarchical threshold pruning based on uniform response criterion, IEEE Trans.\n",
      "Neural Netw. Learn. Syst. (2023).\n",
      "Question:\n",
      "What is reference [31]? Answer:\n",
      "Reference [31]: K. Wang, D. Zhang, Y. Li, R. Zhang, L. Lin, Cost-effective active learning for deep\n",
      "image classification, IEEE Trans. Circuits Syst. Video Technol. 27 (12) (2016)\n",
      "2591–2600.\n",
      "Question:\n",
      "What is reference [32]? Answer:\n",
      "Reference [32]: S. Anwar, K. Hwang, W. Sung, Structured pruning of deep convolutional neural\n",
      "\n",
      "networks, ACM J. Emerg. Technol. Comput. Syst. (JETC) 13 (3) (2017) 1–18.\n",
      "Question:\n",
      "What is reference [33]? Answer:\n",
      "Reference [33]: J.H. Holland, Adaptation in Natural and Artificial Systems: An Introductory\n",
      "Analysis with Applications To Biology, Control, and Artificial Intelligence, MIT\n",
      "Press, 1992.\n",
      "Question:\n",
      "What is reference [34]? Answer:\n",
      "Reference [34]: A. Mohammadi, H. Asadi, S. Mohamed, K. Nelson, S. Nahavandi, OpenGA, a C++\n",
      "genetic algorithm library, in: 2017 IEEE International Conference on Systems,\n",
      "Man, and Cybernetics (SMC), IEEE, 2017, pp. 2051–2056.\n",
      "Question:\n",
      "What is reference [35]? Answer:\n",
      "Reference [35]: K. Deb, H. Jain, An evolutionary many-objective optimization algorithm using\n",
      "reference-point-based nondominated sorting approach, part I: solving problems\n",
      "with box constraints, IEEE Trans. Evol. Comput. 18 (4) (2013) 577–601.\n",
      "Question:\n",
      "What is reference [36]? Answer:\n",
      "Reference [36]: A. Krizhevsky, G. Hinton, et al., Learning multiple layers of features from tiny\n",
      "\n",
      "images, 2009.\n",
      "Question:\n",
      "What is reference [37]? Answer:\n",
      "Reference [37]: L.N. Darlow, E.J. Crowley, A. Antoniou, A.J. Storkey, Cinic-10 is not imagenet\n",
      "\n",
      "or cifar-10, 2018, arXiv preprint arXiv:1810.03505.\n",
      "Question:\n",
      "What is reference [38]? Answer:\n",
      "Reference [38]: L. Cai, Z. An, C. Yang, Y. Xu, Softer pruning, incremental regularization, in:\n",
      "2020 25th International Conference on Pattern Recognition (ICPR), IEEE, 2021,\n",
      "pp. 224–230.\n",
      "Question:\n",
      "What is reference [39]? Answer:\n",
      "Reference [39]: X. Dong, J. Huang, Y. Yang, S. Yan, More is less: A more complicated network\n",
      "in: Proceedings of the IEEE Conference on\n",
      "\n",
      "with less inference complexity,\n",
      "Computer Vision and Pattern Recognition, 2017, pp. 5840–5848.\n",
      "Question:\n",
      "What is reference [40]? Answer:\n",
      "Reference [40]: Y. He, P. Liu, Z. Wang, Z. Hu, Y. Yang, Filter pruning via geometric median\n",
      "for deep convolutional neural networks acceleration,\n",
      "in: Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp.\n",
      "4340–4349.\n",
      "Question:\n",
      "What is reference [41]? Answer:\n",
      "Reference [41]: X. Dong, Y. Yang, Network pruning via transformable architecture search, Adv.\n",
      "\n",
      "Neural Inf. Process. Syst. 32 (2019).\n",
      "Question:\n",
      "What is reference [42]? Answer:\n",
      "Reference [42]: Y. He, X. Zhang, J. Sun, Channel pruning for accelerating very deep neural\n",
      "networks, in: Proceedings of the IEEE International Conference on Computer\n",
      "Vision, 2017, pp. 1389–1397.\n",
      "Question:\n",
      "What is reference [43]? Answer:\n",
      "Reference [43]: S. Lin, R. Ji, C. Yan, B. Zhang, L. Cao, Q. Ye, F. Huang, D. Doermann,\n",
      "Towards optimal structured cnn pruning via generative adversarial\n",
      "learning,\n",
      "in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, 2019, pp. 2790–2799.\n",
      "Question:\n",
      "What is reference [44]? Answer:\n",
      "Reference [44]: L. Cai, Z. An, C. Yang, Y. Xu, Soft and hard filter pruning via dimension\n",
      "reduction, in: 2021 International Joint Conference on Neural Networks (IJCNN),\n",
      "IEEE, 2021, pp. 1–8.\n",
      "Question:\n",
      "What is reference [45]? Answer:\n",
      "Reference [45]: X. Yang, H. Lu, H. Shuai, X.-T. Yuan, Pruning convolutional neural networks\n",
      "via stochastic gradient hard thresholding, in: Pattern Recognition and Computer\n",
      "Vision: Second Chinese Conference, PRCV 2019, Xi’an, China, November 8–11,\n",
      "2019, Proceedings, Part I, Springer, 2019, pp. 373–385.\n",
      "Question:\n",
      "What is reference [46]? Answer:\n",
      "Reference [46]: O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A.\n",
      "Karpathy, A. Khosla, M. Bernstein, et al., Imagenet large scale visual recognition\n",
      "challenge, Int. J. Comput. Vis. 115 (3) (2015) 211–252.\n",
      "Question:\n",
      "What is reference [47]? Answer:\n",
      "Reference [47]: Y. Chen, X. Wen, Y. Zhang, Q. He, FPC: Filter pruning via the contribution\n",
      "of output feature map for deep convolutional neural networks acceleration,\n",
      "Knowl.-Based Syst. 238 (2022) 107876.\n",
      "Question:\n",
      "What is reference [48]? Answer:\n",
      "Reference [48]: Y. Chen, X. Wen, Y. Zhang, W. Shi, CCPrune: Collaborative channel pruning for\n",
      "\n",
      "learning compact convolutional networks, Neurocomputing 451 (2021) 35–45.\n",
      "Question:\n",
      "What is reference [49]? Answer:\n",
      "Reference [49]: X. Chen, H. Ma, J. Wan, B. Li, T. Xia, Multi-view 3d object detection network\n",
      "for autonomous driving, in: Proceedings of the IEEE Conference on Computer\n",
      "Vision and Pattern Recognition, 2017, pp. 1907–1915.\n",
      "Question:\n",
      "What is reference [50]? Answer:\n",
      "Reference [50]: G. Hinton, O. Vinyals, J. Dean, et al., Distilling the knowledge in a neural\n",
      "\n",
      "network, arXiv preprint arXiv:1503.02531 2 (7) (2015).\n",
      "Question:\n",
      "What is reference [51]? Answer:\n",
      "Reference [51]: J. Horn, N. Nafpliotis, D.E. Goldberg, Multiobjective optimization using the\n",
      "\n",
      "niched Pareto genetic algorithm, 1993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for input_question, gt_answer in tqdm(zip(df['text_question'], df['text_answer'])):\n",
    "    print(input_question, gt_answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "097cb2e4a84c4697accf681c93e12edc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "120b8c0968924793b60706849f301c72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a0f0423754d4d3f8ebca48256e9efcd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "218f807496354b909652f6c6706b90bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d573cd9ac584ef2ab74b809128c3222": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "30f9633de2f94c8b8abc6c1912dd88ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3912c31c04404f3d8c1d6a7ec43dec6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c5eac3321524f9a9bae43c0e94c5139",
      "max": 4081004224,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c7773e3d50b9480bab23d1fcf7914986",
      "value": 4081004224
     }
    },
    "4574e6dd01a3407b8de152ee6ff6e91c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b20fee1d85874abd958e6ac9d7b9795b",
      "placeholder": "​",
      "style": "IPY_MODEL_30f9633de2f94c8b8abc6c1912dd88ff",
      "value": " 4.08G/4.08G [00:20&lt;00:00, 198MB/s]"
     }
    },
    "5545902c540341cd9b191b801d6b22ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9791694d89b4c15ae60d0451d9cd266",
      "placeholder": "​",
      "style": "IPY_MODEL_f2cf09552e3442dc943d29ddd805fd1d",
      "value": " 1/1 [00:20&lt;00:00, 20.67s/it]"
     }
    },
    "6893bc60756e426b83c369afd86f61a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6e79685834b54b32ad5ebbd739a79b28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d75dbcb357d440fb99557eb392d37d31",
      "placeholder": "​",
      "style": "IPY_MODEL_2d573cd9ac584ef2ab74b809128c3222",
      "value": " 1/1 [00:00&lt;00:00,  4.73it/s]"
     }
    },
    "73b44b7bbb68487ca1f4ccf36ac72ce8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80ce0260305a483b962333d9b8123468": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9bfa488500c943b799a847ccd2d547f3",
       "IPY_MODEL_bf18a9e32a654af0922275ac35599f07",
       "IPY_MODEL_5545902c540341cd9b191b801d6b22ff"
      ],
      "layout": "IPY_MODEL_d07dcd7523054609967e40180a8eb8bb"
     }
    },
    "848fa525fb26496b9f0067fa7372f7ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c5eac3321524f9a9bae43c0e94c5139": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d8b17c487954d2ab74ead5bc05ac122": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "924fabd5bf674833b67f64eb0a303d43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d88f22704304fd1aa527195f14d81f1",
       "IPY_MODEL_3912c31c04404f3d8c1d6a7ec43dec6d",
       "IPY_MODEL_4574e6dd01a3407b8de152ee6ff6e91c"
      ],
      "layout": "IPY_MODEL_1a0f0423754d4d3f8ebca48256e9efcd"
     }
    },
    "92f4b38bc9dd4c18b2aebaec96a859d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9be78f99437f4fc2bb4c8bcf396a8e1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e84c5125f27f43fa95f751a88fee2a9b",
       "IPY_MODEL_a65f090e12d64ec7b2b2c872c231bbc9",
       "IPY_MODEL_6e79685834b54b32ad5ebbd739a79b28"
      ],
      "layout": "IPY_MODEL_b93d54427cc34a95847576dc4097beeb"
     }
    },
    "9bfa488500c943b799a847ccd2d547f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_097cb2e4a84c4697accf681c93e12edc",
      "placeholder": "​",
      "style": "IPY_MODEL_c0b61b5cdc65433badae02e608fe8e75",
      "value": "Fetching 1 files: 100%"
     }
    },
    "9cee5dc5e9904bc7a9318de2c24bc800": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d88f22704304fd1aa527195f14d81f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8ba2c8f83714853ba0692f93357f026",
      "placeholder": "​",
      "style": "IPY_MODEL_92f4b38bc9dd4c18b2aebaec96a859d0",
      "value": "llama-2-7b-chat.Q4_K_M.gguf: 100%"
     }
    },
    "a65f090e12d64ec7b2b2c872c231bbc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9cee5dc5e9904bc7a9318de2c24bc800",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6893bc60756e426b83c369afd86f61a2",
      "value": 1
     }
    },
    "b20fee1d85874abd958e6ac9d7b9795b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3ab0f92121349c5b52afcf0255a8fd6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b751d585ea1444e69a9bd3e0d69ba656": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8865106e66248e599a8710fbfac2582": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_120b8c0968924793b60706849f301c72",
      "placeholder": "​",
      "style": "IPY_MODEL_cbed7304d53e4120a4f1e9fccfcab034",
      "value": "config.json: 100%"
     }
    },
    "b93d54427cc34a95847576dc4097beeb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be0de1377a254afab7c0eae0a33c6e61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b751d585ea1444e69a9bd3e0d69ba656",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e01e298f7b884a8480d693a863ca355e",
      "value": 29
     }
    },
    "bf18a9e32a654af0922275ac35599f07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73b44b7bbb68487ca1f4ccf36ac72ce8",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8d8b17c487954d2ab74ead5bc05ac122",
      "value": 1
     }
    },
    "c0b61b5cdc65433badae02e608fe8e75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c7773e3d50b9480bab23d1fcf7914986": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cbed7304d53e4120a4f1e9fccfcab034": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d07dcd7523054609967e40180a8eb8bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d75dbcb357d440fb99557eb392d37d31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8ba2c8f83714853ba0692f93357f026": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d936a3a89ea840f7a7f2e381c5991cf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dcc5a79431cf46deab2ea727e7f4f20c",
      "placeholder": "​",
      "style": "IPY_MODEL_e01ef623b8f341b582a91b5d53de557f",
      "value": " 29.0/29.0 [00:00&lt;00:00, 2.35kB/s]"
     }
    },
    "dcc5a79431cf46deab2ea727e7f4f20c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e01e298f7b884a8480d693a863ca355e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e01ef623b8f341b582a91b5d53de557f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e84c5125f27f43fa95f751a88fee2a9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_218f807496354b909652f6c6706b90bf",
      "placeholder": "​",
      "style": "IPY_MODEL_848fa525fb26496b9f0067fa7372f7ce",
      "value": "Fetching 1 files: 100%"
     }
    },
    "f2cf09552e3442dc943d29ddd805fd1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9791694d89b4c15ae60d0451d9cd266": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbd29588fafe4c3a931a1a520404aede": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b8865106e66248e599a8710fbfac2582",
       "IPY_MODEL_be0de1377a254afab7c0eae0a33c6e61",
       "IPY_MODEL_d936a3a89ea840f7a7f2e381c5991cf5"
      ],
      "layout": "IPY_MODEL_b3ab0f92121349c5b52afcf0255a8fd6"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
